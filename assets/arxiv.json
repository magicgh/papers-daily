{"Computer Vision": {"Computer Vision": {"2202.12884": {"publish_time": "2022-02-25", "title": "Learning to Identify Perceptual Bugs in 3D Video Games", "author": "Benedict Wilkins et.al.", "abstract": "Automated Bug Detection (ABD) in video games is composed of two distinct but complementary problems: automated game exploration and bug identification. Automated game exploration has received much recent attention, spurred on by developments in fields such as reinforcement learning. The complementary problem of identifying the bugs present in a player's experience has for the most part relied on the manual specification of rules. Although it is widely recognised that many bugs of interest cannot be identified with such methods, little progress has been made in this direction. In this work we show that it is possible to identify a range of perceptual bugs using learning-based methods by making use of only the rendered game screen as seen by the player. To support our work, we have developed World of Bugs (WOB) an open platform for testing ABD methods in 3D game environments.", "paper_url": "http://arxiv.org/abs/2202.12884v1", "pdf_url": "http://arxiv.org/pdf/2202.12884v1", "repo_url": null}, "2202.12860": {"publish_time": "2022-02-25", "title": "ARIA: Adversarially Robust Image Attribution for Content Provenance", "author": "Maksym Andriushchenko et.al.", "abstract": "Image attribution -- matching an image back to a trusted source -- is an emerging tool in the fight against online misinformation. Deep visual fingerprinting models have recently been explored for this purpose. However, they are not robust to tiny input perturbations known as adversarial examples. First we illustrate how to generate valid adversarial images that can easily cause incorrect image attribution. Then we describe an approach to prevent imperceptible adversarial attacks on deep visual fingerprinting models, via robust contrastive learning. The proposed training procedure leverages training on $\\ell_\\infty$-bounded adversarial examples, it is conceptually simple and incurs only a small computational overhead. The resulting models are substantially more robust, are accurate even on unperturbed images, and perform well even over a database with millions of images. In particular, we achieve 91.6% standard and 85.1% adversarial recall under $\\ell_\\infty$-bounded perturbations on manipulated images compared to 80.1% and 0.0% from prior work. We also show that robustness generalizes to other types of imperceptible perturbations unseen during training. Finally, we show how to train an adversarially robust image comparator model for detecting editorial changes in matched images.", "paper_url": "http://arxiv.org/abs/2202.12860v1", "pdf_url": "http://arxiv.org/pdf/2202.12860v1", "repo_url": null}, "2202.12838": {"publish_time": "2022-02-25", "title": "RELMOBNET: A Robust Two-Stage End-To-End Training Approach For MOBILENETV3 Based Relative Camera Pose Estimation", "author": "Praveen Kumar Rajendran et.al.", "abstract": "Relative camera pose estimation plays a pivotal role in dealing with 3D reconstruction and visual localization. To address this, we propose a Siamese network based on MobileNetV3-Large for an end-to-end relative camera pose regression independent of camera parameters. The proposed network uses pair of images taken at different locations in the same scene to estimate the 3D translation vector and rotation vector in unit quaternion. To increase the generality of the model, rather than training it for a single scene, data for four scenes are combined to train a single universal model to estimate the relative pose. Further for independency of hyperparameter weighing between translation and rotation loss is not used. Instead we use the novel two-stage training procedure to learn the balance implicitly with faster convergence. We compare the results obtained with the Cambridge Landmarks dataset, comprising of different scenes, with existing CNN-based regression methods as baselines, e.g., RPNet and RCPNet. The findings indicate that, when compared to RCPNet, proposed model improves the estimation of the translation vector by a percentage change of 16.11%, 28.88%, 52.27% on the Kings College, Old Hospital, St Marys Church scenes from Cambridge Landmarks dataset, respectively.", "paper_url": "http://arxiv.org/abs/2202.12838v1", "pdf_url": "http://arxiv.org/pdf/2202.12838v1", "repo_url": null}, "2202.12825": {"publish_time": "2022-02-25", "title": "NeuralFusion: Neural Volumetric Rendering under Human-object Interactions", "author": "Yuheng Jiang et.al.", "abstract": "4D reconstruction and rendering of human activities is critical for immersive VR/AR experience. Recent advances still fail to recover fine geometry and texture results with the level of detail present in the input images from sparse multi-view RGB cameras. In this paper, we propose NeuralHumanFVV, a real-time neural human performance capture and rendering system to generate both high-quality geometry and photo-realistic texture of human activities in arbitrary novel views. We propose a neural geometry generation scheme with a hierarchical sampling strategy for real-time implicit geometry inference, as well as a novel neural blending scheme to generate high resolution (e.g., 1k) and photo-realistic texture results in the novel views. Furthermore, we adopt neural normal blending to enhance geometry details and formulate our neural geometry and texture rendering into a multi-task learning framework. Extensive experiments demonstrate the effectiveness of our approach to achieve high-quality geometry and photo-realistic free view-point reconstruction for challenging human performances.", "paper_url": "http://arxiv.org/abs/2202.12825v1", "pdf_url": "http://arxiv.org/pdf/2202.12825v1", "repo_url": null}, "2202.12818": {"publish_time": "2022-02-25", "title": "Improving generalization with synthetic training data for deep learning based quality inspection", "author": "Antoine Cordier et.al.", "abstract": "Automating quality inspection with computer vision techniques is often a very data-demanding task. Specifically, supervised deep learning requires a large amount of annotated images for training. In practice, collecting and annotating such data is not only costly and laborious, but also inefficient, given the fact that only a few instances may be available for certain defect classes. If working with video frames can increase the number of these instances, it has a major disadvantage: the resulting images will be highly correlated with one another. As a consequence, models trained under such constraints are expected to be very sensitive to input distribution changes, which may be caused in practice by changes in the acquisition system (cameras, lights), in the parts or in the defects aspect. In this work, we demonstrate the use of randomly generated synthetic training images can help tackle domain instability issues, making the trained models more robust to contextual changes. We detail both our synthetic data generation pipeline and our deep learning methodology for answering these questions.", "paper_url": "http://arxiv.org/abs/2202.12818v1", "pdf_url": "http://arxiv.org/pdf/2202.12818v1", "repo_url": null}, "2202.14034": {"publish_time": "2022-02-28", "title": "Attribute Descent: Simulating Object-Centric Datasets on the Content Level and Beyond", "author": "Yue Yao et.al.", "abstract": "This article aims to use graphic engines to simulate a large number of training data that have free annotations and possibly strongly resemble to real-world data. Between synthetic and real, a two-level domain gap exists, involving content level and appearance level. While the latter is concerned with appearance style, the former problem arises from a different mechanism, i.e., content mismatch in attributes such as camera viewpoint, object placement and lighting conditions. In contrast to the widely-studied appearance-level gap, the content-level discrepancy has not been broadly studied. To address the content-level misalignment, we propose an attribute descent approach that automatically optimizes engine attributes to enable synthetic data to approximate real-world data. We verify our method on object-centric tasks, wherein an object takes up a major portion of an image. In these tasks, the search space is relatively small, and the optimization of each attribute yields sufficiently obvious supervision signals. We collect a new synthetic asset VehicleX, and reformat and reuse existing the synthetic assets ObjectX and PersonX. Extensive experiments on image classification and object re-identification confirm that adapted synthetic data can be effectively used in three scenarios: training with synthetic data only, training data augmentation and numerically understanding dataset content.", "paper_url": "http://arxiv.org/abs/2202.14034v1", "pdf_url": "http://arxiv.org/pdf/2202.14034v1", "repo_url": null}, "2202.14030": {"publish_time": "2022-02-28", "title": "Learning Semantic Segmentation from Multiple Datasets with Label Shifts", "author": "Dongwan Kim et.al.", "abstract": "With increasing applications of semantic segmentation, numerous datasets have been proposed in the past few years. Yet labeling remains expensive, thus, it is desirable to jointly train models across aggregations of datasets to enhance data volume and diversity. However, label spaces differ across datasets and may even be in conflict with one another. This paper proposes UniSeg, an effective approach to automatically train models across multiple datasets with differing label spaces, without any manual relabeling efforts. Specifically, we propose two losses that account for conflicting and co-occurring labels to achieve better generalization performance in unseen domains. First, a gradient conflict in training due to mismatched label spaces is identified and a class-independent binary cross-entropy loss is proposed to alleviate such label conflicts. Second, a loss function that considers class-relationships across datasets is proposed for a better multi-dataset training scheme. Extensive quantitative and qualitative analyses on road-scene datasets show that UniSeg improves over multi-dataset baselines, especially on unseen datasets, e.g., achieving more than 8% gain in IoU on KITTI averaged over all the settings.", "paper_url": "http://arxiv.org/abs/2202.14030v1", "pdf_url": "http://arxiv.org/pdf/2202.14030v1", "repo_url": null}, "2202.14026": {"publish_time": "2022-02-28", "title": "Robust Training under Label Noise by Over-parameterization", "author": "Sheng Liu et.al.", "abstract": "Recently, over-parameterized deep networks, with increasingly more network parameters than training samples, have dominated the performances of modern machine learning. However, when the training data is corrupted, it has been well-known that over-parameterized networks tend to overfit and do not generalize. In this work, we propose a principled approach for robust training of over-parameterized deep networks in classification tasks where a proportion of training labels are corrupted. The main idea is yet very simple: label noise is sparse and incoherent with the network learned from clean data, so we model the noise and learn to separate it from the data. Specifically, we model the label noise via another sparse over-parameterization term, and exploit implicit algorithmic regularizations to recover and separate the underlying corruptions. Remarkably, when trained using such a simple method in practice, we demonstrate state-of-the-art test accuracy against label noise on a variety of real datasets. Furthermore, our experimental results are corroborated by theory on simplified linear models, showing that exact separation between sparse noise and low-rank data can be achieved under incoherent conditions. The work opens many interesting directions for improving over-parameterized models by using sparse over-parameterization and implicit regularization.", "paper_url": "http://arxiv.org/abs/2202.14026v1", "pdf_url": "http://arxiv.org/pdf/2202.14026v1", "repo_url": "https://github.com/shengliu66/sop"}, "2202.14020": {"publish_time": "2022-02-28", "title": "State-of-the-Art in the Architecture, Methods and Applications of StyleGAN", "author": "Amit H. Bermano et.al.", "abstract": "Generative Adversarial Networks (GANs) have established themselves as a prevalent approach to image synthesis. Of these, StyleGAN offers a fascinating case study, owing to its remarkable visual quality and an ability to support a large array of downstream tasks. This state-of-the-art report covers the StyleGAN architecture, and the ways it has been employed since its conception, while also analyzing its severe limitations. It aims to be of use for both newcomers, who wish to get a grasp of the field, and for more experienced readers that might benefit from seeing current research trends and existing tools laid out. Among StyleGAN's most interesting aspects is its learned latent space. Despite being learned with no supervision, it is surprisingly well-behaved and remarkably disentangled. Combined with StyleGAN's visual quality, these properties gave rise to unparalleled editing capabilities. However, the control offered by StyleGAN is inherently limited to the generator's learned distribution, and can only be applied to images generated by StyleGAN itself. Seeking to bring StyleGAN's latent control to real-world scenarios, the study of GAN inversion and latent space embedding has quickly gained in popularity. Meanwhile, this same study has helped shed light on the inner workings and limitations of StyleGAN. We map out StyleGAN's impressive story through these investigations, and discuss the details that have made StyleGAN the go-to generator. We further elaborate on the visual priors StyleGAN constructs, and discuss their use in downstream discriminative tasks. Looking forward, we point out StyleGAN's limitations and speculate on current trends and promising directions for future research, such as task and target specific fine-tuning.", "paper_url": "http://arxiv.org/abs/2202.14020v1", "pdf_url": "http://arxiv.org/pdf/2202.14020v1", "repo_url": null}, "2202.14019": {"publish_time": "2022-02-28", "title": "Domain Knowledge-Informed Self-Supervised Representations for Workout Form Assessment", "author": "Paritosh Parmar et.al.", "abstract": "Maintaining proper form while exercising is important for preventing injuries and maximizing muscle mass gains. While fitness apps are becoming popular, they lack the functionality to detect errors in workout form. Detecting such errors naturally requires estimating users' body pose. However, off-the-shelf pose estimators struggle to perform well on the videos recorded in gym scenarios due to factors such as camera angles, occlusion from gym equipment, illumination, and clothing. To aggravate the problem, the errors to be detected in the workouts are very subtle. To that end, we propose to learn exercise-specific representations from unlabeled samples such that a small dataset annotated by experts suffices for supervised error detection. In particular, our domain knowledge-informed self-supervised approaches exploit the harmonic motion of the exercise actions, and capitalize on the large variances in camera angles, clothes, and illumination to learn powerful representations. To facilitate our self-supervised pretraining, and supervised finetuning, we curated a new exercise dataset, Fitness-AQA, comprising of three exercises: BackSquat, BarbellRow, and OverheadPress. It has been annotated by expert trainers for multiple crucial and typically occurring exercise errors. Experimental results show that our self-supervised representations outperform off-the-shelf 2D- and 3D-pose estimators and several other baselines.", "paper_url": "http://arxiv.org/abs/2202.14019v1", "pdf_url": "http://arxiv.org/pdf/2202.14019v1", "repo_url": null}, "2203.00680": {"publish_time": "2022-03-01", "title": "CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding", "author": "Mohamed Afham et.al.", "abstract": "Manual annotation of large-scale point cloud dataset for varying tasks such as 3D object classification, segmentation and detection is often laborious owing to the irregular structure of point clouds. Self-supervised learning, which operates without any human labeling, is a promising approach to address this issue. We observe in the real world that humans are capable of mapping the visual concepts learnt from 2D images to understand the 3D world. Encouraged by this insight, we propose CrossPoint, a simple cross-modal contrastive learning approach to learn transferable 3D point cloud representations. It enables a 3D-2D correspondence of objects by maximizing agreement between point clouds and the corresponding rendered 2D image in the invariant space, while encouraging invariance to transformations in the point cloud modality. Our joint training objective combines the feature correspondences within and across modalities, thus ensembles a rich learning signal from both 3D point cloud and 2D image modalities in a self-supervised fashion. Experimental results show that our approach outperforms the previous unsupervised learning methods on a diverse range of downstream tasks including 3D object classification and segmentation. Further, the ablation studies validate the potency of our approach for a better point cloud understanding. Code and pretrained models are available at http://github.com/MohamedAfham/CrossPoint.", "paper_url": "http://arxiv.org/abs/2203.00680v1", "pdf_url": "http://arxiv.org/pdf/2203.00680v1", "repo_url": "https://github.com/mohamedafham/crosspoint"}, "2203.00672": {"publish_time": "2022-03-01", "title": "Generalizable Person Re-Identification via Self-Supervised Batch Norm Test-Time Adaption", "author": "Ke Han et.al.", "abstract": "In this paper, we investigate the generalization problem of person re-identification (re-id), whose major challenge is the distribution shift on an unseen domain. As an important tool of regularizing the distribution, batch normalization (BN) has been widely used in existing methods. However, they neglect that BN is severely biased to the training domain and inevitably suffers the performance drop if directly generalized without being updated. To tackle this issue, we propose Batch Norm Test-time Adaption (BNTA), a novel re-id framework that applies the self-supervised strategy to update BN parameters adaptively. Specifically, BNTA quickly explores the domain-aware information within unlabeled target data before inference, and accordingly modulates the feature distribution normalized by BN to adapt to the target domain. This is accomplished by two designed self-supervised auxiliary tasks, namely part positioning and part nearest neighbor matching, which help the model mine the domain-aware information with respect to the structure and identity of body parts, respectively. To demonstrate the effectiveness of our method, we conduct extensive experiments on three re-id datasets and confirm the superior performance to the state-of-the-art methods.", "paper_url": "http://arxiv.org/abs/2203.00672v1", "pdf_url": "http://arxiv.org/pdf/2203.00672v1", "repo_url": null}, "2203.00667": {"publish_time": "2022-03-01", "title": "Generative Adversarial Networks", "author": "Gilad Cohen et.al.", "abstract": "Generative Adversarial Networks (GANs) are very popular frameworks for generating high-quality data, and are immensely used in both the academia and industry in many domains. Arguably, their most substantial impact has been in the area of computer vision, where they achieve state-of-the-art image generation. This chapter gives an introduction to GANs, by discussing their principle mechanism and presenting some of their inherent problems during training and evaluation. We focus on these three issues: (1) mode collapse, (2) vanishing gradients, and (3) generation of low-quality images. We then list some architecture-variant and loss-variant GANs that remedy the above challenges. Lastly, we present two utilization examples of GANs for real-world applications: Data augmentation and face images generation.", "paper_url": "http://arxiv.org/abs/2203.00667v1", "pdf_url": "http://arxiv.org/pdf/2203.00667v1", "repo_url": null}, "2203.00645": {"publish_time": "2022-03-01", "title": "Variational Autoencoders Without the Variation", "author": "Gregory A. Daly et.al.", "abstract": "Variational autoencdoers (VAE) are a popular approach to generative modelling. However, exploiting the capabilities of VAEs in practice can be difficult. Recent work on regularised and entropic autoencoders have begun to explore the potential, for generative modelling, of removing the variational approach and returning to the classic deterministic autoencoder (DAE) with additional novel regularisation methods. In this paper we empirically explore the capability of DAEs for image generation without additional novel methods and the effect of the implicit regularisation and smoothness of large networks. We find that DAEs can be used successfully for image generation without additional loss terms, and that many of the useful properties of VAEs can arise implicitly from sufficiently large convolutional encoders and decoders when trained on CIFAR-10 and CelebA.", "paper_url": "http://arxiv.org/abs/2203.00645v1", "pdf_url": "http://arxiv.org/pdf/2203.00645v1", "repo_url": null}, "2203.00641": {"publish_time": "2022-03-01", "title": "Multi-Task Multi-Scale Learning For Outcome Prediction in 3D PET Images", "author": "Amine Amyar et.al.", "abstract": "Background and Objectives: Predicting patient response to treatment and survival in oncology is a prominent way towards precision medicine. To that end, radiomics was proposed as a field of study where images are used instead of invasive methods. The first step in radiomic analysis is the segmentation of the lesion. However, this task is time consuming and can be physician subjective. Automated tools based on supervised deep learning have made great progress to assist physicians. However, they are data hungry, and annotated data remains a major issue in the medical field where only a small subset of annotated images is available. Methods: In this work, we propose a multi-task learning framework to predict patient's survival and response. We show that the encoder can leverage multiple tasks to extract meaningful and powerful features that improve radiomics performance. We show also that subsidiary tasks serve as an inductive bias so that the model can better generalize. Results: Our model was tested and validated for treatment response and survival in lung and esophageal cancers, with an area under the ROC curve of 77% and 71% respectively, outperforming single task learning methods. Conclusions: We show that, by using a multi-task learning approach, we can boost the performance of radiomic analysis by extracting rich information of intratumoral and peritumoral regions.", "paper_url": "http://arxiv.org/abs/2203.00641v1", "pdf_url": "http://arxiv.org/pdf/2203.00641v1", "repo_url": null}, "2203.01318": {"publish_time": "2022-03-02", "title": "Protecting Celebrities with Identity Consistency Transformer", "author": "Xiaoyi Dong et.al.", "abstract": "In this work we propose Identity Consistency Transformer, a novel face forgery detection method that focuses on high-level semantics, specifically identity information, and detecting a suspect face by finding identity inconsistency in inner and outer face regions. The Identity Consistency Transformer incorporates a consistency loss for identity consistency determination. We show that Identity Consistency Transformer exhibits superior generalization ability not only across different datasets but also across various types of image degradation forms found in real-world applications including deepfake videos. The Identity Consistency Transformer can be easily enhanced with additional identity information when such information is available, and for this reason it is especially well-suited for detecting face forgeries involving celebrities.", "paper_url": "http://arxiv.org/abs/2203.01318v1", "pdf_url": "http://arxiv.org/pdf/2203.01318v1", "repo_url": null}, "2203.01311": {"publish_time": "2022-03-02", "title": "HighMMT: Towards Modality and Task Generalization for High-Modality Representation Learning", "author": "Paul Pu Liang et.al.", "abstract": "Learning multimodal representations involves discovering correspondences and integrating information from multiple heterogeneous sources of data. While recent research has begun to explore the design of more general-purpose multimodal models (contrary to prior focus on domain and modality-specific architectures), these methods are still largely focused on a small set of modalities in the language, vision, and audio space. In order to accelerate generalization towards diverse and understudied modalities, we investigate methods for high-modality (a large set of diverse modalities) and partially-observable (each task only defined on a small subset of modalities) scenarios. To tackle these challenges, we design a general multimodal model that enables multitask and transfer learning: multitask learning with shared parameters enables stable parameter counts (addressing scalability), and cross-modal transfer learning enables information sharing across modalities and tasks (addressing partial observability). Our resulting model generalizes across text, image, video, audio, time-series, sensors, tables, and set modalities from different research areas, improves the tradeoff between performance and efficiency, transfers to new modalities and tasks, and reveals surprising insights on the nature of information sharing in multitask models. We release our code and benchmarks which we hope will present a unified platform for subsequent theoretical and empirical analysis: https://github.com/pliang279/HighMMT.", "paper_url": "http://arxiv.org/abs/2203.01311v1", "pdf_url": "http://arxiv.org/pdf/2203.01311v1", "repo_url": null}, "2203.01305": {"publish_time": "2022-03-02", "title": "DN-DETR: Accelerate DETR Training by Introducing Query DeNoising", "author": "Feng Li et.al.", "abstract": "We present in this paper a novel denoising training method to speedup DETR (DEtection TRansformer) training and offer a deepened understanding of the slow convergence issue of DETR-like methods. We show that the slow convergence results from the instability of bipartite graph matching which causes inconsistent optimization goals in early training stages. To address this issue, except for the Hungarian loss, our method additionally feeds ground-truth bounding boxes with noises into Transformer decoder and trains the model to reconstruct the original boxes, which effectively reduces the bipartite graph matching difficulty and leads to a faster convergence. Our method is universal and can be easily plugged into any DETR-like methods by adding dozens of lines of code to achieve a remarkable improvement. As a result, our DN-DETR results in a remarkable improvement ($+1.9$AP) under the same setting and achieves the best result (AP $43.4$ and $48.6$ with $12$ and $50$ epochs of training respectively) among DETR-like methods with ResNet-$50$ backbone. Compared with the baseline under the same setting, DN-DETR achieves comparable performance with $50\\%$ training epochs. Code is available at \\url{https://github.com/FengLi-ust/DN-DETR}.", "paper_url": "http://arxiv.org/abs/2203.01305v1", "pdf_url": "http://arxiv.org/pdf/2203.01305v1", "repo_url": null}, "2203.01296": {"publish_time": "2022-03-02", "title": "Half Wavelet Attention on M-Net+ for Low-Light Image Enhancement", "author": "Chi-Mao Fan et.al.", "abstract": "Low-Light Image Enhancement is a computer vision task which intensifies the dark images to appropriate brightness. It can also be seen as an ill-posed problem in image restoration domain. With the success of deep neural networks, the convolutional neural networks surpass the traditional algorithm-based methods and become the mainstream in the computer vision area. To advance the performance of enhancement algorithms, we propose an image enhancement network (HWMNet) based on an improved hierarchical model: M-Net+. Specifically, we use a half wavelet attention block on M-Net+ to enrich the features from wavelet domain. Furthermore, our HWMNet has competitive performance results on two image enhancement datasets in terms of quantitative metrics and visual quality. The source code and pretrained model are available at https://github.com/FanChiMao/HWMNet.", "paper_url": "http://arxiv.org/abs/2203.01296v1", "pdf_url": "http://arxiv.org/pdf/2203.01296v1", "repo_url": "https://github.com/fanchimao/hwmnet"}, "2203.01289": {"publish_time": "2022-03-02", "title": "ADVISE: ADaptive Feature Relevance and VISual Explanations for Convolutional Neural Networks", "author": "Mohammad Mahdi Dehshibi et.al.", "abstract": "To equip Convolutional Neural Networks (CNNs) with explainability, it is essential to interpret how opaque models take specific decisions, understand what causes the errors, improve the architecture design, and identify unethical biases in the classifiers. This paper introduces ADVISE, a new explainability method that quantifies and leverages the relevance of each unit of the feature map to provide better visual explanations. To this end, we propose using adaptive bandwidth kernel density estimation to assign a relevance score to each unit of the feature map with respect to the predicted class. We also propose an evaluation protocol to quantitatively assess the visual explainability of CNN models. We extensively evaluate our idea in the image classification task using AlexNet, VGG16, ResNet50, and Xception pretrained on ImageNet. We compare ADVISE with the state-of-the-art visual explainable methods and show that the proposed method outperforms competing approaches in quantifying feature-relevance and visual explainability while maintaining competitive time complexity. Our experiments further show that ADVISE fulfils the sensitivity and implementation independence axioms while passing the sanity checks. The implementation is accessible for reproducibility purposes on https://github.com/dehshibi/ADVISE.", "paper_url": "http://arxiv.org/abs/2203.01289v1", "pdf_url": "http://arxiv.org/pdf/2203.01289v1", "repo_url": "https://github.com/dehshibi/advise"}, "2203.01929": {"publish_time": "2022-03-03", "title": "CenterSnap: Single-Shot Multi-Object 3D Shape Reconstruction and Categorical 6D Pose and Size Estimation", "author": "Muhammad Zubair Irshad et.al.", "abstract": "This paper studies the complex task of simultaneous multi-object 3D reconstruction, 6D pose and size estimation from a single-view RGB-D observation. In contrast to instance-level pose estimation, we focus on a more challenging problem where CAD models are not available at inference time. Existing approaches mainly follow a complex multi-stage pipeline which first localizes and detects each object instance in the image and then regresses to either their 3D meshes or 6D poses. These approaches suffer from high-computational cost and low performance in complex multi-object scenarios, where occlusions can be present. Hence, we present a simple one-stage approach to predict both the 3D shape and estimate the 6D pose and size jointly in a bounding-box free manner. In particular, our method treats object instances as spatial centers where each center denotes the complete shape of an object along with its 6D pose and size. Through this per-pixel representation, our approach can reconstruct in real-time (40 FPS) multiple novel object instances and predict their 6D pose and sizes in a single-forward pass. Through extensive experiments, we demonstrate that our approach significantly outperforms all shape completion and categorical 6D pose and size estimation baselines on multi-object ShapeNet and NOCS datasets respectively with a 12.6% absolute improvement in mAP for 6D pose for novel real-world object instances.", "paper_url": "http://arxiv.org/abs/2203.01929v1", "pdf_url": "http://arxiv.org/pdf/2203.01929v1", "repo_url": null}, "2203.01925": {"publish_time": "2022-03-03", "title": "Label-Only Model Inversion Attacks via Boundary Repulsion", "author": "Mostafa Kahla et.al.", "abstract": "Recent studies show that the state-of-the-art deep neural networks are vulnerable to model inversion attacks, in which access to a model is abused to reconstruct private training data of any given target class. Existing attacks rely on having access to either the complete target model (whitebox) or the model's soft-labels (blackbox). However, no prior work has been done in the harder but more practical scenario, in which the attacker only has access to the model's predicted label, without a confidence measure. In this paper, we introduce an algorithm, Boundary-Repelling Model Inversion (BREP-MI), to invert private training data using only the target model's predicted labels. The key idea of our algorithm is to evaluate the model's predicted labels over a sphere and then estimate the direction to reach the target class's centroid. Using the example of face recognition, we show that the images reconstructed by BREP-MI successfully reproduce the semantics of the private training data for various datasets and target model architectures. We compare BREP-MI with the state-of-the-art whitebox and blackbox model inversion attacks and the results show that despite assuming less knowledge about the target model, BREP-MI outperforms the blackbox attack and achieves comparable results to the whitebox attack.", "paper_url": "http://arxiv.org/abs/2203.01925v1", "pdf_url": "http://arxiv.org/pdf/2203.01925v1", "repo_url": null}, "2203.01923": {"publish_time": "2022-03-03", "title": "Recovering 3D Human Mesh from Monocular Images: A Survey", "author": "Yating Tian et.al.", "abstract": "Estimating human pose and shape from monocular images is a long-standing problem in computer vision. Since the release of statistical body models, 3D human mesh recovery has been drawing broader attention. With the same goal of obtaining well-aligned and physically plausible mesh results, two paradigms have been developed to overcome challenges in the 2D-to-3D lifting process: i) an optimization-based paradigm, where different data terms and regularization terms are exploited as optimization objectives; and ii) a regression-based paradigm, where deep learning techniques are embraced to solve the problem in an end-to-end fashion. Meanwhile, continuous efforts are devoted to improving the quality of 3D mesh labels for a wide range of datasets. Though remarkable progress has been achieved in the past decade, the task is still challenging due to flexible body motions, diverse appearances, complex environments, and insufficient in-the-wild annotations. To the best of our knowledge, this is the first survey to focus on the task of monocular 3D human mesh recovery. We start with the introduction of body models, and then introduce recovery frameworks and training objectives by providing in-depth analyses of their strengths and weaknesses. We also summarize datasets, evaluation metrics, and benchmark results. Open issues and future directions are discussed in the end, hoping to motivate researchers and facilitate their research in this area. A regularly updated project page can be found at https://github.com/tinatiansjz/hmr-survey.", "paper_url": "http://arxiv.org/abs/2203.01923v1", "pdf_url": "http://arxiv.org/pdf/2203.01923v1", "repo_url": "https://github.com/tinatiansjz/hmr-survey"}, "2203.01922": {"publish_time": "2022-03-03", "title": "Vision-Language Intelligence: Tasks, Representation Learning, and Large Models", "author": "Feng Li et.al.", "abstract": "This paper presents a comprehensive survey of vision-language (VL) intelligence from the perspective of time. This survey is inspired by the remarkable progress in both computer vision and natural language processing, and recent trends shifting from single modality processing to multiple modality comprehension. We summarize the development in this field into three time periods, namely task-specific methods, vision-language pre-training (VLP) methods, and larger models empowered by large-scale weakly-labeled data. We first take some common VL tasks as examples to introduce the development of task-specific methods. Then we focus on VLP methods and comprehensively review key components of the model structures and training methods. After that, we show how recent work utilizes large-scale raw image-text data to learn language-aligned visual representations that generalize better on zero or few shot learning tasks. Finally, we discuss some potential future trends towards modality cooperation, unified representation, and knowledge incorporation. We believe that this review will be of help for researchers and practitioners of AI and ML, especially those interested in computer vision and natural language processing.", "paper_url": "http://arxiv.org/abs/2203.01922v1", "pdf_url": "http://arxiv.org/pdf/2203.01922v1", "repo_url": null}, "2203.01921": {"publish_time": "2022-03-03", "title": "NUQ: A Noise Metric for Diffusion MRI via Uncertainty Discrepancy Quantification", "author": "Shreyas Fadnavis et.al.", "abstract": "Diffusion MRI (dMRI) is the only non-invasive technique sensitive to tissue micro-architecture, which can, in turn, be used to reconstruct tissue microstructure and white matter pathways. The accuracy of such tasks is hampered by the low signal-to-noise ratio in dMRI. Today, the noise is characterized mainly by visual inspection of residual maps and estimated standard deviation. However, it is hard to estimate the impact of noise on downstream tasks based only on such qualitative assessments. To address this issue, we introduce a novel metric, Noise Uncertainty Quantification (NUQ), for quantitative image quality analysis in the absence of a ground truth reference image. NUQ uses a recent Bayesian formulation of dMRI models to estimate the uncertainty of microstructural measures. Specifically, NUQ uses the maximum mean discrepancy metric to compute a pooled quality score by comparing samples drawn from the posterior distribution of the microstructure measures. We show that NUQ allows a fine-grained analysis of noise, capturing details that are visually imperceptible. We perform qualitative and quantitative comparisons on real datasets, showing that NUQ generates consistent scores across different denoisers and acquisitions. Lastly, by using NUQ on a cohort of schizophrenics and controls, we quantify the substantial impact of denoising on group differences.", "paper_url": "http://arxiv.org/abs/2203.01921v1", "pdf_url": "http://arxiv.org/pdf/2203.01921v1", "repo_url": null}, "2203.02503": {"publish_time": "2022-03-04", "title": "HyperTransformer: A Textural and Spectral Feature Fusion Transformer for Pansharpening", "author": "Wele Gedara Chaminda Bandara et.al.", "abstract": "Pansharpening aims to fuse a registered high-resolution panchromatic image (PAN) with a low-resolution hyperspectral image (LR-HSI) to generate an enhanced HSI with high spectral and spatial resolution. Existing pansharpening approaches neglect using an attention mechanism to transfer HR texture features from PAN to LR-HSI features, resulting in spatial and spectral distortions. In this paper, we present a novel attention mechanism for pansharpening called HyperTransformer, in which features of LR-HSI and PAN are formulated as queries and keys in a transformer, respectively. HyperTransformer consists of three main modules, namely two separate feature extractors for PAN and HSI, a multi-head feature soft attention module, and a spatial-spectral feature fusion module. Such a network improves both spatial and spectral quality measures of the pansharpened HSI by learning cross-feature space dependencies and long-range details of PAN and LR-HSI. Furthermore, HyperTransformer can be utilized across multiple spatial scales at the backbone for obtaining improved performance. Extensive experiments conducted on three widely used datasets demonstrate that HyperTransformer achieves significant improvement over the state-of-the-art methods on both spatial and spectral quality measures. Implementation code and pre-trained weights can be accessed at https://github.com/wgcban/HyperTransformer.", "paper_url": "http://arxiv.org/abs/2203.02503v1", "pdf_url": "http://arxiv.org/pdf/2203.02503v1", "repo_url": "https://github.com/wgcban/HyperTransformer"}, "2203.02489": {"publish_time": "2022-03-04", "title": "Pedestrian Stop and Go Forecasting with Hybrid Feature Fusion", "author": "Dongxu Guo et.al.", "abstract": "Forecasting pedestrians' future motions is essential for autonomous driving systems to safely navigate in urban areas. However, existing prediction algorithms often overly rely on past observed trajectories and tend to fail around abrupt dynamic changes, such as when pedestrians suddenly start or stop walking. We suggest that predicting these highly non-linear transitions should form a core component to improve the robustness of motion prediction algorithms. In this paper, we introduce the new task of pedestrian stop and go forecasting. Considering the lack of suitable existing datasets for it, we release TRANS, a benchmark for explicitly studying the stop and go behaviors of pedestrians in urban traffic. We build it from several existing datasets annotated with pedestrians' walking motions, in order to have various scenarios and behaviors. We also propose a novel hybrid model that leverages pedestrian-specific and scene features from several modalities, both video sequences and high-level attributes, and gradually fuses them to integrate multiple levels of context. We evaluate our model and several baselines on TRANS, and set a new benchmark for the community to work on pedestrian stop and go forecasting.", "paper_url": "http://arxiv.org/abs/2203.02489v1", "pdf_url": "http://arxiv.org/pdf/2203.02489v1", "repo_url": null}, "2203.02488": {"publish_time": "2022-03-04", "title": "Behavioural Curves Analysis Using Near-Infrared-Iris Image Sequences", "author": "L. Causa et.al.", "abstract": "This paper proposes a new method to estimate behavioural curves from a stream of Near-Infra-Red (NIR) iris video frames. This method can be used in a Fitness For Duty system (FFD). The research focuses on determining the effect of external factors such as alcohol, drugs, and sleepiness on the Central Nervous System (CNS). The aim is to analyse how this behaviour is represented on iris and pupil movements and if it is possible to capture these changes with a standard NIR camera. The behaviour analysis showed essential differences in pupil and iris behaviour to classify the workers in \"Fit\" or \"Unfit\" conditions. The best results can distinguish subjects robustly under alcohol, drug consumption, and sleep conditions. The Multi-Layer-Perceptron and Gradient Boosted Machine reached the best results in all groups with an overall accuracy for Fit and Unfit classes of 74.0% and 75.5%, respectively. These results open a new application for iris capture devices.", "paper_url": "http://arxiv.org/abs/2203.02488v1", "pdf_url": "http://arxiv.org/pdf/2203.02488v1", "repo_url": null}, "2203.02486": {"publish_time": "2022-03-04", "title": "The Familiarity Hypothesis: Explaining the Behavior of Deep Open Set Methods", "author": "Thomas G. Dietterich et.al.", "abstract": "In many object recognition applications, the set of possible categories is an open set, and the deployed recognition system will encounter novel objects belonging to categories unseen during training. Detecting such \"novel category\" objects is usually formulated as an anomaly detection problem. Anomaly detection algorithms for feature-vector data identify anomalies as outliers, but outlier detection has not worked well in deep learning. Instead, methods based on the computed logits of visual object classifiers give state-of-the-art performance. This paper proposes the Familiarity Hypothesis that these methods succeed because they are detecting the absence of familiar learned features rather than the presence of novelty. The paper reviews evidence from the literature and presents additional evidence from our own experiments that provide strong support for this hypothesis. The paper concludes with a discussion of whether familiarity detection is an inevitable consequence of representation learning.", "paper_url": "http://arxiv.org/abs/2203.02486v1", "pdf_url": "http://arxiv.org/pdf/2203.02486v1", "repo_url": null}, "2203.02480": {"publish_time": "2022-03-04", "title": "Didn't see that coming: a survey on non-verbal social human behavior forecasting", "author": "German Barquero et.al.", "abstract": "Non-verbal social human behavior forecasting has increasingly attracted the interest of the research community in recent years. Its direct applications to human-robot interaction and socially-aware human motion generation make it a very attractive field. In this survey, we define the behavior forecasting problem for multiple interactive agents in a generic way that aims at unifying the fields of social signals prediction and human motion forecasting, traditionally separated. We hold that both problem formulations refer to the same conceptual problem, and identify many shared fundamental challenges: future stochasticity, context awareness, history exploitation, etc. We also propose a taxonomy that comprises methods published in the last 5 years in a very informative way and describes the current main concerns of the community with regard to this problem. In order to promote further research on this field, we also provide a summarised and friendly overview of audiovisual datasets featuring non-acted social interactions. Finally, we describe the most common metrics used in this task and their particular issues.", "paper_url": "http://arxiv.org/abs/2203.02480v1", "pdf_url": "http://arxiv.org/pdf/2203.02480v1", "repo_url": null}, "2203.03610": {"publish_time": "2022-03-07", "title": "ZippyPoint: Fast Interest Point Detection, Description, and Matching through Mixed Precision Discretization", "author": "Simon Maurer et.al.", "abstract": "The design of more complex and powerful neural network models has significantly advanced the state-of-the-art in local feature detection and description. These advances can be attributed to deeper networks, improved training methodologies through self-supervision, or the introduction of new building blocks, such as graph neural networks for feature matching. However, in the pursuit of increased performance, efficient architectures that generate lightweight descriptors have received surprisingly little attention. In this paper, we investigate the adaptations neural networks for detection and description require in order to enable their use in embedded platforms. To that end, we investigate and adapt network quantization techniques for use in real-time applications. In addition, we revisit common practices in descriptor quantization and propose the use of a binary descriptor normalization layer, enabling the generation of distinctive length-invariant binary descriptors. ZippyPoint, our efficient network, runs at 47.2 fps on the Apple M1 CPU. This is up to 5x faster than other learned detection and description models, making it the only real-time learned network. ZippyPoint consistently outperforms all other binary detection and descriptor methods in visual localization and homography estimation tasks. Code and trained models will be released upon publication.", "paper_url": "http://arxiv.org/abs/2203.03610v1", "pdf_url": "http://arxiv.org/pdf/2203.03610v1", "repo_url": null}, "2203.03609": {"publish_time": "2022-03-07", "title": "Human-Aware Object Placement for Visual Environment Reconstruction", "author": "Hongwei Yi et.al.", "abstract": "Humans are in constant contact with the world as they move through it and interact with it. This contact is a vital source of information for understanding 3D humans, 3D scenes, and the interactions between them. In fact, we demonstrate that these human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we accumulate HSIs across multiple input images, and optimize the 3D scene to reconstruct a consistent, physically plausible and functional 3D scene layout. Our optimization-based approach exploits three types of HSI constraints: (1) humans that move in a scene are occluded or occlude objects, thus, defining the depth ordering of the objects, (2) humans move through free space and do not interpenetrate objects, (3) when humans and objects are in contact, the contact surfaces occupy the same place in space. Using these constraints in an optimization formulation across all observations, we significantly improve the 3D scene layout reconstruction. Furthermore, we show that our scene reconstruction can be used to refine the initial 3D human pose and shape (HPS) estimation. We evaluate the 3D scene layout reconstruction and HPS estimation qualitatively and quantitatively using the PROX and PiGraphs datasets. The code and data are available for research purposes at https://mover.is.tue.mpg.de/.", "paper_url": "http://arxiv.org/abs/2203.03609v1", "pdf_url": "http://arxiv.org/pdf/2203.03609v1", "repo_url": null}, "2203.03605": {"publish_time": "2022-03-07", "title": "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection", "author": "Hao Zhang et.al.", "abstract": "We present DINO (\\textbf{D}ETR with \\textbf{I}mproved de\\textbf{N}oising anch\\textbf{O}r boxes), a state-of-the-art end-to-end object detector. % in this paper. DINO improves over previous DETR-like models in performance and efficiency by using a contrastive way for denoising training, a mixed query selection method for anchor initialization, and a look forward twice scheme for box prediction. DINO achieves $48.3$AP in $12$ epochs and $51.0$AP in $36$ epochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a significant improvement of $\\textbf{+4.9}$\\textbf{AP} and $\\textbf{+2.4}$\\textbf{AP}, respectively, compared to DN-DETR, the previous best DETR-like model. DINO scales well in both model size and data size. Without bells and whistles, after pre-training on the Objects365 dataset with a SwinL backbone, DINO obtains the best results on both COCO \\texttt{val2017} ($\\textbf{63.2}$\\textbf{AP}) and \\texttt{test-dev} (\\textbf{$\\textbf{63.3}$AP}). Compared to other models on the leaderboard, DINO significantly reduces its model size and pre-training data size while achieving better results. Our code will be available at \\url{https://github.com/IDEACVR/DINO}.", "paper_url": "http://arxiv.org/abs/2203.03605v1", "pdf_url": "http://arxiv.org/pdf/2203.03605v1", "repo_url": null}, "2203.03598": {"publish_time": "2022-03-07", "title": "Audio-visual Generalised Zero-shot Learning with Cross-modal Attention and Language", "author": "Otniel-Bogdan Mercea et.al.", "abstract": "Learning to classify video data from classes not included in the training data, i.e. video-based zero-shot learning, is challenging. We conjecture that the natural alignment between the audio and visual modalities in video data provides a rich training signal for learning discriminative multi-modal representations. Focusing on the relatively underexplored task of audio-visual zero-shot learning, we propose to learn multi-modal representations from audio-visual data using cross-modal attention and exploit textual label embeddings for transferring knowledge from seen classes to unseen classes. Taking this one step further, in our generalised audio-visual zero-shot learning setting, we include all the training classes in the test-time search space which act as distractors and increase the difficulty while making the setting more realistic. Due to the lack of a unified benchmark in this domain, we introduce a (generalised) zero-shot learning benchmark on three audio-visual datasets of varying sizes and difficulty, VGGSound, UCF, and ActivityNet, ensuring that the unseen test classes do not appear in the dataset used for supervised training of the backbone deep models. Comparing multiple relevant and recent methods, we demonstrate that our proposed AVCA model achieves state-of-the-art performance on all three datasets. Code and data will be available at \\url{https://github.com/ExplainableML/AVCA-GZSL}.", "paper_url": "http://arxiv.org/abs/2203.03598v1", "pdf_url": "http://arxiv.org/pdf/2203.03598v1", "repo_url": "https://github.com/explainableml/avca-gzsl"}, "2203.03587": {"publish_time": "2022-03-07", "title": "On the pitfalls of entropy-based uncertainty for multi-class semi-supervised segmentation", "author": "Martin Van Waerebeke et.al.", "abstract": "Semi-supervised learning has emerged as an appealing strategy to train deep models with limited supervision. Most prior literature under this learning paradigm resorts to dual-based architectures, typically composed of a teacher-student duple. To drive the learning of the student, many of these models leverage the aleatoric uncertainty derived from the entropy of the predictions. While this has shown to work well in a binary scenario, we demonstrate in this work that this strategy leads to suboptimal results in a multi-class context, a more realistic and challenging setting. We argue, indeed, that these approaches underperform due to the erroneous uncertainty approximations in the presence of inter-class overlap. Furthermore, we propose an alternative solution to compute the uncertainty in a multi-class setting, based on divergence distances and which account for inter-class overlap. We evaluate the proposed solution on a challenging multi-class segmentation dataset and in two well-known uncertainty-based segmentation methods. The reported results demonstrate that by simply replacing the mechanism used to compute the uncertainty, our proposed solution brings substantial improvement on tested setups.", "paper_url": "http://arxiv.org/abs/2203.03587v1", "pdf_url": "http://arxiv.org/pdf/2203.03587v1", "repo_url": null}, "2203.04287": {"publish_time": "2022-03-08", "title": "A Simple Multi-Modality Transfer Learning Baseline for Sign Language Translation", "author": "Yutong Chen et.al.", "abstract": "This paper proposes a simple transfer learning baseline for sign language translation. Existing sign language datasets (e.g. PHOENIX-2014T, CSL-Daily) contain only about 10K-20K pairs of sign videos, gloss annotations and texts, which are an order of magnitude smaller than typical parallel data for training spoken language translation models. Data is thus a bottleneck for training effective sign language translation models. To mitigate this problem, we propose to progressively pretrain the model from general-domain datasets that include a large amount of external supervision to within-domain datasets. Concretely, we pretrain the sign-to-gloss visual network on the general domain of human actions and the within-domain of a sign-to-gloss dataset, and pretrain the gloss-to-text translation network on the general domain of a multilingual corpus and the within-domain of a gloss-to-text corpus. The joint model is fine-tuned with an additional module named the visual-language mapper that connects the two networks. This simple baseline surpasses the previous state-of-the-art results on two sign language translation benchmarks, demonstrating the effectiveness of transfer learning. With its simplicity and strong performance, this approach can serve as a solid baseline for future research.", "paper_url": "http://arxiv.org/abs/2203.04287v1", "pdf_url": "http://arxiv.org/pdf/2203.04287v1", "repo_url": null}, "2203.04279": {"publish_time": "2022-03-08", "title": "Probabilistic Warp Consistency for Weakly-Supervised Semantic Correspondences", "author": "Prune Truong et.al.", "abstract": "We propose Probabilistic Warp Consistency, a weakly-supervised learning objective for semantic matching. Our approach directly supervises the dense matching scores predicted by the network, encoded as a conditional probability distribution. We first construct an image triplet by applying a known warp to one of the images in a pair depicting different instances of the same object class. Our probabilistic learning objectives are then derived using the constraints arising from the resulting image triplet. We further account for occlusion and background clutter present in real image pairs by extending our probabilistic output space with a learnable unmatched state. To supervise it, we design an objective between image pairs depicting different object classes. We validate our method by applying it to four recent semantic matching architectures. Our weakly-supervised approach sets a new state-of-the-art on four challenging semantic matching benchmarks. Lastly, we demonstrate that our objective also brings substantial improvements in the strongly-supervised regime, when combined with keypoint annotations.", "paper_url": "http://arxiv.org/abs/2203.04279v1", "pdf_url": "http://arxiv.org/pdf/2203.04279v1", "repo_url": "https://github.com/PruneTruong/DenseMatching"}, "2203.04275": {"publish_time": "2022-03-08", "title": "Robust Multi-Task Learning and Online Refinement for Spacecraft Pose Estimation across Domain Gap", "author": "Tae Ha Park et.al.", "abstract": "This work presents Spacecraft Pose Network v2 (SPNv2), a Convolutional Neural Network (CNN) for pose estimation of noncooperative spacecraft across domain gap. SPNv2 is a multi-scale, multi-task CNN which consists of a shared multi-scale feature encoder and multiple prediction heads that perform different tasks on a shared feature output. These tasks are all related to detection and pose estimation of a target spacecraft from an image, such as prediction of pre-defined satellite keypoints, direct pose regression, and binary segmentation of the satellite foreground. It is shown that by jointly training on different yet related tasks with extensive data augmentations on synthetic images only, the shared encoder learns features that are common across image domains that have fundamentally different visual characteristics compared to synthetic images. This work also introduces Online Domain Refinement (ODR) which refines the parameters of the normalization layers of SPNv2 on the target domain images online at deployment. Specifically, ODR performs self-supervised entropy minimization of the predicted satellite foreground, thereby improving the CNN's performance on the target domain images without their pose labels and with minimal computational efforts. The GitHub repository for SPNv2 will be made available in the near future.", "paper_url": "http://arxiv.org/abs/2203.04275v1", "pdf_url": "http://arxiv.org/pdf/2203.04275v1", "repo_url": null}, "2203.04251": {"publish_time": "2022-03-08", "title": "End-to-End Semi-Supervised Learning for Video Action Detection", "author": "Akash Kumar et.al.", "abstract": "In this work, we focus on semi-supervised learning for video action detection which utilizes both labeled as well as unlabeled data. We propose a simple end-to-end consistency based approach which effectively utilizes the unlabeled data. Video action detection requires both, action class prediction as well as a spatio-temporal localization of actions. Therefore, we investigate two types of constraints, classification consistency, and spatio-temporal consistency. The presence of predominant background and static regions in a video makes it challenging to utilize spatio-temporal consistency for action detection. To address this, we propose two novel regularization constraints for spatio-temporal consistency; 1) temporal coherency, and 2) gradient smoothness. Both these aspects exploit the temporal continuity of action in videos and are found to be effective for utilizing unlabeled videos for action detection. We demonstrate the effectiveness of the proposed approach on two different action detection benchmark datasets, UCF101-24 and JHMDB-21. In addition, we also show the effectiveness of the proposed approach for video object segmentation on the Youtube-VOS dataset which demonstrates its generalization capability to other tasks. The proposed approach achieves competitive performance by using merely 20% of annotations on UCF101-24 when compared with recent fully supervised methods. On UCF101-24, it improves the score by +8.9% and +11% at 0.5 f-mAP and v-mAP respectively, compared to supervised approach.", "paper_url": "http://arxiv.org/abs/2203.04251v1", "pdf_url": "http://arxiv.org/pdf/2203.04251v1", "repo_url": null}, "2203.04232": {"publish_time": "2022-03-08", "title": "A Lightweight and Detector-free 3D Single Object Tracker on Point Clouds", "author": "Yan Xia et.al.", "abstract": "Recent works on 3D single object tracking treat the tracking as a target-specific 3D detection task, where an off-the-shelf 3D detector is commonly employed for tracking. However, it is non-trivial to perform accurate target-specific detection since the point cloud of objects in raw LiDAR scans is usually sparse and incomplete. In this paper, we address this issue by explicitly leveraging temporal motion cues and propose DMT, a Detector-free Motion prediction based 3D Tracking network that totally removes the usage of complicated 3D detectors, which is lighter, faster, and more accurate than previous trackers. Specifically, the motion prediction module is firstly introduced to estimate a potential target center of the current frame in a point-cloud free way. Then, an explicit voting module is proposed to directly regress the 3D box from the estimated target center. Extensive experiments on KITTI and NuScenes datasets demonstrate that our DMT, without applying any complicated 3D detectors, can still achieve better performance (~10% improvement on the NuScenes dataset) and faster tracking speed (i.e., 72 FPS) than state-of-the-art approaches. Our codes will be released publicly.", "paper_url": "http://arxiv.org/abs/2203.04232v1", "pdf_url": "http://arxiv.org/pdf/2203.04232v1", "repo_url": null}, "2203.04946": {"publish_time": "2022-03-09", "title": "On the surprising tradeoff between ImageNet accuracy and perceptual similarity", "author": "Manoj Kumar et.al.", "abstract": "Perceptual distances between images, as measured in the space of pre-trained deep features, have outperformed prior low-level, pixel-based metrics on assessing image similarity. While the capabilities of older and less accurate models such as AlexNet and VGG to capture perceptual similarity are well known, modern and more accurate models are less studied. First, we observe a surprising inverse correlation between ImageNet accuracy and Perceptual Scores of modern networks such as ResNets, EfficientNets, and Vision Transformers: that is better classifiers achieve worse Perceptual Scores. Then, we perform a large-scale study and examine the ImageNet accuracy/Perceptual Score relationship on varying the depth, width, number of training steps, weight decay, label smoothing, and dropout. Higher accuracy improves Perceptual Score up to a certain point, but we uncover a Pareto frontier between accuracies and Perceptual Score in the mid-to-high accuracy regime. We explore this relationship further using distortion invariance, spatial frequency sensitivity, and alternative perceptual functions. Interestingly we discover shallow ResNets, trained for less than 5 epochs only on ImageNet, whose emergent Perceptual Score matches the prior best networks trained directly on supervised human perceptual judgements.", "paper_url": "http://arxiv.org/abs/2203.04946v1", "pdf_url": "http://arxiv.org/pdf/2203.04946v1", "repo_url": null}, "2203.04930": {"publish_time": "2022-03-09", "title": "Triangular Character Animation Sampling with Motion, Emotion, and Relation", "author": "Yizhou Zhao et.al.", "abstract": "Dramatic progress has been made in animating individual characters. However, we still lack automatic control over activities between characters, especially those involving interactions. In this paper, we present a novel energy-based framework to sample and synthesize animations by associating the characters' body motions, facial expressions, and social relations. We propose a Spatial-Temporal And-Or graph (ST-AOG), a stochastic grammar model, to encode the contextual relationship between motion, emotion, and relation, forming a triangle in a conditional random field. We train our model from a labeled dataset of two-character interactions. Experiments demonstrate that our method can recognize the social relation between two characters and sample new scenes of vivid motion and emotion using Markov Chain Monte Carlo (MCMC) given the social relation. Thus, our method can provide animators with an automatic way to generate 3D character animations, help synthesize interactions between Non-Player Characters (NPCs), and enhance machine emotion intelligence (EQ) in virtual reality (VR).", "paper_url": "http://arxiv.org/abs/2203.04930v1", "pdf_url": "http://arxiv.org/pdf/2203.04930v1", "repo_url": null}, "2203.04913": {"publish_time": "2022-03-09", "title": "Leveling Down in Computer Vision: Pareto Inefficiencies in Fair Deep Classifiers", "author": "Dominik Zietlow et.al.", "abstract": "Algorithmic fairness is frequently motivated in terms of a trade-off in which overall performance is decreased so as to improve performance on disadvantaged groups where the algorithm would otherwise be less accurate. Contrary to this, we find that applying existing fairness approaches to computer vision improve fairness by degrading the performance of classifiers across all groups (with increased degradation on the best performing groups).   Extending the bias-variance decomposition for classification to fairness, we theoretically explain why the majority of fairness classifiers designed for low capacity models should not be used in settings involving high-capacity models, a scenario common to computer vision. We corroborate this analysis with extensive experimental support that shows that many of the fairness heuristics used in computer vision also degrade performance on the most disadvantaged groups. Building on these insights, we propose an adaptive augmentation strategy that, uniquely, of all methods tested, improves performance for the disadvantaged groups.", "paper_url": "http://arxiv.org/abs/2203.04913v1", "pdf_url": "http://arxiv.org/pdf/2203.04913v1", "repo_url": null}, "2203.04908": {"publish_time": "2022-03-09", "title": "Rethinking data-driven point spread function modeling with a differentiable optical model", "author": "Tobias Liaudat et.al.", "abstract": "In astronomy, upcoming space telescopes with wide-field optical instruments have a spatially varying point spread function (PSF). Certain scientific goals require a high-fidelity estimation of the PSF at target positions where no direct measurement of the PSF is provided. Even though observations of the PSF are available at some positions of the field of view (FOV), they are undersampled, noisy, and integrated in wavelength in the instrument's passband. PSF modeling requires building a model from these observations that can infer a super-resolved PSF at any wavelength and any position in the FOV. Current data-driven PSF models can tackle spatial variations and super-resolution, but are not capable of capturing chromatic variations. Our model, coined WaveDiff, proposes a paradigm shift in the data-driven modeling of the point spread function field of telescopes. By adding a differentiable optical forward model into the modeling framework, we change the data-driven modeling space from the pixels to the wavefront. The proposed model relies on efficient automatic differentiation technology as well as modern stochastic first-order optimization techniques recently developed by the thriving machine-learning community. Our framework paves the way to building powerful models that are physically motivated and do not require special calibration data. This paper demonstrates the WaveDiff model on a simplified setting of a space telescope. The proposed framework represents a performance breakthrough with respect to existing data-driven approaches. The pixel reconstruction errors decrease 6-fold at observation resolution and 44-fold for a 3x super-resolution. The ellipticity errors are reduced by a factor of at least 20 and the size error by a factor of more than 250. By only using noisy broad-band in-focus observations, we successfully capture the PSF chromatic variations due to diffraction.", "paper_url": "http://arxiv.org/abs/2203.04908v1", "pdf_url": "http://arxiv.org/pdf/2203.04908v1", "repo_url": null}, "2203.04907": {"publish_time": "2022-03-09", "title": "Pose Guided Multi-person Image Generation From Text", "author": "Soon Yau Cheong et.al.", "abstract": "Transformers have recently been shown to generate high quality images from texts. However, existing methods struggle to create high fidelity full-body images, especially multiple people. A person's pose has a high degree of freedom that is difficult to describe using words only; this creates errors in the generated image, such as incorrect body proportions and pose. We propose a pose-guided text-to-image model, using pose as an additional input constraint. Using the proposed Keypoint Pose Encoding (KPE) to encode human pose into low dimensional representation, our model can generate novel multi-person images accurately representing the pose and text descriptions provided, with minimal errors. We demonstrate that KPE is invariant to changes in the target image domain and image resolution; we show results on the Deepfashion dataset and create a new multi-person Deepfashion dataset to demonstrate the multi-capabilities of our approach.", "paper_url": "http://arxiv.org/abs/2203.04907v1", "pdf_url": "http://arxiv.org/pdf/2203.04907v1", "repo_url": null}, "2203.05557": {"publish_time": "2022-03-10", "title": "Conditional Prompt Learning for Vision-Language Models", "author": "Kaiyang Zhou et.al.", "abstract": "With the rise of powerful pre-trained vision-language models like CLIP, it becomes essential to investigate ways to adapt these models to downstream datasets. A recently proposed method named Context Optimization (CoOp) introduces the concept of prompt learning -- a recent trend in NLP -- to the vision domain for adapting pre-trained vision-language models. Specifically, CoOp turns context words in a prompt into a set of learnable vectors and, with only a few labeled images for learning, can achieve huge improvements over intensively-tuned manual prompts. In our study we identify a critical problem of CoOp: the learned context is not generalizable to wider unseen classes within the same dataset, suggesting that CoOp overfits base classes observed during training. To address the problem, we propose Conditional Context Optimization (CoCoOp), which extends CoOp by further learning a lightweight neural network to generate for each image an input-conditional token (vector). Compared to CoOp's static prompts, our dynamic prompts adapt to each instance and are thus less sensitive to class shift. Extensive experiments show that CoCoOp generalizes much better than CoOp to unseen classes, even showing promising transferability beyond a single dataset; and yields stronger domain generalization performance as well. Code is available at https://github.com/KaiyangZhou/CoOp.", "paper_url": "http://arxiv.org/abs/2203.05557v1", "pdf_url": "http://arxiv.org/pdf/2203.05557v1", "repo_url": "https://github.com/kaiyangzhou/coop"}, "2203.05553": {"publish_time": "2022-03-10", "title": "Transfer of Representations to Video Label Propagation: Implementation Factors Matter", "author": "Daniel McKee et.al.", "abstract": "This work studies feature representations for dense label propagation in video, with a focus on recently proposed methods that learn video correspondence using self-supervised signals such as colorization or temporal cycle consistency. In the literature, these methods have been evaluated with an array of inconsistent settings, making it difficult to discern trends or compare performance fairly. Starting with a unified formulation of the label propagation algorithm that encompasses most existing variations, we systematically study the impact of important implementation factors in feature extraction and label propagation. Along the way, we report the accuracies of properly tuned supervised and unsupervised still image baselines, which are higher than those found in previous works. We also demonstrate that augmenting video-based correspondence cues with still-image-based ones can further improve performance. We then attempt a fair comparison of recent video-based methods on the DAVIS benchmark, showing convergence of best methods to performance levels near our strong ImageNet baseline, despite the usage of a variety of specialized video-based losses and training particulars. Additional comparisons on JHMDB and VIP datasets confirm the similar performance of current methods. We hope that this study will help to improve evaluation practices and better inform future research directions in temporal correspondence.", "paper_url": "http://arxiv.org/abs/2203.05553v1", "pdf_url": "http://arxiv.org/pdf/2203.05553v1", "repo_url": null}, "2203.05550": {"publish_time": "2022-03-10", "title": "An Empirical Investigation of 3D Anomaly Detection and Segmentation", "author": "Eliahu Horwitz et.al.", "abstract": "Anomaly detection and segmentation in images has made tremendous progress in recent years while 3D information has often been ignored. The objective of this paper is to further understand the benefit and role of 3D as opposed to color in image anomaly detection. Our study begins by presenting a surprising finding: standard color-only anomaly segmentation methods, when applied to 3D datasets, significantly outperform all current methods. On the other hand, we observe that color-only methods are insufficient for images containing geometric anomalies where shape cannot be unambiguously inferred from 2D. This suggests that better 3D methods are needed. We investigate different representations for 3D anomaly detection and discover that handcrafted orientation-invariant representations are unreasonably effective on this task. We uncover a simple 3D-only method that outperforms all recent approaches while not using deep learning, external pretraining datasets, or color information. As the 3D-only method cannot detect color and texture anomalies, we combine it with 2D color features, granting us the best current results by a large margin (Pixel-wise ROCAUC: 99.2%, PRO: 95.9% on MVTec 3D-AD). We conclude by discussing future challenges for 3D anomaly detection and segmentation.", "paper_url": "http://arxiv.org/abs/2203.05550v1", "pdf_url": "http://arxiv.org/pdf/2203.05550v1", "repo_url": null}, "2203.05534": {"publish_time": "2022-03-10", "title": "AGCN: Augmented Graph Convolutional Network for Lifelong Multi-label Image Recognition", "author": "Kaile Du et.al.", "abstract": "The Lifelong Multi-Label (LML) image recognition builds an online class-incremental classifier in a sequential multi-label image recognition data stream. The key challenges of LML image recognition are the construction of label relationships on Partial Labels of training data and the Catastrophic Forgetting on old classes, resulting in poor generalization. To solve the problems, the study proposes an Augmented Graph Convolutional Network (AGCN) model that can construct the label relationships across the sequential recognition tasks and sustain the catastrophic forgetting. First, we build an Augmented Correlation Matrix (ACM) across all seen classes, where the intra-task relationships derive from the hard label statistics while the inter-task relationships leverage both hard and soft labels from data and a constructed expert network. Then, based on the ACM, the proposed AGCN captures label dependencies with dynamic augmented structure and yields effective class representations. Last, to suppress the forgetting of label dependencies across old tasks, we propose a relationship-preserving loss as a constraint to the construction of label relationships. The proposed method is evaluated using two multi-label image benchmarks and the experimental results show that the proposed method is effective for LML image recognition and can build convincing correlation across tasks even if the labels of previous tasks are missing. Our code is available at https://github.com/Kaile-Du/AGCN.", "paper_url": "http://arxiv.org/abs/2203.05534v1", "pdf_url": "http://arxiv.org/pdf/2203.05534v1", "repo_url": null}, "2203.05508": {"publish_time": "2022-03-10", "title": "Towards Less Constrained Macro-Neural Architecture Search", "author": "Vasco Lopes et.al.", "abstract": "Networks found with Neural Architecture Search (NAS) achieve state-of-the-art performance in a variety of tasks, out-performing human-designed networks. However, most NAS methods heavily rely on human-defined assumptions that constrain the search: architecture's outer-skeletons, number of layers, parameter heuristics and search spaces. Additionally, common search spaces consist of repeatable modules (cells) instead of fully exploring the architecture's search space by designing entire architectures (macro-search). Imposing such constraints requires deep human expertise and restricts the search to pre-defined settings. In this paper, we propose LCMNAS, a method that pushes NAS to less constrained search spaces by performing macro-search without relying on pre-defined heuristics or bounded search spaces. LCMNAS introduces three components for the NAS pipeline: i) a method that leverages information about well-known architectures to autonomously generate complex search spaces based on Weighted Directed Graphs with hidden properties, ii) a evolutionary search strategy that generates complete architectures from scratch, and iii) a mixed-performance estimation approach that combines information about architectures at initialization stage and lower fidelity estimates to infer their trainability and capacity to model complex functions. We present experiments showing that LCMNAS generates state-of-the-art architectures from scratch with minimal GPU computation. We study the importance of different NAS components on a macro-search setting. Code for reproducibility is public at \\url{https://github.com/VascoLopes/LCMNAS}.", "paper_url": "http://arxiv.org/abs/2203.05508v1", "pdf_url": "http://arxiv.org/pdf/2203.05508v1", "repo_url": "https://github.com/vascolopes/lcmnas"}, "2203.06173": {"publish_time": "2022-03-11", "title": "Masked Visual Pre-training for Motor Control", "author": "Tete Xiao et.al.", "abstract": "This paper shows that self-supervised visual pre-training from real-world images is effective for learning motor control tasks from pixels. We first train the visual representations by masked modeling of natural images. We then freeze the visual encoder and train neural network controllers on top with reinforcement learning. We do not perform any task-specific fine-tuning of the encoder; the same visual representations are used for all motor control tasks. To the best of our knowledge, this is the first self-supervised model to exploit real-world images at scale for motor control. To accelerate progress in learning from pixels, we contribute a benchmark suite of hand-designed tasks varying in movements, scenes, and robots. Without relying on labels, state-estimation, or expert demonstrations, we consistently outperform supervised encoders by up to 80% absolute success rate, sometimes even matching the oracle state performance. We also find that in-the-wild images, e.g., from YouTube or Egocentric videos, lead to better visual representations for various manipulation tasks than ImageNet images.", "paper_url": "http://arxiv.org/abs/2203.06173v1", "pdf_url": "http://arxiv.org/pdf/2203.06173v1", "repo_url": null}, "2203.06172": {"publish_time": "2022-03-11", "title": "Deep AutoAugment", "author": "Yu Zheng et.al.", "abstract": "While recent automated data augmentation methods lead to state-of-the-art results, their design spaces and the derived data augmentation strategies still incorporate strong human priors. In this work, instead of fixing a set of hand-picked default augmentations alongside the searched data augmentations, we propose a fully automated approach for data augmentation search named Deep AutoAugment (DeepAA). DeepAA progressively builds a multi-layer data augmentation pipeline from scratch by stacking augmentation layers one at a time until reaching convergence. For each augmentation layer, the policy is optimized to maximize the cosine similarity between the gradients of the original and augmented data along the direction with low variance. Our experiments show that even without default augmentations, we can learn an augmentation policy that achieves strong performance with that of previous works. Extensive ablation studies show that the regularized gradient matching is an effective search method for data augmentation policies. Our code is available at: https://github.com/MSU-MLSys-Lab/DeepAA .", "paper_url": "http://arxiv.org/abs/2203.06172v1", "pdf_url": "http://arxiv.org/pdf/2203.06172v1", "repo_url": "https://github.com/msu-mlsys-lab/deepaa"}, "2203.06145": {"publish_time": "2022-03-11", "title": "Neuromorphic Data Augmentation for Training Spiking Neural Networks", "author": "Yuhang Li et.al.", "abstract": "Developing neuromorphic intelligence on event-based datasets with spiking neural networks (SNNs) has recently attracted much research attention. However, the limited size of event-based datasets makes SNNs prone to overfitting and unstable convergence. This issue remains unexplored by previous academic works. In an effort to minimize this generalization gap, we propose neuromorphic data augmentation (NDA), a family of geometric augmentations specifically designed for event-based datasets with the goal of significantly stabilizing the SNN training and reducing the generalization gap between training and test performance. The proposed method is simple and compatible with existing SNN training pipelines. Using the proposed augmentation, for the first time, we demonstrate the feasibility of unsupervised contrastive learning for SNNs. We conduct comprehensive experiments on prevailing neuromorphic vision benchmarks and show that NDA yields substantial improvements over previous state-of-the-art results. For example, NDA-based SNN achieves accuracy gain on CIFAR10-DVS and N-Caltech 101 by 10.1% and 13.7%, respectively.", "paper_url": "http://arxiv.org/abs/2203.06145v1", "pdf_url": "http://arxiv.org/pdf/2203.06145v1", "repo_url": null}, "2203.06127": {"publish_time": "2022-03-11", "title": "Spatial Consistency Loss for Training Multi-Label Classifiers from Single-Label Annotations", "author": "Thomas Verelst et.al.", "abstract": "As natural images usually contain multiple objects, multi-label image classification is more applicable \"in the wild\" than single-label classification. However, exhaustively annotating images with every object of interest is costly and time-consuming. We aim to train multi-label classifiers from single-label annotations only. We show that adding a consistency loss, ensuring that the predictions of the network are consistent over consecutive training epochs, is a simple yet effective method to train multi-label classifiers in a weakly supervised setting. We further extend this approach spatially, by ensuring consistency of the spatial feature maps produced over consecutive training epochs, maintaining per-class running-average heatmaps for each training image. We show that this spatial consistency loss further improves the multi-label mAP of the classifiers. In addition, we show that this method overcomes shortcomings of the \"crop\" data-augmentation by recovering correct supervision signal even when most of the single ground truth object is cropped out of the input image by the data augmentation. We demonstrate gains of the consistency and spatial consistency losses over the binary cross-entropy baseline, and over competing methods, on MS-COCO and Pascal VOC. We also demonstrate improved multi-label classification mAP on ImageNet-1K using the ReaL multi-label validation set.", "paper_url": "http://arxiv.org/abs/2203.06127v1", "pdf_url": "http://arxiv.org/pdf/2203.06127v1", "repo_url": null}, "2203.06113": {"publish_time": "2022-03-11", "title": "Detection of multiple retinal diseases in ultra-widefield fundus images using deep learning: data-driven identification of relevant regions", "author": "Justin Engelmann et.al.", "abstract": "Ultra-widefield (UWF) imaging is a promising modality that captures a larger retinal field of view compared to traditional fundus photography. Previous studies showed that deep learning (DL) models are effective for detecting retinal disease in UWF images, but primarily considered individual diseases under less-than-realistic conditions (excluding images with other diseases, artefacts, comorbidities, or borderline cases; and balancing healthy and diseased images) and did not systematically investigate which regions of the UWF images are relevant for disease detection. We first improve on the state of the field by proposing a DL model that can recognise multiple retinal diseases under more realistic conditions. We then use global explainability methods to identify which regions of the UWF images the model generally attends to. Our model performs very well, separating between healthy and diseased retinas with an area under the curve (AUC) of 0.9206 on an internal test set, and an AUC of 0.9841 on a challenging, external test set. When diagnosing specific diseases, the model attends to regions where we would expect those diseases to occur. We further identify the posterior pole as the most important region in a purely data-driven fashion. Surprisingly, 10% of the image around the posterior pole is sufficient for achieving comparable performance to having the full images available.", "paper_url": "http://arxiv.org/abs/2203.06113v1", "pdf_url": "http://arxiv.org/pdf/2203.06113v1", "repo_url": null}, "2203.07363": {"publish_time": "2022-03-14", "title": "Implicit Motion Handling for Video Camouflaged Object Detection", "author": "Xuelian Cheng et.al.", "abstract": "We propose a new video camouflaged object detection (VCOD) framework that can exploit both short-term dynamics and long-term temporal consistency to detect camouflaged objects from video frames. An essential property of camouflaged objects is that they usually exhibit patterns similar to the background and thus make them hard to identify from still images. Therefore, effectively handling temporal dynamics in videos becomes the key for the VCOD task as the camouflaged objects will be noticeable when they move. However, current VCOD methods often leverage homography or optical flows to represent motions, where the detection error may accumulate from both the motion estimation error and the segmentation error. On the other hand, our method unifies motion estimation and object segmentation within a single optimization framework. Specifically, we build a dense correlation volume to implicitly capture motions between neighbouring frames and utilize the final segmentation supervision to optimize the implicit motion estimation and segmentation jointly. Furthermore, to enforce temporal consistency within a video sequence, we jointly utilize a spatio-temporal transformer to refine the short-term predictions. Extensive experiments on VCOD benchmarks demonstrate the architectural effectiveness of our approach. We also provide a large-scale VCOD dataset named MoCA-Mask with pixel-level handcrafted ground-truth masks and construct a comprehensive VCOD benchmark with previous methods to facilitate research in this direction. Dataset Link: https://xueliancheng.github.io/SLT-Net-project.", "paper_url": "http://arxiv.org/abs/2203.07363v1", "pdf_url": "http://arxiv.org/pdf/2203.07363v1", "repo_url": null}, "2203.07345": {"publish_time": "2022-03-14", "title": "Federated Cycling (FedCy): Semi-supervised Federated Learning of Surgical Phases", "author": "Hasan Kassem et.al.", "abstract": "Recent advancements in deep learning methods bring computer-assistance a step closer to fulfilling promises of safer surgical procedures. However, the generalizability of such methods is often dependent on training on diverse datasets from multiple medical institutions, which is a restrictive requirement considering the sensitive nature of medical data. Recently proposed collaborative learning methods such as Federated Learning (FL) allow for training on remote datasets without the need to explicitly share data. Even so, data annotation still represents a bottleneck, particularly in medicine and surgery where clinical expertise is often required. With these constraints in mind, we propose FedCy, a federated semi-supervised learning (FSSL) method that combines FL and self-supervised learning to exploit a decentralized dataset of both labeled and unlabeled videos, thereby improving performance on the task of surgical phase recognition. By leveraging temporal patterns in the labeled data, FedCy helps guide unsupervised training on unlabeled data towards learning task-specific features for phase recognition. We demonstrate significant performance gains over state-of-the-art FSSL methods on the task of automatic recognition of surgical phases using a newly collected multi-institutional dataset of laparoscopic cholecystectomy videos. Furthermore, we demonstrate that our approach also learns more generalizable features when tested on data from an unseen domain.", "paper_url": "http://arxiv.org/abs/2203.07345v1", "pdf_url": "http://arxiv.org/pdf/2203.07345v1", "repo_url": null}, "2203.07341": {"publish_time": "2022-03-14", "title": "Defending From Physically-Realizable Adversarial Attacks Through Internal Over-Activation Analysis", "author": "Giulio Rossolini et.al.", "abstract": "This work presents Z-Mask, a robust and effective strategy to improve the adversarial robustness of convolutional networks against physically-realizable adversarial attacks. The presented defense relies on specific Z-score analysis performed on the internal network features to detect and mask the pixels corresponding to adversarial objects in the input image. To this end, spatially contiguous activations are examined in shallow and deep layers to suggest potential adversarial regions. Such proposals are then aggregated through a multi-thresholding mechanism. The effectiveness of Z-Mask is evaluated with an extensive set of experiments carried out on models for both semantic segmentation and object detection. The evaluation is performed with both digital patches added to the input images and printed patches positioned in the real world. The obtained results confirm that Z-Mask outperforms the state-of-the-art methods in terms of both detection accuracy and overall performance of the networks under attack. Additional experiments showed that Z-Mask is also robust against possible defense-aware attacks.", "paper_url": "http://arxiv.org/abs/2203.07341v1", "pdf_url": "http://arxiv.org/pdf/2203.07341v1", "repo_url": null}, "2203.07319": {"publish_time": "2022-03-14", "title": "GCFSR: a Generative and Controllable Face Super Resolution Method Without Facial and GAN Priors", "author": "Jingwen He et.al.", "abstract": "Face image super resolution (face hallucination) usually relies on facial priors to restore realistic details and preserve identity information. Recent advances can achieve impressive results with the help of GAN prior. They either design complicated modules to modify the fixed GAN prior or adopt complex training strategies to finetune the generator. In this work, we propose a generative and controllable face SR framework, called GCFSR, which can reconstruct images with faithful identity information without any additional priors. Generally, GCFSR has an encoder-generator architecture. Two modules called style modulation and feature modulation are designed for the multi-factor SR task. The style modulation aims to generate realistic face details and the feature modulation dynamically fuses the multi-level encoded features and the generated ones conditioned on the upscaling factor. The simple and elegant architecture can be trained from scratch in an end-to-end manner. For small upscaling factors (<=8), GCFSR can produce surprisingly good results with only adversarial loss. After adding L1 and perceptual losses, GCFSR can outperform state-of-the-art methods for large upscaling factors (16, 32, 64). During the test phase, we can modulate the generative strength via feature modulation by changing the conditional upscaling factor continuously to achieve various generative effects.", "paper_url": "http://arxiv.org/abs/2203.07319v1", "pdf_url": "http://arxiv.org/pdf/2203.07319v1", "repo_url": null}, "2203.07308": {"publish_time": "2022-03-14", "title": "Accelerating Plug-and-Play Image Reconstruction via Multi-Stage Sketched Gradients", "author": "Junqi Tang et.al.", "abstract": "In this work we propose a new paradigm for designing fast plug-and-play (PnP) algorithms using dimensionality reduction techniques. Unlike existing approaches which utilize stochastic gradient iterations for acceleration, we propose novel multi-stage sketched gradient iterations which first perform downsampling dimensionality reduction in the image space, and then efficiently approximate the true gradient using the sketched gradient in the low-dimensional space. This sketched gradient scheme can also be naturally combined with PnP-SGD methods for further improvement on computational complexity. As a generic acceleration scheme, it can be applied to accelerate any existing PnP/RED algorithm. Our numerical experiments on X-ray fan-beam CT demonstrate the remarkable effectiveness of our scheme, that a computational free-lunch can be obtained using this dimensionality reduction in the image space.", "paper_url": "http://arxiv.org/abs/2203.07308v1", "pdf_url": "http://arxiv.org/pdf/2203.07308v1", "repo_url": null}, "2203.08141": {"publish_time": "2022-03-15", "title": "Object Manipulation via Visual Target Localization", "author": "Kiana Ehsani et.al.", "abstract": "Object manipulation is a critical skill required for Embodied AI agents interacting with the world around them. Training agents to manipulate objects, poses many challenges. These include occlusion of the target object by the agent's arm, noisy object detection and localization, and the target frequently going out of view as the agent moves around in the scene. We propose Manipulation via Visual Object Location Estimation (m-VOLE), an approach that explores the environment in search for target objects, computes their 3D coordinates once they are located, and then continues to estimate their 3D locations even when the objects are not visible, thus robustly aiding the task of manipulating these objects throughout the episode. Our evaluations show a massive 3x improvement in success rate over a model that has access to the same sensory suite but is trained without the object location estimator, and our analysis shows that our agent is robust to noise in depth perception and agent localization. Importantly, our proposed approach relaxes several assumptions about idealized localization and perception that are commonly employed by recent works in embodied AI -- an important step towards training agents for object manipulation in the real world.", "paper_url": "http://arxiv.org/abs/2203.08141v1", "pdf_url": "http://arxiv.org/pdf/2203.08141v1", "repo_url": null}, "2203.08140": {"publish_time": "2022-03-15", "title": "Learning Spatio-Temporal Downsampling for Effective Video Upscaling", "author": "Xiaoyu Xiang et.al.", "abstract": "Downsampling is one of the most basic image processing operations. Improper spatio-temporal downsampling applied on videos can cause aliasing issues such as moir\\'e patterns in space and the wagon-wheel effect in time. Consequently, the inverse task of upscaling a low-resolution, low frame-rate video in space and time becomes a challenging ill-posed problem due to information loss and aliasing artifacts. In this paper, we aim to solve the space-time aliasing problem by learning a spatio-temporal downsampler. Towards this goal, we propose a neural network framework that jointly learns spatio-temporal downsampling and upsampling. It enables the downsampler to retain the key patterns of the original video and maximizes the reconstruction performance of the upsampler. To make the downsamping results compatible with popular image and video storage formats, the downsampling results are encoded to uint8 with a differentiable quantization layer. To fully utilize the space-time correspondences, we propose two novel modules for explicit temporal propagation and space-time feature rearrangement. Experimental results show that our proposed method significantly boosts the space-time reconstruction quality by preserving spatial textures and motion patterns in both downsampling and upscaling. Moreover, our framework enables a variety of applications, including arbitrary video resampling, blurry frame reconstruction, and efficient video storage.", "paper_url": "http://arxiv.org/abs/2203.08140v1", "pdf_url": "http://arxiv.org/pdf/2203.08140v1", "repo_url": null}, "2203.08138": {"publish_time": "2022-03-15", "title": "CryoAI: Amortized Inference of Poses for Ab Initio Reconstruction of 3D Molecular Volumes from Real Cryo-EM Images", "author": "Axel Levy et.al.", "abstract": "Cryo-electron microscopy (cryo-EM) has become a tool of fundamental importance in structural biology, helping us understand the basic building blocks of life. The algorithmic challenge of cryo-EM is to jointly estimate the unknown 3D poses and the 3D electron scattering potential of a biomolecule from millions of extremely noisy 2D images. Existing reconstruction algorithms, however, cannot easily keep pace with the rapidly growing size of cryo-EM datasets due to their high computational and memory cost. We introduce cryoAI, an ab initio reconstruction algorithm for homogeneous conformations that uses direct gradient-based optimization of particle poses and the electron scattering potential from single-particle cryo-EM data. CryoAI combines a learned encoder that predicts the poses of each particle image with a physics-based decoder to aggregate each particle image into an implicit representation of the scattering potential volume. This volume is stored in the Fourier domain for computational efficiency and leverages a modern coordinate network architecture for memory efficiency. Combined with a symmetrized loss function, this framework achieves results of a quality on par with state-of-the-art cryo-EM solvers for both simulated and experimental data, one order of magnitude faster for large datasets and with significantly lower memory requirements than existing methods.", "paper_url": "http://arxiv.org/abs/2203.08138v1", "pdf_url": "http://arxiv.org/pdf/2203.08138v1", "repo_url": null}, "2203.08133": {"publish_time": "2022-03-15", "title": "Animatable Neural Implicit Surfaces for Creating Avatars from Videos", "author": "Sida Peng et.al.", "abstract": "This paper aims to reconstruct an animatable human model from a video of very sparse camera views. Some recent works represent human geometry and appearance with neural radiance fields and utilize parametric human models to produce deformation fields for animation, which enables them to recover detailed 3D human models from videos. However, their reconstruction results tend to be noisy due to the lack of surface constraints on radiance fields. Moreover, as they generate the human appearance in 3D space, their rendering quality heavily depends on the accuracy of deformation fields. To solve these problems, we propose Animatable Neural Implicit Surface (AniSDF), which models the human geometry with a signed distance field and defers the appearance generation to the 2D image space with a 2D neural renderer. The signed distance field naturally regularizes the learned geometry, enabling the high-quality reconstruction of human bodies, which can be further used to improve the rendering speed. Moreover, the 2D neural renderer can be learned to compensate for geometric errors, making the rendering more robust to inaccurate deformations. Experiments on several datasets show that the proposed approach outperforms recent human reconstruction and synthesis methods by a large margin.", "paper_url": "http://arxiv.org/abs/2203.08133v1", "pdf_url": "http://arxiv.org/pdf/2203.08133v1", "repo_url": null}, "2203.08130": {"publish_time": "2022-03-15", "title": "One Network Doesn't Rule Them All: Moving Beyond Handcrafted Architectures in Self-Supervised Learning", "author": "Sharath Girish et.al.", "abstract": "The current literature on self-supervised learning (SSL) focuses on developing learning objectives to train neural networks more effectively on unlabeled data. The typical development process involves taking well-established architectures, e.g., ResNet demonstrated on ImageNet, and using them to evaluate newly developed objectives on downstream scenarios. While convenient, this does not take into account the role of architectures which has been shown to be crucial in the supervised learning literature. In this work, we establish extensive empirical evidence showing that a network architecture plays a significant role in SSL. We conduct a large-scale study with over 100 variants of ResNet and MobileNet architectures and evaluate them across 11 downstream scenarios in the SSL setting. We show that there is no one network that performs consistently well across the scenarios. Based on this, we propose to learn not only network weights but also architecture topologies in the SSL regime. We show that \"self-supervised architectures\" outperform popular handcrafted architectures (ResNet18 and MobileNetV2) while performing competitively with the larger and computationally heavy ResNet50 on major image classification benchmarks (ImageNet-1K, iNat2021, and more). Our results suggest that it is time to consider moving beyond handcrafted architectures in SSL and start thinking about incorporating architecture search into self-supervised learning objectives.", "paper_url": "http://arxiv.org/abs/2203.08130v1", "pdf_url": "http://arxiv.org/pdf/2203.08130v1", "repo_url": null}, "2203.08796": {"publish_time": "2022-03-16", "title": "A Continual Learning Framework for Adaptive Defect Classification and Inspection", "author": "Wenbo Sun et.al.", "abstract": "Machine-vision-based defect classification techniques have been widely adopted for automatic quality inspection in manufacturing processes. This article describes a general framework for classifying defects from high volume data batches with efficient inspection of unlabelled samples. The concept is to construct a detector to identify new defect types, send them to the inspection station for labelling, and dynamically update the classifier in an efficient manner that reduces both storage and computational needs imposed by data samples of previously observed batches. Both a simulation study on image classification and a case study on surface defect detection via 3D point clouds are performed to demonstrate the effectiveness of the proposed method.", "paper_url": "http://arxiv.org/abs/2203.08796v1", "pdf_url": "http://arxiv.org/pdf/2203.08796v1", "repo_url": null}, "2203.08795": {"publish_time": "2022-03-16", "title": "Zero Pixel Directional Boundary by Vector Transform", "author": "Edoardo Mello Rella et.al.", "abstract": "Boundaries are among the primary visual cues used by human and computer vision systems. One of the key problems in boundary detection is the label representation, which typically leads to class imbalance and, as a consequence, to thick boundaries that require non-differential post-processing steps to be thinned. In this paper, we re-interpret boundaries as 1-D surfaces and formulate a one-to-one vector transform function that allows for training of boundary prediction completely avoiding the class imbalance issue. Specifically, we define the boundary representation at any point as the unit vector pointing to the closest boundary surface. Our problem formulation leads to the estimation of direction as well as richer contextual information of the boundary, and, if desired, the availability of zero-pixel thin boundaries also at training time. Our method uses no hyper-parameter in the training loss and a fixed stable hyper-parameter at inference. We provide theoretical justification/discussions of the vector transform representation. We evaluate the proposed loss method using a standard architecture and show the excellent performance over other losses and representations on several datasets.", "paper_url": "http://arxiv.org/abs/2203.08795v1", "pdf_url": "http://arxiv.org/pdf/2203.08795v1", "repo_url": null}, "2203.08792": {"publish_time": "2022-03-16", "title": "PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical Research", "author": "R. James Cotton et.al.", "abstract": "There has been significant progress in machine learning algorithms for human pose estimation that may provide immense value in rehabilitation and movement sciences. However, there remain several challenges to routine use of these tools for clinical practice and translational research, including: 1) high technical barrier to entry, 2) rapidly evolving space of algorithms, 3) challenging algorithmic interdependencies, and 4) complex data management requirements between these components. To mitigate these barriers, we developed a human pose estimation pipeline that facilitates running state-of-the-art algorithms on data acquired in clinical context. Our system allows for running different implementations of several classes of algorithms and handles their interdependencies easily. These algorithm classes include subject identification and tracking, 2D keypoint detection, 3D joint location estimation, and estimating the pose of body models. The system uses a database to manage videos, intermediate analyses, and data for computations at each stage. It also provides tools for data visualization, including generating video overlays that also obscure faces to enhance privacy. Our goal in this work is not to train new algorithms, but to advance the use of cutting-edge human pose estimation algorithms for clinical and translation research. We show that this tool facilitates analyzing large numbers of videos of human movement ranging from gait laboratories analyses, to clinic and therapy visits, to people in the community. We also highlight limitations of these algorithms when applied to clinical populations in a rehabilitation setting.", "paper_url": "http://arxiv.org/abs/2203.08792v1", "pdf_url": "http://arxiv.org/pdf/2203.08792v1", "repo_url": "https://github.com/peabody124/posepipeline"}, "2203.08777": {"publish_time": "2022-03-16", "title": "Object discovery and representation networks", "author": "Olivier J. H\u00e9naff et.al.", "abstract": "The promise of self-supervised learning (SSL) is to leverage large amounts of unlabeled data to solve complex tasks. While there has been excellent progress with simple, image-level learning, recent methods have shown the advantage of including knowledge of image structure. However, by introducing hand-crafted image segmentations to define regions of interest, or specialized augmentation strategies, these methods sacrifice the simplicity and generality that makes SSL so powerful. Instead, we propose a self-supervised learning paradigm that discovers the structure encoded in these priors by itself. Our method, Odin, couples object discovery and representation networks to discover meaningful image segmentations without any supervision. The resulting learning paradigm is simpler, less brittle, and more general, and achieves state-of-the-art transfer learning results for object detection and instance segmentation on COCO, and semantic segmentation on PASCAL and Cityscapes, while strongly surpassing supervised pre-training for video segmentation on DAVIS.", "paper_url": "http://arxiv.org/abs/2203.08777v1", "pdf_url": "http://arxiv.org/pdf/2203.08777v1", "repo_url": null}, "2203.08765": {"publish_time": "2022-03-16", "title": "Efficient conditioned face animation using frontally-viewed embedding", "author": "Maxime Oquab et.al.", "abstract": "As the quality of few shot facial animation from landmarks increases, new applications become possible, such as ultra low bandwidth video chat compression with a high degree of realism. However, there are some important challenges to tackle in order to improve the experience in real world conditions. In particular, the current approaches fail to represent profile views without distortions, while running in a low compute regime. We focus on this key problem by introducing a multi-frames embedding dubbed Frontalizer to improve profile views rendering. In addition to this core improvement, we explore the learning of a latent code conditioning generations along with landmarks to better convey facial expressions. Our dense models achieves 22% of improvement in perceptual quality and 73% reduction of landmark error over the first order model baseline on a subset of DFDC videos containing head movements. Declined with mobile architectures, our models outperform the previous state-of-the-art (improving perceptual quality by more than 16% and reducing landmark error by more than 47% on two datasets) while running on real time on iPhone 8 with very low bandwidth requirements.", "paper_url": "http://arxiv.org/abs/2203.08765v1", "pdf_url": "http://arxiv.org/pdf/2203.08765v1", "repo_url": null}, "2203.09517": {"publish_time": "2022-03-17", "title": "TensoRF: Tensorial Radiance Fields", "author": "Anpei Chen et.al.", "abstract": "We present TensoRF, a novel approach to model and reconstruct radiance fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a scene as a 4D tensor, which represents a 3D voxel grid with per-voxel multi-channel features. Our central idea is to factorize the 4D scene tensor into multiple compact low-rank tensor components. We demonstrate that applying traditional CP decomposition -- that factorizes tensors into rank-one components with compact vectors -- in our framework leads to improvements over vanilla NeRF. To further boost performance, we introduce a novel vector-matrix (VM) decomposition that relaxes the low-rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors. Beyond superior rendering quality, our models with CP and VM decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per-voxel features. Experimentally, we demonstrate that TensoRF with CP decomposition achieves fast reconstruction (<30 min) with better rendering quality and even a smaller model size (<4 MB) compared to NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality and outperforms previous state-of-the-art methods, while reducing the reconstruction time (<10 min) and retaining a compact model size (<75 MB).", "paper_url": "http://arxiv.org/abs/2203.09517v1", "pdf_url": "http://arxiv.org/pdf/2203.09517v1", "repo_url": null}, "2203.09516": {"publish_time": "2022-03-17", "title": "AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation", "author": "Paritosh Mittal et.al.", "abstract": "Powerful priors allow us to perform inference with insufficient information. In this paper, we propose an autoregressive prior for 3D shapes to solve multimodal 3D tasks such as shape completion, reconstruction, and generation. We model the distribution over 3D shapes as a non-sequential autoregressive distribution over a discretized, low-dimensional, symbolic grid-like latent representation of 3D shapes. This enables us to represent distributions over 3D shapes conditioned on information from an arbitrary set of spatially anchored query locations and thus perform shape completion in such arbitrary settings (e.g., generating a complete chair given only a view of the back leg). We also show that the learned autoregressive prior can be leveraged for conditional tasks such as single-view reconstruction and language-based generation. This is achieved by learning task-specific naive conditionals which can be approximated by light-weight models trained on minimal paired data. We validate the effectiveness of the proposed method using both quantitative and qualitative evaluation and show that the proposed method outperforms the specialized state-of-the-art methods trained for individual tasks. The project page with code and video visualizations can be found at https://yccyenchicheng.github.io/AutoSDF/.", "paper_url": "http://arxiv.org/abs/2203.09516v1", "pdf_url": "http://arxiv.org/pdf/2203.09516v1", "repo_url": null}, "2203.09513": {"publish_time": "2022-03-17", "title": "On Multi-Domain Long-Tailed Recognition, Generalization and Beyond", "author": "Yuzhe Yang et.al.", "abstract": "Real-world data often exhibit imbalanced label distributions. Existing studies on data imbalance focus on single-domain settings, i.e., samples are from the same data distribution. However, natural data can originate from distinct domains, where a minority class in one domain could have abundant instances from other domains. We formalize the task of Multi-Domain Long-Tailed Recognition (MDLT), which learns from multi-domain imbalanced data, addresses label imbalance, domain shift, and divergent label distributions across domains, and generalizes to all domain-class pairs. We first develop the domain-class transferability graph, and show that such transferability governs the success of learning in MDLT. We then propose BoDA, a theoretically grounded learning strategy that tracks the upper bound of transferability statistics, and ensures balanced alignment and calibration across imbalanced domain-class distributions. We curate five MDLT benchmarks based on widely-used multi-domain datasets, and compare BoDA to twenty algorithms that span different learning strategies. Extensive and rigorous experiments verify the superior performance of BoDA. Further, as a byproduct, BoDA establishes new state-of-the-art on Domain Generalization benchmarks, improving generalization to unseen domains. Code and data are available at https://github.com/YyzHarry/multi-domain-imbalance.", "paper_url": "http://arxiv.org/abs/2203.09513v1", "pdf_url": "http://arxiv.org/pdf/2203.09513v1", "repo_url": "https://github.com/yyzharry/multi-domain-imbalance"}, "2203.09510": {"publish_time": "2022-03-17", "title": "DetMatch: Two Teachers are Better Than One for Joint 2D and 3D Semi-Supervised Object Detection", "author": "Jinhyung Park et.al.", "abstract": "While numerous 3D detection works leverage the complementary relationship between RGB images and point clouds, developments in the broader framework of semi-supervised object recognition remain uninfluenced by multi-modal fusion. Current methods develop independent pipelines for 2D and 3D semi-supervised learning despite the availability of paired image and point cloud frames. Observing that the distinct characteristics of each sensor cause them to be biased towards detecting different objects, we propose DetMatch, a flexible framework for joint semi-supervised learning on 2D and 3D modalities. By identifying objects detected in both sensors, our pipeline generates a cleaner, more robust set of pseudo-labels that both demonstrates stronger performance and stymies single-modality error propagation. Further, we leverage the richer semantics of RGB images to rectify incorrect 3D class predictions and improve localization of 3D boxes. Evaluating on the challenging KITTI and Waymo datasets, we improve upon strong semi-supervised learning methods and observe higher quality pseudo-labels. Code will be released at https://github.com/Divadi/DetMatch", "paper_url": "http://arxiv.org/abs/2203.09510v1", "pdf_url": "http://arxiv.org/pdf/2203.09510v1", "repo_url": "https://github.com/divadi/detmatch"}, "2203.09507": {"publish_time": "2022-03-17", "title": "Towards Data-Efficient Detection Transformers", "author": "Wen Wang et.al.", "abstract": "Detection Transformers have achieved competitive performance on the sample-rich COCO dataset. However, we show most of them suffer from significant performance drops on small-size datasets, like Cityscapes. In other words, the detection transformers are generally data-hungry. To tackle this problem, we empirically analyze the factors that affect data efficiency, through a step-by-step transition from a data-efficient RCNN variant to the representative DETR. The empirical results suggest that sparse feature sampling from local image areas holds the key. Based on this observation, we alleviate the data-hungry issue of existing detection transformers by simply alternating how key and value sequences are constructed in the cross-attention layer, with minimum modifications to the original models. Besides, we introduce a simple yet effective label augmentation method to provide richer supervision and improve data efficiency. Experiments show that our method can be readily applied to different detection transformers and improve their performance on both small-size and sample-rich datasets. Code will be made publicly available at \\url{https://github.com/encounter1997/DE-DETRs}.", "paper_url": "http://arxiv.org/abs/2203.09507v1", "pdf_url": "http://arxiv.org/pdf/2203.09507v1", "repo_url": null}, "2203.10078": {"publish_time": "2022-03-18", "title": "Bayesian Inversion for Nonlinear Imaging Models using Deep Generative Priors", "author": "Pakshal Bohra et.al.", "abstract": "Most modern imaging systems involve a computational reconstruction pipeline to infer the image of interest from acquired measurements. The Bayesian reconstruction framework relies on the characterization of the posterior distribution, which depends on a model of the imaging system and prior knowledge on the image, for solving such inverse problems. Here, the choice of the prior distribution is critical for obtaining high-quality estimates. In this work, we use deep generative models to represent the prior distribution. We develop a posterior sampling scheme for the class of nonlinear inverse problems where the forward model has a neural-network-like structure. This class includes most existing imaging modalities. We introduce the notion of augmented generative models in order to suitably handle quantitative image recovery. We illustrate the advantages of our framework by applying it to two nonlinear imaging modalities-phase retrieval and optical diffraction tomography.", "paper_url": "http://arxiv.org/abs/2203.10078v1", "pdf_url": "http://arxiv.org/pdf/2203.10078v1", "repo_url": null}, "2203.10073": {"publish_time": "2022-03-18", "title": "Lunar Rover Localization Using Craters as Landmarks", "author": "Larry Matthies et.al.", "abstract": "Onboard localization capabilities for planetary rovers to date have used relative navigation, by integrating combinations of wheel odometry, visual odometry, and inertial measurements during each drive to track position relative to the start of each drive. At the end of each drive, a ground-in-the-loop (GITL) interaction is used to get a position update from human operators in a more global reference frame, by matching images or local maps from onboard the rover to orbital reconnaissance images or maps of a large region around the rover's current position. Autonomous rover drives are limited in distance so that accumulated relative navigation error does not risk the possibility of the rover driving into hazards known from orbital images. However, several rover mission concepts have recently been studied that require much longer drives between GITL cycles, particularly for the Moon. These concepts require greater autonomy to minimize GITL cycles to enable such large range; onboard global localization is a key element of such autonomy. Multiple techniques have been studied in the past for onboard rover global localization, but a satisfactory solution has not yet emerged. For the Moon, the ubiquitous craters offer a new possibility, which involves mapping craters from orbit, then recognizing crater landmarks with cameras and-or a lidar onboard the rover. This approach is applicable everywhere on the Moon, does not require high resolution stereo imaging from orbit as some other approaches do, and has potential to enable position knowledge with order of 5 to 10 m accuracy at all times. This paper describes our technical approach to crater-based lunar rover localization and presents initial results on crater detection using 3D point cloud data from onboard lidar or stereo cameras, as well as using shading cues in monocular onboard imagery.", "paper_url": "http://arxiv.org/abs/2203.10073v1", "pdf_url": "http://arxiv.org/pdf/2203.10073v1", "repo_url": null}, "2203.10062": {"publish_time": "2022-03-18", "title": "Imaging-based histological features are predictive of MET alterations in Non-Small Cell Lung Cancer", "author": "Rohan P. Joshi et.al.", "abstract": "MET is a proto-oncogene whose somatic activation in non-small cell lung cancer leads to increased cell growth and tumor progression. The two major classes of MET alterations are gene amplification and exon 14 deletion, both of which are therapeutic targets and detectable using existing molecular assays. However, existing tests are limited by their consumption of valuable tissue, cost and complexity that prevent widespread use. MET alterations could have an effect on cell morphology, and quantifying these associations could open new avenues for research and development of morphology-based screening tools. Using H&E-stained whole slide images (WSIs), we investigated the association of distinct cell-morphological features with MET amplifications and MET exon 14 deletions. We found that cell shape, color, grayscale intensity and texture-based features from both tumor infiltrating lymphocytes and tumor cells distinguished MET wild-type from MET amplified or MET exon 14 deletion cases. The association of individual cell features with MET alterations suggested a predictive model could distinguish MET wild-type from MET amplification or MET exon 14 deletion. We therefore developed an L1-penalized logistic regression model, achieving a mean Area Under the Receiver Operating Characteristic Curve (ROC-AUC) of 0.77 +/- 0.05sd in cross-validation and 0.77 on an independent holdout test set. A sparse set of 43 features differentiated these classes, which included features similar to what was found in the univariate analysis as well as the percent of tumor cells in the tissue. Our study demonstrates that MET alterations result in a detectable morphological signal in tumor cells and lymphocytes. These results suggest that development of low-cost predictive models based on H&E-stained WSIs may improve screening for MET altered tumors.", "paper_url": "http://arxiv.org/abs/2203.10062v1", "pdf_url": "http://arxiv.org/pdf/2203.10062v1", "repo_url": null}, "2203.10039": {"publish_time": "2022-03-18", "title": "Multi-input segmentation of damaged brain in acute ischemic stroke patients using slow fusion with skip connection", "author": "Luca Tomasetti et.al.", "abstract": "Time is a fundamental factor during stroke treatments. A fast, automatic approach that segments the ischemic regions helps treatment decisions. In clinical use today, a set of color-coded parametric maps generated from computed tomography perfusion (CTP) images are investigated manually to decide a treatment plan. We propose an automatic method based on a neural network using a set of parametric maps to segment the two ischemic regions (core and penumbra) in patients affected by acute ischemic stroke. Our model is based on a convolution-deconvolution bottleneck structure with multi-input and slow fusion. A loss function based on the focal Tversky index addresses the data imbalance issue. The proposed architecture demonstrates effective performance and results comparable to the ground truth annotated by neuroradiologists. A Dice coefficient of 0.81 for penumbra and 0.52 for core over the large vessel occlusion test set is achieved. The full implementation is available at: https://git.io/JtFGb.", "paper_url": "http://arxiv.org/abs/2203.10039v1", "pdf_url": "http://arxiv.org/pdf/2203.10039v1", "repo_url": null}, "2203.10035": {"publish_time": "2022-03-18", "title": "SHREC 2021: Classification in cryo-electron tomograms", "author": "Ilja Gubins et.al.", "abstract": "Cryo-electron tomography (cryo-ET) is an imaging technique that allows three-dimensional visualization of macro-molecular assemblies under near-native conditions. Cryo-ET comes with a number of challenges, mainly low signal-to-noise and inability to obtain images from all angles. Computational methods are key to analyze cryo-electron tomograms.   To promote innovation in computational methods, we generate a novel simulated dataset to benchmark different methods of localization and classification of biological macromolecules in tomograms. Our publicly available dataset contains ten tomographic reconstructions of simulated cell-like volumes. Each volume contains twelve different types of complexes, varying in size, function and structure.   In this paper, we have evaluated seven different methods of finding and classifying proteins. Seven research groups present results obtained with learning-based methods and trained on the simulated dataset, as well as a baseline template matching (TM), a traditional method widely used in cryo-ET research. We show that learning-based approaches can achieve notably better localization and classification performance than TM. We also experimentally confirm that there is a negative relationship between particle size and performance for all methods.", "paper_url": "http://arxiv.org/abs/2203.10035v1", "pdf_url": "http://arxiv.org/pdf/2203.10035v1", "repo_url": null}, "2203.11194": {"publish_time": "2022-03-21", "title": "Generating Fast and Slow: Scene Decomposition via Reconstruction", "author": "Mihir Prabhudesai et.al.", "abstract": "We consider the problem of segmenting scenes into constituent entities, i.e. underlying objects and their parts. Current supervised visual detectors though impressive within their training distribution, often fail to segment out-of-distribution scenes into their constituent entities. Recent slot-centric generative models break such dependence on supervision, by attempting to segment scenes into entities unsupervised, by reconstructing pixels. However, they have been restricted thus far to toy scenes as they suffer from a reconstruction-segmentation trade-off: as the entity bottleneck gets wider, reconstruction improves but then the segmentation collapses. We propose GFS-Nets (Generating Fast and Slow Networks) that alleviate this issue with two ingredients: i) curriculum training in the form of primitives, often missing from current generative models and, ii) test-time adaptation per scene through gradient descent on the reconstruction objective, what we call slow inference, missing from current feed-forward detectors. We show the proposed curriculum suffices to break the reconstruction-segmentation trade-off, and slow inference greatly improves segmentation in out-of-distribution scenes. We evaluate GFS-Nets in 3D and 2D scene segmentation benchmarks of PartNet, CLEVR, Room Diverse++, and show large ( 50%) performance improvements against SOTA supervised feed-forward detectors and unsupervised object discovery methods", "paper_url": "http://arxiv.org/abs/2203.11194v1", "pdf_url": "http://arxiv.org/pdf/2203.11194v1", "repo_url": null}, "2203.11192": {"publish_time": "2022-03-21", "title": "Transforming Model Prediction for Tracking", "author": "Christoph Mayer et.al.", "abstract": "Optimization based tracking methods have been widely successful by integrating a target model prediction module, providing effective global reasoning by minimizing an objective function. While this inductive bias integrates valuable domain knowledge, it limits the expressivity of the tracking network. In this work, we therefore propose a tracker architecture employing a Transformer-based model prediction module. Transformers capture global relations with little inductive bias, allowing it to learn the prediction of more powerful target models. We further extend the model predictor to estimate a second set of weights that are applied for accurate bounding box regression. The resulting tracker relies on training and on test frame information in order to predict all weights transductively. We train the proposed tracker end-to-end and validate its performance by conducting comprehensive experiments on multiple tracking datasets. Our tracker sets a new state of the art on three benchmarks, achieving an AUC of 68.5% on the challenging LaSOT dataset.", "paper_url": "http://arxiv.org/abs/2203.11192v1", "pdf_url": "http://arxiv.org/pdf/2203.11192v1", "repo_url": null}, "2203.11191": {"publish_time": "2022-03-21", "title": "Robust Visual Tracking by Segmentation", "author": "Matthieu Paul et.al.", "abstract": "Estimating the target extent poses a fundamental challenge in visual object tracking. Typically, trackers are box-centric and fully rely on a bounding box to define the target in the scene. In practice, objects often have complex shapes and are not aligned with the image axis. In these cases, bounding boxes do not provide an accurate description of the target and often contain a majority of background pixels. We propose a segmentation-centric tracking pipeline that not only produces a highly accurate segmentation mask, but also works internally with segmentation masks instead of bounding boxes. Thus, our tracker is able to better learn a target representation that clearly differentiates the target in the scene from background content. In order to achieve the necessary robustness for the challenging tracking scenario, we propose a separate instance localization component that is used to condition the segmentation decoder when producing the output mask. We infer a bounding box from the segmentation mask and validate our tracker on challenging tracking datasets and achieve the new state of the art on LaSOT with a success AUC score of 69.7%. Since fully evaluating the predicted masks on tracking datasets is not possible due to the missing mask annotations, we further validate our segmentation quality on two popular video object segmentation datasets.", "paper_url": "http://arxiv.org/abs/2203.11191v1", "pdf_url": "http://arxiv.org/pdf/2203.11191v1", "repo_url": null}, "2203.11183": {"publish_time": "2022-03-21", "title": "Masked Discrimination for Self-Supervised Learning on Point Clouds", "author": "Haotian Liu et.al.", "abstract": "Masked autoencoding has achieved great success for self-supervised learning in the image and language domains. However, mask based pretraining has yet to show benefits for point cloud understanding, likely due to standard backbones like PointNet being unable to properly handle the training versus testing distribution mismatch introduced by masking during training. In this paper, we bridge this gap by proposing a discriminative mask pretraining Transformer framework, MaskPoint}, for point clouds. Our key idea is to represent the point cloud as discrete occupancy values (1 if part of the point cloud; 0 if not), and perform simple binary classification between masked object points and sampled noise points as the proxy task. In this way, our approach is robust to the point sampling variance in point clouds, and facilitates learning rich representations. We evaluate our pretrained models across several downstream tasks, including 3D shape classification, segmentation, and real-word object detection, and demonstrate state-of-the-art results while achieving a significant pretraining speedup (e.g., 4.1x on ScanNet) compared to the prior state-of-the-art Transformer baseline. Code will be publicly available at https://github.com/haotian-liu/MaskPoint.", "paper_url": "http://arxiv.org/abs/2203.11183v1", "pdf_url": "http://arxiv.org/pdf/2203.11183v1", "repo_url": null}, "2203.11174": {"publish_time": "2022-03-21", "title": "DiffPoseNet: Direct Differentiable Camera Pose Estimation", "author": "Chethan M. Parameshwara et.al.", "abstract": "Current deep neural network approaches for camera pose estimation rely on scene structure for 3D motion estimation, but this decreases the robustness and thereby makes cross-dataset generalization difficult. In contrast, classical approaches to structure from motion estimate 3D motion utilizing optical flow and then compute depth. Their accuracy, however, depends strongly on the quality of the optical flow. To avoid this issue, direct methods have been proposed, which separate 3D motion from depth estimation but compute 3D motion using only image gradients in the form of normal flow. In this paper, we introduce a network NFlowNet, for normal flow estimation which is used to enforce robust and direct constraints. In particular, normal flow is used to estimate relative camera pose based on the cheirality (depth positivity) constraint. We achieve this by formulating the optimization problem as a differentiable cheirality layer, which allows for end-to-end learning of camera pose. We perform extensive qualitative and quantitative evaluation of the proposed DiffPoseNet's sensitivity to noise and its generalization across datasets. We compare our approach to existing state-of-the-art methods on KITTI, TartanAir, and TUM-RGBD datasets.", "paper_url": "http://arxiv.org/abs/2203.11174v1", "pdf_url": "http://arxiv.org/pdf/2203.11174v1", "repo_url": null}, "2203.11938": {"publish_time": "2022-03-22", "title": "\u03c6-SfT: Shape-from-Template with a Physics-Based Deformation Model", "author": "Navami Kairanda et.al.", "abstract": "Shape-from-Template (SfT) methods estimate 3D surface deformations from a single monocular RGB camera while assuming a 3D state known in advance (a template). This is an important yet challenging problem due to the under-constrained nature of the monocular setting. Existing SfT techniques predominantly use geometric and simplified deformation models, which often limits their reconstruction abilities. In contrast to previous works, this paper proposes a new SfT approach explaining 2D observations through physical simulations accounting for forces and material properties. Our differentiable physics simulator regularises the surface evolution and optimises the material elastic properties such as bending coefficients, stretching stiffness and density. We use a differentiable renderer to minimise the dense reprojection error between the estimated 3D states and the input images and recover the deformation parameters using an adaptive gradient-based optimisation. For the evaluation, we record with an RGB-D camera challenging real surfaces exposed to physical forces with various material properties and textures. Our approach significantly reduces the 3D reconstruction error compared to multiple competing methods. For the source code and data, see https://4dqv.mpi-inf.mpg.de/phi-SfT/.", "paper_url": "http://arxiv.org/abs/2203.11938v1", "pdf_url": "http://arxiv.org/pdf/2203.11938v1", "repo_url": null}, "2203.11937": {"publish_time": "2022-03-22", "title": "4D-OR: Semantic Scene Graphs for OR Domain Modeling", "author": "Ege \u00d6zsoy et.al.", "abstract": "Surgical procedures are conducted in highly complex operating rooms (OR), comprising different actors, devices, and interactions. To date, only medically trained human experts are capable of understanding all the links and interactions in such a demanding environment. This paper aims to bring the community one step closer to automated, holistic and semantic understanding and modeling of OR domain. Towards this goal, for the first time, we propose using semantic scene graphs (SSG) to describe and summarize the surgical scene. The nodes of the scene graphs represent different actors and objects in the room, such as medical staff, patients, and medical equipment, whereas edges are the relationships between them. To validate the possibilities of the proposed representation, we create the first publicly available 4D surgical SSG dataset, 4D-OR, containing ten simulated total knee replacement surgeries recorded with six RGB-D sensors in a realistic OR simulation center. 4D-OR includes 6734 frames and is richly annotated with SSGs, human and object poses, and clinical roles. We propose an end-to-end neural network-based SSG generation pipeline, with a rate of success of 0.75 macro F1, indeed being able to infer semantic reasoning in the OR. We further demonstrate the representation power of our scene graphs by using it for the problem of clinical role prediction, where we achieve 0.85 macro F1. The code and dataset will be made available upon acceptance.", "paper_url": "http://arxiv.org/abs/2203.11937v1", "pdf_url": "http://arxiv.org/pdf/2203.11937v1", "repo_url": null}, "2203.11933": {"publish_time": "2022-03-22", "title": "A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning", "author": "Hugo Berg et.al.", "abstract": "Vision-language models can encode societal biases and stereotypes, but there are challenges to measuring and mitigating these harms. Prior proposed bias measurements lack robustness and feature degradation occurs when mitigating bias without access to pretraining data. We address both of these challenges in this paper: First, we evaluate different bias measures and propose the use of retrieval metrics to image-text representations via a bias measuring framework. Second, we investigate debiasing methods and show that optimizing for adversarial loss via learnable token embeddings minimizes various bias measures without substantially degrading feature representations.", "paper_url": "http://arxiv.org/abs/2203.11933v1", "pdf_url": "http://arxiv.org/pdf/2203.11933v1", "repo_url": "https://github.com/oxai/debias-vision-lang"}, "2203.11934": {"publish_time": "2022-03-22", "title": "Learning from All Vehicles", "author": "Dian Chen et.al.", "abstract": "In this paper, we present a system to train driving policies from experiences collected not just from the ego-vehicle, but all vehicles that it observes. This system uses the behaviors of other agents to create more diverse driving scenarios without collecting additional data. The main difficulty in learning from other vehicles is that there is no sensor information. We use a set of supervisory tasks to learn an intermediate representation that is invariant to the viewpoint of the controlling vehicle. This not only provides a richer signal at training time but also allows more complex reasoning during inference. Learning how all vehicles drive helps predict their behavior at test time and can avoid collisions. We evaluate this system in closed-loop driving simulations. Our system outperforms all prior methods on the public CARLA Leaderboard by a wide margin, improving driving score by 25 and route completion rate by 24 points. Our method won the 2021 CARLA Autonomous Driving challenge. Demo videos are available at https://dotchen.github.io/LAV/.", "paper_url": "http://arxiv.org/abs/2203.11934v1", "pdf_url": "http://arxiv.org/pdf/2203.11934v1", "repo_url": "https://github.com/dotchen/LAV"}, "2203.11932": {"publish_time": "2022-03-22", "title": "Dataset Distillation by Matching Training Trajectories", "author": "George Cazenavette et.al.", "abstract": "Dataset distillation is the task of synthesizing a small dataset such that a model trained on the synthetic set will match the test accuracy of the model trained on the full dataset. In this paper, we propose a new formulation that optimizes our distilled data to guide networks to a similar state as those trained on real data across many training steps. Given a network, we train it for several iterations on our distilled data and optimize the distilled data with respect to the distance between the synthetically trained parameters and the parameters trained on real data. To efficiently obtain the initial and target network parameters for large-scale datasets, we pre-compute and store training trajectories of expert networks trained on the real dataset. Our method handily outperforms existing methods and also allows us to distill higher-resolution visual data.", "paper_url": "http://arxiv.org/abs/2203.11932v1", "pdf_url": "http://arxiv.org/pdf/2203.11932v1", "repo_url": "https://github.com/georgecazenavette/mtt-distillation"}, "2203.12614": {"publish_time": "2022-03-23", "title": "Unsupervised Salient Object Detection with Spectral Cluster Voting", "author": "Gyungin Shin et.al.", "abstract": "In this paper, we tackle the challenging task of unsupervised salient object detection (SOD) by leveraging spectral clustering on self-supervised features. We make the following contributions: (i) We revisit spectral clustering and demonstrate its potential to group the pixels of salient objects; (ii) Given mask proposals from multiple applications of spectral clustering on image features computed from various self-supervised models, e.g., MoCov2, SwAV, DINO, we propose a simple but effective winner-takes-all voting mechanism for selecting the salient masks, leveraging object priors based on framing and distinctiveness; (iii) Using the selected object segmentation as pseudo groundtruth masks, we train a salient object detector, dubbed SelfMask, which outperforms prior approaches on three unsupervised SOD benchmarks. Code is publicly available at https://github.com/NoelShin/selfmask.", "paper_url": "http://arxiv.org/abs/2203.12614v1", "pdf_url": "http://arxiv.org/pdf/2203.12614v1", "repo_url": "https://github.com/noelshin/selfmask"}, "2203.12613": {"publish_time": "2022-03-23", "title": "A Hybrid Mesh-neural Representation for 3D Transparent Object Reconstruction", "author": "Jiamin Xu et.al.", "abstract": "We propose a novel method to reconstruct the 3D shapes of transparent objects using hand-held captured images under natural light conditions. It combines the advantage of explicit mesh and multi-layer perceptron (MLP) network, a hybrid representation, to simplify the capture setting used in recent contributions. After obtaining an initial shape through the multi-view silhouettes, we introduce surface-based local MLPs to encode the vertex displacement field (VDF) for the reconstruction of surface details. The design of local MLPs allows to represent the VDF in a piece-wise manner using two layer MLP networks, which is beneficial to the optimization algorithm. Defining local MLPs on the surface instead of the volume also reduces the searching space. Such a hybrid representation enables us to relax the ray-pixel correspondences that represent the light path constraint to our designed ray-cell correspondences, which significantly simplifies the implementation of single-image based environment matting algorithm. We evaluate our representation and reconstruction algorithm on several transparent objects with ground truth models. Our experiments show that our method can produce high-quality reconstruction results superior to state-of-the-art methods using a simplified data acquisition setup.", "paper_url": "http://arxiv.org/abs/2203.12613v1", "pdf_url": "http://arxiv.org/pdf/2203.12613v1", "repo_url": null}, "2203.12612": {"publish_time": "2022-03-23", "title": "StructToken : Rethinking Semantic Segmentation with Structural Prior", "author": "Fangjian Lin et.al.", "abstract": "In this paper, we present structure token (StructToken), a new paradigm for semantic segmentation. From a perspective on semantic segmentation as per-pixel classification, the previous deep learning-based methods learn the per-pixel representation first through an encoder and a decoder head and then classify each pixel representation to a specific category to obtain the semantic masks. Differently, we propose a structure-aware algorithm that takes structural information as prior to predict semantic masks directly without per-pixel classification. Specifically, given an input image, the learnable structure token interacts with the image representations to reason the final semantic masks. Three interaction approaches are explored and the results not only outperform the state-of-the-art methods but also contain more structural information. Experiments are conducted on three widely used datasets including ADE20k, Cityscapes, and COCO-Stuff 10K. We hope that structure token could serve as an alternative for semantic segmentation and inspire future research.", "paper_url": "http://arxiv.org/abs/2203.12612v1", "pdf_url": "http://arxiv.org/pdf/2203.12612v1", "repo_url": null}, "2203.12609": {"publish_time": "2022-03-23", "title": "Improving the Fairness of Chest X-ray Classifiers", "author": "Haoran Zhang et.al.", "abstract": "Deep learning models have reached or surpassed human-level performance in the field of medical imaging, especially in disease diagnosis using chest x-rays. However, prior work has found that such classifiers can exhibit biases in the form of gaps in predictive performance across protected groups. In this paper, we question whether striving to achieve zero disparities in predictive performance (i.e. group fairness) is the appropriate fairness definition in the clinical setting, over minimax fairness, which focuses on maximizing the performance of the worst-case group. We benchmark the performance of nine methods in improving classifier fairness across these two definitions. We find, consistent with prior work on non-clinical data, that methods which strive to achieve better worst-group performance do not outperform simple data balancing. We also find that methods which achieve group fairness do so by worsening performance for all groups. In light of these results, we discuss the utility of fairness definitions in the clinical setting, advocating for an investigation of the bias-inducing mechanisms in the underlying data generating process whenever possible.", "paper_url": "http://arxiv.org/abs/2203.12609v1", "pdf_url": "http://arxiv.org/pdf/2203.12609v1", "repo_url": "https://github.com/mlforhealth/cxr_fairness"}, "2203.12602": {"publish_time": "2022-03-23", "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training", "author": "Zhan Tong et.al.", "abstract": "Pre-training video transformers on extra large-scale datasets is generally required to achieve premier performance on relatively small datasets. In this paper, we show that video masked autoencoders (VideoMAE) are data-efficient learners for self-supervised video pre-training (SSVP). We are inspired by the recent ImageMAE and propose customized video tube masking and reconstruction. These simple designs turn out to be effective for overcoming information leakage caused by the temporal correlation during video reconstruction. We obtain three important findings on SSVP: (1) An extremely high proportion of masking ratio (i.e., 90% to 95%) still yields favorable performance of VideoMAE. The temporally redundant video content enables higher masking ratio than that of images. (2) VideoMAE achieves impressive results on very small datasets (i.e., around 3k-4k videos) without using any extra data. This is partially ascribed to the challenging task of video reconstruction to enforce high-level structure learning. (3) VideoMAE shows that data quality is more important than data quantity for SSVP. Domain shift between pre-training and target datasets are important issues in SSVP. Notably, our VideoMAE with the vanilla ViT backbone can achieve 83.9% on Kinects-400, 75.3% on Something-Something V2, 90.8% on UCF101, and 61.1% on HMDB51 without using any extra data. Code will be released at https://github.com/MCG-NJU/VideoMAE.", "paper_url": "http://arxiv.org/abs/2203.12602v1", "pdf_url": "http://arxiv.org/pdf/2203.12602v1", "repo_url": null}, "2203.13254": {"publish_time": "2022-03-24", "title": "EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation", "author": "Hansheng Chen et.al.", "abstract": "Locating 3D objects from a single RGB image via Perspective-n-Points (PnP) is a long-standing problem in computer vision. Driven by end-to-end deep learning, recent studies suggest interpreting PnP as a differentiable layer, so that 2D-3D point correspondences can be partly learned by backpropagating the gradient w.r.t. object pose. Yet, learning the entire set of unrestricted 2D-3D points from scratch fails to converge with existing approaches, since the deterministic pose is inherently non-differentiable. In this paper, we propose the EPro-PnP, a probabilistic PnP layer for general end-to-end pose estimation, which outputs a distribution of pose on the SE(3) manifold, essentially bringing categorical Softmax to the continuous domain. The 2D-3D coordinates and corresponding weights are treated as intermediate variables learned by minimizing the KL divergence between the predicted and target pose distribution. The underlying principle unifies the existing approaches and resembles the attention mechanism. EPro-PnP significantly outperforms competitive baselines, closing the gap between PnP-based method and the task-specific leaders on the LineMOD 6DoF pose estimation and nuScenes 3D object detection benchmarks.", "paper_url": "http://arxiv.org/abs/2203.13254v1", "pdf_url": "http://arxiv.org/pdf/2203.13254v1", "repo_url": "https://github.com/tjiiv-cprg/epro-pnp"}, "2203.13253": {"publish_time": "2022-03-24", "title": "Video Instance Segmentation via Multi-scale Spatio-temporal Split Attention Transformer", "author": "Omkar Thawakar et.al.", "abstract": "State-of-the-art transformer-based video instance segmentation (VIS) approaches typically utilize either single-scale spatio-temporal features or per-frame multi-scale features during the attention computations. We argue that such an attention computation ignores the multi-scale spatio-temporal feature relationships that are crucial to tackle target appearance deformations in videos. To address this issue, we propose a transformer-based VIS framework, named MS-STS VIS, that comprises a novel multi-scale spatio-temporal split (MS-STS) attention module in the encoder. The proposed MS-STS module effectively captures spatio-temporal feature relationships at multiple scales across frames in a video. We further introduce an attention block in the decoder to enhance the temporal consistency of the detected instances in different frames of a video. Moreover, an auxiliary discriminator is introduced during training to ensure better foreground-background separability within the multi-scale spatio-temporal feature space. We conduct extensive experiments on two benchmarks: Youtube-VIS (2019 and 2021). Our MS-STS VIS achieves state-of-the-art performance on both benchmarks. When using the ResNet50 backbone, our MS-STS achieves a mask AP of 50.1 %, outperforming the best reported results in literature by 2.7 % and by 4.8 % at higher overlap threshold of AP_75, while being comparable in model size and speed on Youtube-VIS 2019 val. set. When using the Swin Transformer backbone, MS-STS VIS achieves mask AP of 61.0 % on Youtube-VIS 2019 val. set. Our code and models are available at https://github.com/OmkarThawakar/MSSTS-VIS.", "paper_url": "http://arxiv.org/abs/2203.13253v1", "pdf_url": "http://arxiv.org/pdf/2203.13253v1", "repo_url": "https://github.com/OmkarThawakar/MSSTS-VIS"}, "2203.13251": {"publish_time": "2022-03-24", "title": "Dexterous Imitation Made Easy: A Learning-Based Framework for Efficient Dexterous Manipulation", "author": "Sridhar Pandian Arunachalam et.al.", "abstract": "Optimizing behaviors for dexterous manipulation has been a longstanding challenge in robotics, with a variety of methods from model-based control to model-free reinforcement learning having been previously explored in literature. Perhaps one of the most powerful techniques to learn complex manipulation strategies is imitation learning. However, collecting and learning from demonstrations in dexterous manipulation is quite challenging. The complex, high-dimensional action-space involved with multi-finger control often leads to poor sample efficiency of learning-based methods. In this work, we propose 'Dexterous Imitation Made Easy' (DIME) a new imitation learning framework for dexterous manipulation. DIME only requires a single RGB camera to observe a human operator and teleoperate our robotic hand. Once demonstrations are collected, DIME employs standard imitation learning methods to train dexterous manipulation policies. On both simulation and real robot benchmarks we demonstrate that DIME can be used to solve complex, in-hand manipulation tasks such as 'flipping', 'spinning', and 'rotating' objects with the Allegro hand. Our framework along with pre-collected demonstrations is publicly available at https://nyu-robot-learning.github.io/dime.", "paper_url": "http://arxiv.org/abs/2203.13251v1", "pdf_url": "http://arxiv.org/pdf/2203.13251v1", "repo_url": null}, "2203.13250": {"publish_time": "2022-03-24", "title": "Global Tracking Transformers", "author": "Xingyi Zhou et.al.", "abstract": "We present a novel transformer-based architecture for global multi-object tracking. Our network takes a short sequence of frames as input and produces global trajectories for all objects. The core component is a global tracking transformer that operates on objects from all frames in the sequence. The transformer encodes object features from all frames, and uses trajectory queries to group them into trajectories. The trajectory queries are object features from a single frame and naturally produce unique trajectories. Our global tracking transformer does not require intermediate pairwise grouping or combinatorial association, and can be jointly trained with an object detector. It achieves competitive performance on the popular MOT17 benchmark, with 75.3 MOTA and 59.1 HOTA. More importantly, our framework seamlessly integrates into state-of-the-art large-vocabulary detectors to track any objects. Experiments on the challenging TAO dataset show that our framework consistently improves upon baselines that are based on pairwise association, outperforming published works by a significant 7.7 tracking mAP. Code is available at https://github.com/xingyizhou/GTR.", "paper_url": "http://arxiv.org/abs/2203.13250v1", "pdf_url": "http://arxiv.org/pdf/2203.13250v1", "repo_url": "https://github.com/xingyizhou/GTR"}, "2203.13249": {"publish_time": "2022-03-24", "title": "BigDetection: A Large-scale Benchmark for Improved Object Detector Pre-training", "author": "Likun Cai et.al.", "abstract": "Multiple datasets and open challenges for object detection have been introduced in recent years. To build more general and powerful object detection systems, in this paper, we construct a new large-scale benchmark termed BigDetection. Our goal is to simply leverage the training data from existing datasets (LVIS, OpenImages and Object365) with carefully designed principles, and curate a larger dataset for improved detector pre-training. Specifically, we generate a new taxonomy which unifies the heterogeneous label spaces from different sources. Our BigDetection dataset has 600 object categories and contains over 3.4M training images with 36M bounding boxes. It is much larger in multiple dimensions than previous benchmarks, which offers both opportunities and challenges. Extensive experiments demonstrate its validity as a new benchmark for evaluating different object detection methods, and its effectiveness as a pre-training dataset.", "paper_url": "http://arxiv.org/abs/2203.13249v1", "pdf_url": "http://arxiv.org/pdf/2203.13249v1", "repo_url": "https://github.com/amazon-research/bigdetection"}, "2203.13817": {"publish_time": "2022-03-25", "title": "AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling", "author": "Ziqian Bai et.al.", "abstract": "Neural fields such as implicit surfaces have recently enabled avatar modeling from raw scans without explicit temporal correspondences. In this work, we exploit autoregressive modeling to further extend this notion to capture dynamic effects, such as soft-tissue deformations. Although autoregressive models are naturally capable of handling dynamics, it is non-trivial to apply them to implicit representations, as explicit state decoding is infeasible due to prohibitive memory requirements. In this work, for the first time, we enable autoregressive modeling of implicit avatars. To reduce the memory bottleneck and efficiently model dynamic implicit surfaces, we introduce the notion of articulated observer points, which relate implicit states to the explicit surface of a parametric human body model. We demonstrate that encoding implicit surfaces as a set of height fields defined on articulated observer points leads to significantly better generalization compared to a latent representation. The experiments show that our approach outperforms the state of the art, achieving plausible dynamic deformations even for unseen motions. https://zqbai-jeremy.github.io/autoavatar", "paper_url": "http://arxiv.org/abs/2203.13817v1", "pdf_url": "http://arxiv.org/pdf/2203.13817v1", "repo_url": null}, "2203.13815": {"publish_time": "2022-03-25", "title": "Versatile Multi-Modal Pre-Training for Human-Centric Perception", "author": "Fangzhou Hong et.al.", "abstract": "Human-centric perception plays a vital role in vision and graphics. But their data annotations are prohibitively expensive. Therefore, it is desirable to have a versatile pre-train model that serves as a foundation for data-efficient downstream tasks transfer. To this end, we propose the Human-Centric Multi-Modal Contrastive Learning framework HCMoCo that leverages the multi-modal nature of human data (e.g. RGB, depth, 2D keypoints) for effective representation learning. The objective comes with two main challenges: dense pre-train for multi-modality data, efficient usage of sparse human priors. To tackle the challenges, we design the novel Dense Intra-sample Contrastive Learning and Sparse Structure-aware Contrastive Learning targets by hierarchically learning a modal-invariant latent space featured with continuous and ordinal feature distribution and structure-aware semantic consistency. HCMoCo provides pre-train for different modalities by combining heterogeneous datasets, which allows efficient usage of existing task-specific human data. Extensive experiments on four downstream tasks of different modalities demonstrate the effectiveness of HCMoCo, especially under data-efficient settings (7.16% and 12% improvement on DensePose Estimation and Human Parsing). Moreover, we demonstrate the versatility of HCMoCo by exploring cross-modality supervision and missing-modality inference, validating its strong ability in cross-modal association and reasoning.", "paper_url": "http://arxiv.org/abs/2203.13815v1", "pdf_url": "http://arxiv.org/pdf/2203.13815v1", "repo_url": null}, "2203.13812": {"publish_time": "2022-03-25", "title": "Spatially Multi-conditional Image Generation", "author": "Ritika Chakraborty et.al.", "abstract": "In most scenarios, conditional image generation can be thought of as an inversion of the image understanding process. Since generic image understanding involves the solving of multiple tasks, it is natural to aim at the generation of images via multi-conditioning. However, multi-conditional image generation is a very challenging problem due to the heterogeneity and the sparsity of the (in practice) available conditioning labels. In this work, we propose a novel neural architecture to address the problem of heterogeneity and sparsity of the spatially multi-conditional labels. Our choice of spatial conditioning, such as by semantics and depth, is driven by the promise it holds for better control of the image generation process. The proposed method uses a transformer-like architecture operating pixel-wise, which receives the available labels as input tokens to merge them in a learned homogeneous space of labels. The merged labels are then used for image generation via conditional generative adversarial training. In this process, the sparsity of the labels is handled by simply dropping the input tokens corresponding to the missing labels at the desired locations, thanks to the proposed pixel-wise operating architecture. Our experiments on three benchmark datasets demonstrate the clear superiority of our method over the state-of-the-art and the compared baselines.", "paper_url": "http://arxiv.org/abs/2203.13812v1", "pdf_url": "http://arxiv.org/pdf/2203.13812v1", "repo_url": null}, "2203.13802": {"publish_time": "2022-03-25", "title": "Playing Lottery Tickets in Style Transfer Models", "author": "Meihao Kong et.al.", "abstract": "Style transfer has achieved great success and attracted a wide range of attention from both academic and industrial communities due to its flexible application scenarios. However, the dependence on pretty large VGG based autoencoder leads to existing style transfer models have a high parameter complexities which limits the application for resource-constrained devices. Unfortunately, the compression of style transfer model has less been explored. In parallel, study on the lottery ticket hypothesis (LTH) has shown great potential in finding extremely sparse matching subnetworks which can achieve on par or even better performance than original full networks when trained in isolation. In this work, we perform the first empirical study to verify whether such trainable networks also exist in style transfer models. From a wide range of style transfer methods, we choose two of the most popular style transfer models as the main testbeds, i.e., AdaIN and SANet, representing approaches of global and local transformation based style transfer respectively. Through extensive experiments and comprehensive analysis, we draw the following main conclusions. (1) Compared with fixing VGG encoder, style transfer models can benefit more from training the whole network together. (2) Using iterative magnitude pruning, we find the most sparse matching subnetworks at 89.2% in AdaIN and 73.7% in SANet, which suggests that style transfer models can play lottery tickets too. (3) Feature transformation module should also be pruned to get a sparser model without affecting the existence and quality of matching subnetworks. (4) Besides AdaIN and SANet, other models such as LST, MANet, AdaAttN and MCCNet can also play lottert tickets, which shows that LTH can be generalized to various style transfer models.", "paper_url": "http://arxiv.org/abs/2203.13802v1", "pdf_url": "http://arxiv.org/pdf/2203.13802v1", "repo_url": null}, "2203.13800": {"publish_time": "2022-03-25", "title": "Continuous Dynamic-NeRF: Spline-NeRF", "author": "Julian Knodt et.al.", "abstract": "The problem of reconstructing continuous functions over time is important for problems such as reconstructing moving scenes, and interpolating between time steps. Previous approaches that use deep-learning rely on regularization to ensure that reconstructions are approximately continuous, which works well on short sequences. As sequence length grows, though, it becomes more difficult to regularize, and it becomes less feasible to learn only through regularization. We propose a new architecture for function reconstruction based on classical Bezier splines, which ensures $C^0$ and $C^1$-continuity, where $C^0$ continuity is that $\\forall c:\\lim\\limits_{x\\to c} f(x)   = f(c)$, or more intuitively that there are no breaks at any point in the function. In order to demonstrate our architecture, we reconstruct dynamic scenes using Neural Radiance Fields, but hope it is clear that our approach is general and can be applied to a variety of problems. We recover a Bezier spline $B(\\beta, t\\in[0,1])$, parametrized by the control points $\\beta$. Using Bezier splines ensures reconstructions have $C^0$ and $C^1$ continuity, allowing for guaranteed interpolation over time. We reconstruct $\\beta$ with a multi-layer perceptron (MLP), blending machine learning with classical animation techniques. All code is available at https://github.com/JulianKnodt/nerf_atlas, and datasets are from prior work.", "paper_url": "http://arxiv.org/abs/2203.13800v1", "pdf_url": "http://arxiv.org/pdf/2203.13800v1", "repo_url": "https://github.com/JulianKnodt/nerf_atlas"}, "2203.14957": {"publish_time": "2022-03-28", "title": "Frame-wise Action Representations for Long Videos via Sequence Contrastive Learning", "author": "Minghao Chen et.al.", "abstract": "Prior works on action representation learning mainly focus on designing various architectures to extract the global representations for short video clips. In contrast, many practical applications such as video alignment have strong demand for learning dense representations for long videos. In this paper, we introduce a novel contrastive action representation learning (CARL) framework to learn frame-wise action representations, especially for long videos, in a self-supervised manner. Concretely, we introduce a simple yet efficient video encoder that considers spatio-temporal context to extract frame-wise representations. Inspired by the recent progress of self-supervised learning, we present a novel sequence contrastive loss (SCL) applied on two correlated views obtained through a series of spatio-temporal data augmentations. SCL optimizes the embedding space by minimizing the KL-divergence between the sequence similarity of two augmented views and a prior Gaussian distribution of timestamp distance. Experiments on FineGym, PennAction and Pouring datasets show that our method outperforms previous state-of-the-art by a large margin for downstream fine-grained action classification. Surprisingly, although without training on paired videos, our approach also shows outstanding performance on video alignment and fine-grained frame retrieval tasks. Code and models are available at https://github.com/minghchen/CARL_code.", "paper_url": "http://arxiv.org/abs/2203.14957v1", "pdf_url": "http://arxiv.org/pdf/2203.14957v1", "repo_url": "https://github.com/minghchen/carl_code"}, "2203.14956": {"publish_time": "2022-03-28", "title": "LiDAR Distillation: Bridging the Beam-Induced Domain Gap for 3D Object Detection", "author": "Yi Wei et.al.", "abstract": "In this paper, we propose the LiDAR Distillation to bridge the domain gap induced by different LiDAR beams for 3D object detection. In many real-world applications, the LiDAR points used by mass-produced robots and vehicles usually have fewer beams than that in large-scale public datasets. Moreover, as the LiDARs are upgraded to other product models with different beam amount, it becomes challenging to utilize the labeled data captured by previous versions' high-resolution sensors. Despite the recent progress on domain adaptive 3D detection, most methods struggle to eliminate the beam-induced domain gap. We find that it is essential to align the point cloud density of the source domain with that of the target domain during the training process. Inspired by this discovery, we propose a progressive framework to mitigate the beam-induced domain shift. In each iteration, we first generate low-beam pseudo LiDAR by downsampling the high-beam point clouds. Then the teacher-student framework is employed to distill rich information from the data with more beams. Extensive experiments on Waymo, nuScenes and KITTI datasets with three different LiDAR-based detectors demonstrate the effectiveness of our LiDAR Distillation. Notably, our approach does not increase any additional computation cost for inference.", "paper_url": "http://arxiv.org/abs/2203.14956v1", "pdf_url": "http://arxiv.org/pdf/2203.14956v1", "repo_url": "https://github.com/weiyithu/lidar-distillation"}, "2203.14954": {"publish_time": "2022-03-28", "title": "GIRAFFE HD: A High-Resolution 3D-aware Generative Model", "author": "Yang Xue et.al.", "abstract": "3D-aware generative models have shown that the introduction of 3D information can lead to more controllable image generation. In particular, the current state-of-the-art model GIRAFFE can control each object's rotation, translation, scale, and scene camera pose without corresponding supervision. However, GIRAFFE only operates well when the image resolution is low. We propose GIRAFFE HD, a high-resolution 3D-aware generative model that inherits all of GIRAFFE's controllable features while generating high-quality, high-resolution images ($512^2$ resolution and above). The key idea is to leverage a style-based neural renderer, and to independently generate the foreground and background to force their disentanglement while imposing consistency constraints to stitch them together to composite a coherent final image. We demonstrate state-of-the-art 3D controllable high-resolution image generation on multiple natural image datasets.", "paper_url": "http://arxiv.org/abs/2203.14954v1", "pdf_url": "http://arxiv.org/pdf/2203.14954v1", "repo_url": null}, "2203.14952": {"publish_time": "2022-03-28", "title": "Energy-based Latent Aligner for Incremental Learning", "author": "K J Joseph et.al.", "abstract": "Deep learning models tend to forget their earlier knowledge while incrementally learning new tasks. This behavior emerges because the parameter updates optimized for the new tasks may not align well with the updates suitable for older tasks. The resulting latent representation mismatch causes forgetting. In this work, we propose ELI: Energy-based Latent Aligner for Incremental Learning, which first learns an energy manifold for the latent representations such that previous task latents will have low energy and the current task latents have high energy values. This learned manifold is used to counter the representational shift that happens during incremental learning. The implicit regularization that is offered by our proposed methodology can be used as a plug-and-play module in existing incremental learning methodologies. We validate this through extensive evaluation on CIFAR-100, ImageNet subset, ImageNet 1k and Pascal VOC datasets. We observe consistent improvement when ELI is added to three prominent methodologies in class-incremental learning, across multiple incremental settings. Further, when added to the state-of-the-art incremental object detector, ELI provides over 5% improvement in detection accuracy, corroborating its effectiveness and complementary advantage to existing art.", "paper_url": "http://arxiv.org/abs/2203.14952v1", "pdf_url": "http://arxiv.org/pdf/2203.14952v1", "repo_url": "https://github.com/josephkj/eli"}, "2203.14949": {"publish_time": "2022-03-28", "title": "Controllable Dynamic Multi-Task Architectures", "author": "Dripta S. Raychaudhuri et.al.", "abstract": "Multi-task learning commonly encounters competition for resources among tasks, specifically when model capacity is limited. This challenge motivates models which allow control over the relative importance of tasks and total compute cost during inference time. In this work, we propose such a controllable multi-task network that dynamically adjusts its architecture and weights to match the desired task preference as well as the resource constraints. In contrast to the existing dynamic multi-task approaches that adjust only the weights within a fixed architecture, our approach affords the flexibility to dynamically control the total computational cost and match the user-preferred task importance better. We propose a disentangled training of two hypernetworks, by exploiting task affinity and a novel branching regularized loss, to take input preferences and accordingly predict tree-structured models with adapted weights. Experiments on three multi-task benchmarks, namely PASCAL-Context, NYU-v2, and CIFAR-100, show the efficacy of our approach. Project page is available at https://www.nec-labs.com/~mas/DYMU.", "paper_url": "http://arxiv.org/abs/2203.14949v1", "pdf_url": "http://arxiv.org/pdf/2203.14949v1", "repo_url": null}, "2203.15799": {"publish_time": "2022-03-29", "title": "StyleT2I: Toward Compositional and High-Fidelity Text-to-Image Synthesis", "author": "Zhiheng Li et.al.", "abstract": "Although progress has been made for text-to-image synthesis, previous methods fall short of generalizing to unseen or underrepresented attribute compositions in the input text. Lacking compositionality could have severe implications for robustness and fairness, e.g., inability to synthesize the face images of underrepresented demographic groups. In this paper, we introduce a new framework, StyleT2I, to improve the compositionality of text-to-image synthesis. Specifically, we propose a CLIP-guided Contrastive Loss to better distinguish different compositions among different sentences. To further improve the compositionality, we design a novel Semantic Matching Loss and a Spatial Constraint to identify attributes' latent directions for intended spatial region manipulations, leading to better disentangled latent representations of attributes. Based on the identified latent directions of attributes, we propose Compositional Attribute Adjustment to adjust the latent code, resulting in better compositionality of image synthesis. In addition, we leverage the $\\ell_2$-norm regularization of identified latent directions (norm penalty) to strike a nice balance between image-text alignment and image fidelity. In the experiments, we devise a new dataset split and an evaluation metric to evaluate the compositionality of text-to-image synthesis models. The results show that StyleT2I outperforms previous approaches in terms of the consistency between the input text and synthesized images and achieves higher fidelity.", "paper_url": "http://arxiv.org/abs/2203.15799v1", "pdf_url": "http://arxiv.org/pdf/2203.15799v1", "repo_url": "https://github.com/zhihengli-UR/StyleT2I"}, "2203.15798": {"publish_time": "2022-03-29", "title": "DRaCoN -- Differentiable Rasterization Conditioned Neural Radiance Fields for Articulated Avatars", "author": "Amit Raj et.al.", "abstract": "Acquisition and creation of digital human avatars is an important problem with applications to virtual telepresence, gaming, and human modeling. Most contemporary approaches for avatar generation can be viewed either as 3D-based methods, which use multi-view data to learn a 3D representation with appearance (such as a mesh, implicit surface, or volume), or 2D-based methods which learn photo-realistic renderings of avatars but lack accurate 3D representations. In this work, we present, DRaCoN, a framework for learning full-body volumetric avatars which exploits the advantages of both the 2D and 3D neural rendering techniques. It consists of a Differentiable Rasterization module, DiffRas, that synthesizes a low-resolution version of the target image along with additional latent features guided by a parametric body model. The output of DiffRas is then used as conditioning to our conditional neural 3D representation module (c-NeRF) which generates the final high-res image along with body geometry using volumetric rendering. While DiffRas helps in obtaining photo-realistic image quality, c-NeRF, which employs signed distance fields (SDF) for 3D representations, helps to obtain fine 3D geometric details. Experiments on the challenging ZJU-MoCap and Human3.6M datasets indicate that DRaCoN outperforms state-of-the-art methods both in terms of error metrics and visual quality.", "paper_url": "http://arxiv.org/abs/2203.15798v1", "pdf_url": "http://arxiv.org/pdf/2203.15798v1", "repo_url": null}, "2203.15794": {"publish_time": "2022-03-29", "title": "CHEX: CHannel EXploration for CNN Model Compression", "author": "Zejiang Hou et.al.", "abstract": "Channel pruning has been broadly recognized as an effective technique to reduce the computation and memory cost of deep convolutional neural networks. However, conventional pruning methods have limitations in that: they are restricted to pruning process only, and they require a fully pre-trained large model. Such limitations may lead to sub-optimal model quality as well as excessive memory and training cost. In this paper, we propose a novel Channel Exploration methodology, dubbed as CHEX, to rectify these problems. As opposed to pruning-only strategy, we propose to repeatedly prune and regrow the channels throughout the training process, which reduces the risk of pruning important channels prematurely. More exactly: From intra-layer's aspect, we tackle the channel pruning problem via a well known column subset selection (CSS) formulation. From inter-layer's aspect, our regrowing stages open a path for dynamically re-allocating the number of channels across all the layers under a global channel sparsity constraint. In addition, all the exploration process is done in a single training from scratch without the need of a pre-trained large model. Experimental results demonstrate that CHEX can effectively reduce the FLOPs of diverse CNN architectures on a variety of computer vision tasks, including image classification, object detection, instance segmentation, and 3D vision. For example, our compressed ResNet-50 model on ImageNet dataset achieves 76% top1 accuracy with only 25% FLOPs of the original ResNet-50 model, outperforming previous state-of-the-art channel pruning methods. The checkpoints and code are available at here .", "paper_url": "http://arxiv.org/abs/2203.15794v1", "pdf_url": "http://arxiv.org/pdf/2203.15794v1", "repo_url": null}, "2203.15793": {"publish_time": "2022-03-29", "title": "Instance Relation Graph Guided Source-Free Domain Adaptive Object Detection", "author": "Vibashan VS et.al.", "abstract": "Unsupervised Domain Adaptation (UDA) is an effective approach to tackle the issue of domain shift. Specifically, UDA methods try to align the source and target representations to improve the generalization on the target domain. Further, UDA methods work under the assumption that the source data is accessible during the adaptation process. However, in real-world scenarios, the labelled source data is often restricted due to privacy regulations, data transmission constraints, or proprietary data concerns. The Source-Free Domain Adaptation (SFDA) setting aims to alleviate these concerns by adapting a source-trained model for the target domain without requiring access to the source data. In this paper, we explore the SFDA setting for the task of adaptive object detection. To this end, we propose a novel training strategy for adapting a source-trained object detector to the target domain without source data. More precisely, we design a novel contrastive loss to enhance the target representations by exploiting the objects relations for a given target domain input. These object instance relations are modelled using an Instance Relation Graph (IRG) network, which are then used to guide the contrastive representation learning. In addition, we utilize a student-teacher based knowledge distillation strategy to avoid overfitting to the noisy pseudo-labels generated by the source-trained model. Extensive experiments on multiple object detection benchmark datasets show that the proposed approach is able to efficiently adapt source-trained object detectors to the target domain, outperforming previous state-of-the-art domain adaptive detection methods. Code is available at https://github.com/Vibashan/irg-sfda.", "paper_url": "http://arxiv.org/abs/2203.15793v1", "pdf_url": "http://arxiv.org/pdf/2203.15793v1", "repo_url": "https://github.com/vibashan/irg-sfda"}, "2203.15792": {"publish_time": "2022-03-29", "title": "Target and Task specific Source-Free Domain Adaptive Image Segmentation", "author": "Vibashan VS et.al.", "abstract": "Solving the domain shift problem during inference is essential in medical imaging as most deep-learning based solutions suffer from it. In practice, domain shifts are tackled by performing Unsupervised Domain Adaptation (UDA), where a model is adapted to an unlabeled target domain by leveraging the labelled source domain. In medical scenarios, the data comes with huge privacy concerns making it difficult to apply standard UDA techniques. Hence, a closer clinical setting is Source-Free UDA (SFUDA), where we have access to source trained model but not the source data during adaptation. Methods trying to solve SFUDA typically address the domain shift using pseudo-label based self-training techniques. However, due to domain shift, these pseudo-labels are usually of high entropy and denoising them still does not make them perfect labels to supervise the model. Therefore, adapting the source model with noisy pseudo labels reduces its segmentation capability while addressing the domain shift. To this end, we propose a two-stage approach for source-free domain adaptive image segmentation: 1) Target-specific adaptation followed by 2) Task-specific adaptation. In the first stage, we focus on generating target-specific pseudo labels while suppressing high entropy regions by proposing an Ensemble Entropy Minimization loss. We also introduce a selective voting strategy to enhance pseudo-label generation. In the second stage, we focus on adapting the network for task-specific representation by using a teacher-student self-training approach based on augmentation-guided consistency. We evaluate our proposed method on both 2D fundus datasets and 3D MRI volumes across 7 different domain shifts where we achieve better performance than recent UDA and SF-UDA methods for medical image segmentation. Code is available at https://github.com/Vibashan/tt-sfuda.", "paper_url": "http://arxiv.org/abs/2203.15792v1", "pdf_url": "http://arxiv.org/pdf/2203.15792v1", "repo_url": "https://github.com/vibashan/tt-sfuda"}, "2203.16533": {"publish_time": "2022-03-30", "title": "Large-Scale Pre-training for Person Re-identification with Noisy Labels", "author": "Dengpan Fu et.al.", "abstract": "This paper aims to address the problem of pre-training for person re-identification (Re-ID) with noisy labels. To setup the pre-training task, we apply a simple online multi-object tracking system on raw videos of an existing unlabeled Re-ID dataset \"LUPerson\" nd build the Noisy Labeled variant called \"LUPerson-NL\". Since theses ID labels automatically derived from tracklets inevitably contain noises, we develop a large-scale Pre-training framework utilizing Noisy Labels (PNL), which consists of three learning modules: supervised Re-ID learning, prototype-based contrastive learning, and label-guided contrastive learning. In principle, joint learning of these three modules not only clusters similar examples to one prototype, but also rectifies noisy labels based on the prototype assignment. We demonstrate that learning directly from raw videos is a promising alternative for pre-training, which utilizes spatial and temporal correlations as weak supervision. This simple pre-training task provides a scalable way to learn SOTA Re-ID representations from scratch on \"LUPerson-NL\" without bells and whistles. For example, by applying on the same supervised Re-ID method MGN, our pre-trained model improves the mAP over the unsupervised pre-training counterpart by 5.7%, 2.2%, 2.3% on CUHK03, DukeMTMC, and MSMT17 respectively. Under the small-scale or few-shot setting, the performance gain is even more significant, suggesting a better transferability of the learned representation. Code is available at https://github.com/DengpanFu/LUPerson-NL", "paper_url": "http://arxiv.org/abs/2203.16533v1", "pdf_url": "http://arxiv.org/pdf/2203.16533v1", "repo_url": "https://github.com/dengpanfu/luperson-nl"}, "2203.16531": {"publish_time": "2022-03-30", "title": "Understanding 3D Object Articulation in Internet Videos", "author": "Shengyi Qian et.al.", "abstract": "We propose to investigate detecting and characterizing the 3D planar articulation of objects from ordinary videos. While seemingly easy for humans, this problem poses many challenges for computers. We propose to approach this problem by combining a top-down detection system that finds planes that can be articulated along with an optimization approach that solves for a 3D plane that can explain a sequence of observed articulations. We show that this system can be trained on a combination of videos and 3D scan datasets. When tested on a dataset of challenging Internet videos and the Charades dataset, our approach obtains strong performance. Project site: https://jasonqsy.github.io/Articulation3D", "paper_url": "http://arxiv.org/abs/2203.16531v1", "pdf_url": "http://arxiv.org/pdf/2203.16531v1", "repo_url": null}, "2203.16530": {"publish_time": "2022-03-30", "title": "Learning Instance-Specific Adaptation for Cross-Domain Segmentation", "author": "Yuliang Zou et.al.", "abstract": "We propose a test-time adaptation method for cross-domain image segmentation. Our method is simple: Given a new unseen instance at test time, we adapt a pre-trained model by conducting instance-specific BatchNorm (statistics) calibration. Our approach has two core components. First, we replace the manually designed BatchNorm calibration rule with a learnable module. Second, we leverage strong data augmentation to simulate random domain shifts for learning the calibration rule. In contrast to existing domain adaptation methods, our method does not require accessing the target domain data at training time or conducting computationally expensive test-time model training/optimization. Equipping our method with models trained by standard recipes achieves significant improvement, comparing favorably with several state-of-the-art domain generalization and one-shot unsupervised domain adaptation approaches. Combining our method with the domain generalization methods further improves performance, reaching a new state of the art.", "paper_url": "http://arxiv.org/abs/2203.16530v1", "pdf_url": "http://arxiv.org/pdf/2203.16530v1", "repo_url": null}, "2203.16529": {"publish_time": "2022-03-30", "title": "CaDeX: Learning Canonical Deformation Coordinate Space for Dynamic Surface Representation via Neural Homeomorphism", "author": "Jiahui Lei et.al.", "abstract": "While neural representations for static 3D shapes are widely studied, representations for deformable surfaces are limited to be template-dependent or lack efficiency. We introduce Canonical Deformation Coordinate Space (CaDeX), a unified representation of both shape and nonrigid motion. Our key insight is the factorization of the deformation between frames by continuous bijective canonical maps (homeomorphisms) and their inverses that go through a learned canonical shape. Our novel deformation representation and its implementation are simple, efficient, and guarantee cycle consistency, topology preservation, and, if needed, volume conservation. Our modelling of the learned canonical shapes provides a flexible and stable space for shape prior learning. We demonstrate state-of-the-art performance in modelling a wide range of deformable geometries: human bodies, animal bodies, and articulated objects.", "paper_url": "http://arxiv.org/abs/2203.16529v1", "pdf_url": "http://arxiv.org/pdf/2203.16529v1", "repo_url": null}, "2203.16528": {"publish_time": "2022-03-30", "title": "L^3U-net: Low-Latency Lightweight U-net Based Image Segmentation Model for Parallel CNN Processors", "author": "Osman Erman Okman et.al.", "abstract": "In this research, we propose a tiny image segmentation model, L^3U-net, that works on low-resource edge devices in real-time. We introduce a data folding technique that reduces inference latency by leveraging the parallel convolutional layer processing capability of the CNN accelerators. We also deploy the proposed model to such a device, MAX78000, and the results show that L^3U-net achieves more than 90% accuracy over two different segmentation datasets with 10 fps.", "paper_url": "http://arxiv.org/abs/2203.16528v1", "pdf_url": "http://arxiv.org/pdf/2203.16528v1", "repo_url": null}, "2203.17276": {"publish_time": "2022-03-31", "title": "Bringing Old Films Back to Life", "author": "Ziyu Wan et.al.", "abstract": "We present a learning-based framework, recurrent transformer network (RTN), to restore heavily degraded old films. Instead of performing frame-wise restoration, our method is based on the hidden knowledge learned from adjacent frames that contain abundant information about the occlusion, which is beneficial to restore challenging artifacts of each frame while ensuring temporal coherency. Moreover, contrasting the representation of the current frame and the hidden knowledge makes it possible to infer the scratch position in an unsupervised manner, and such defect localization generalizes well to real-world degradations. To better resolve mixed degradation and compensate for the flow estimation error during frame alignment, we propose to leverage more expressive transformer blocks for spatial restoration. Experiments on both synthetic dataset and real-world old films demonstrate the significant superiority of the proposed RTN over existing solutions. In addition, the same framework can effectively propagate the color from keyframes to the whole video, ultimately yielding compelling restored films. The implementation and model will be released at https://github.com/raywzy/Bringing-Old-Films-Back-to-Life.", "paper_url": "http://arxiv.org/abs/2203.17276v1", "pdf_url": "http://arxiv.org/pdf/2203.17276v1", "repo_url": "https://github.com/raywzy/bringing-old-films-back-to-life"}, "2203.17275": {"publish_time": "2022-03-31", "title": "DiffSkill: Skill Abstraction from Differentiable Physics for Deformable Object Manipulations with Tools", "author": "Xingyu Lin et.al.", "abstract": "We consider the problem of sequential robotic manipulation of deformable objects using tools. Previous works have shown that differentiable physics simulators provide gradients to the environment state and help trajectory optimization to converge orders of magnitude faster than model-free reinforcement learning algorithms for deformable object manipulation. However, such gradient-based trajectory optimization typically requires access to the full simulator states and can only solve short-horizon, single-skill tasks due to local optima. In this work, we propose a novel framework, named DiffSkill, that uses a differentiable physics simulator for skill abstraction to solve long-horizon deformable object manipulation tasks from sensory observations. In particular, we first obtain short-horizon skills using individual tools from a gradient-based optimizer, using the full state information in a differentiable simulator; we then learn a neural skill abstractor from the demonstration trajectories which takes RGBD images as input. Finally, we plan over the skills by finding the intermediate goals and then solve long-horizon tasks. We show the advantages of our method in a new set of sequential deformable object manipulation tasks compared to previous reinforcement learning algorithms and compared to the trajectory optimizer.", "paper_url": "http://arxiv.org/abs/2203.17275v1", "pdf_url": "http://arxiv.org/pdf/2203.17275v1", "repo_url": null}, "2203.17273": {"publish_time": "2022-03-31", "title": "FindIt: Generalized Localization with Natural Language Queries", "author": "Weicheng Kuo et.al.", "abstract": "We propose FindIt, a simple and versatile framework that unifies a variety of visual grounding and localization tasks including referring expression comprehension, text-based localization, and object detection. Key to our architecture is an efficient multi-scale fusion module that unifies the disparate localization requirements across the tasks. In addition, we discover that a standard object detector is surprisingly effective in unifying these tasks without a need for task-specific design, losses, or pre-computed detections. Our end-to-end trainable framework responds flexibly and accurately to a wide range of referring expression, localization or detection queries for zero, one, or multiple objects. Jointly trained on these tasks, FindIt outperforms the state of the art on both referring expression and text-based localization, and shows competitive performance on object detection. Finally, FindIt generalizes better to out-of-distribution data and novel categories compared to strong single-task baselines. All of these are accomplished by a single, unified and efficient model. The code will be released.", "paper_url": "http://arxiv.org/abs/2203.17273v1", "pdf_url": "http://arxiv.org/pdf/2203.17273v1", "repo_url": null}, "2203.17274": {"publish_time": "2022-03-31", "title": "Visual Prompting: Modifying Pixel Space to Adapt Pre-trained Models", "author": "Hyojin Bahng et.al.", "abstract": "Prompting has recently become a popular paradigm for adapting language models to downstream tasks. Rather than fine-tuning model parameters or adding task-specific heads, this approach steers a model to perform a new task simply by adding a text prompt to the model's inputs. In this paper, we explore the question: can we create prompts with pixels instead? In other words, can pre-trained vision models be adapted to a new task solely by adding pixels to their inputs? We introduce visual prompting, which learns a task-specific image perturbation such that a frozen pre-trained model prompted with this perturbation performs a new task. We discover that changing only a few pixels is enough to adapt models to new tasks and datasets, and performs on par with linear probing, the current de facto approach to lightweight adaptation. The surprising effectiveness of visual prompting provides a new perspective on how to adapt pre-trained models in vision, and opens up the possibility of adapting models solely through their inputs, which, unlike model parameters or outputs, are typically under an end-user's control. Code is available at http://hjbahng.github.io/visual_prompting .", "paper_url": "http://arxiv.org/abs/2203.17274v1", "pdf_url": "http://arxiv.org/pdf/2203.17274v1", "repo_url": null}, "2203.17272": {"publish_time": "2022-03-31", "title": "MyStyle: A Personalized Generative Prior", "author": "Yotam Nitzan et.al.", "abstract": "We introduce MyStyle, a personalized deep generative prior trained with a few shots of an individual. MyStyle allows to reconstruct, enhance and edit images of a specific person, such that the output is faithful to the person's key facial characteristics. Given a small reference set of portrait images of a person (~100), we tune the weights of a pretrained StyleGAN face generator to form a local, low-dimensional, personalized manifold in the latent space. We show that this manifold constitutes a personalized region that spans latent codes associated with diverse portrait images of the individual. Moreover, we demonstrate that we obtain a personalized generative prior, and propose a unified approach to apply it to various ill-posed image enhancement problems, such as inpainting and super-resolution, as well as semantic editing. Using the personalized generative prior we obtain outputs that exhibit high-fidelity to the input images and are also faithful to the key facial characteristics of the individual in the reference set. We demonstrate our method with fair-use images of numerous widely recognizable individuals for whom we have the prior knowledge for a qualitative evaluation of the expected outcome. We evaluate our approach against few-shots baselines and show that our personalized prior, quantitatively and qualitatively, outperforms state-of-the-art alternatives.", "paper_url": "http://arxiv.org/abs/2203.17272v1", "pdf_url": "http://arxiv.org/pdf/2203.17272v1", "repo_url": null}, "2204.00616": {"publish_time": "2022-04-01", "title": "Simplicial Embeddings in Self-Supervised Learning and Downstream Classification", "author": "Samuel Lavoie et.al.", "abstract": "We introduce Simplicial Embeddings (SEMs) as a way to constrain the encoded representations of a self-supervised model to $L$ simplices of $V$ dimensions each using a Softmax operation. This procedure imposes a structure on the representations that reduce their expressivity for training downstream classifiers, which helps them generalize better. Specifically, we show that the temperature $\\tau$ of the Softmax operation controls for the SEM representation's expressivity, allowing us to derive a tighter downstream classifier generalization bound than that for classifiers using unnormalized representations. We empirically demonstrate that SEMs considerably improve generalization on natural image datasets such as CIFAR-100 and ImageNet. Finally, we also present evidence of the emergence of semantically relevant features in SEMs, a pattern that is absent from baseline self-supervised models.", "paper_url": "http://arxiv.org/abs/2204.00616v1", "pdf_url": "http://arxiv.org/pdf/2204.00616v1", "repo_url": null}, "2204.00613": {"publish_time": "2022-04-01", "title": "On the Importance of Asymmetry for Siamese Representation Learning", "author": "Xiao Wang et.al.", "abstract": "Many recent self-supervised frameworks for visual representation learning are based on certain forms of Siamese networks. Such networks are conceptually symmetric with two parallel encoders, but often practically asymmetric as numerous mechanisms are devised to break the symmetry. In this work, we conduct a formal study on the importance of asymmetry by explicitly distinguishing the two encoders within the network -- one produces source encodings and the other targets. Our key insight is keeping a relatively lower variance in target than source generally benefits learning. This is empirically justified by our results from five case studies covering different variance-oriented designs, and is aligned with our preliminary theoretical analysis on the baseline. Moreover, we find the improvements from asymmetric designs generalize well to longer training schedules, multiple other frameworks and newer backbones. Finally, the combined effect of several asymmetric designs achieves a state-of-the-art accuracy on ImageNet linear probing and competitive results on downstream transfer. We hope our exploration will inspire more research in exploiting asymmetry for Siamese representation learning.", "paper_url": "http://arxiv.org/abs/2204.00613v1", "pdf_url": "http://arxiv.org/pdf/2204.00613v1", "repo_url": "https://github.com/facebookresearch/asym-siam"}, "2204.00604": {"publish_time": "2022-04-01", "title": "Quantized GAN for Complex Music Generation from Dance Videos", "author": "Ye Zhu et.al.", "abstract": "We present Dance2Music-GAN (D2M-GAN), a novel adversarial multi-modal framework that generates complex musical samples conditioned on dance videos. Our proposed framework takes dance video frames and human body motion as input, and learns to generate music samples that plausibly accompany the corresponding input. Unlike most existing conditional music generation works that generate specific types of mono-instrumental sounds using symbolic audio representations (e.g., MIDI), and that heavily rely on pre-defined musical synthesizers, in this work we generate dance music in complex styles (e.g., pop, breakdancing, etc.) by employing a Vector Quantized (VQ) audio representation, and leverage both its generality and the high abstraction capacity of its symbolic and continuous counterparts. By performing an extensive set of experiments on multiple datasets, and following a comprehensive evaluation protocol, we assess the generative quality of our approach against several alternatives. The quantitative results, which measure the music consistency, beats correspondence, and music diversity, clearly demonstrate the effectiveness of our proposed method. Last but not least, we curate a challenging dance-music dataset of in-the-wild TikTok videos, which we use to further demonstrate the efficacy of our approach in real-world applications - and which we hope to serve as a starting point for relevant future research.", "paper_url": "http://arxiv.org/abs/2204.00604v1", "pdf_url": "http://arxiv.org/pdf/2204.00604v1", "repo_url": "https://github.com/l-yezhu/d2m-gan"}, "2204.00598": {"publish_time": "2022-04-01", "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language", "author": "Andy Zeng et.al.", "abstract": "Large foundation models can exhibit unique capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g. from spreadsheets, to SAT questions). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this model diversity is symbiotic, and can be leveraged to build AI systems with structured Socratic dialogue -- in which new multimodal tasks are formulated as a guided language-based exchange between different pre-existing foundation models, without additional finetuning. In the context of egocentric perception, we present a case study of Socratic Models (SMs) that can provide meaningful results for complex tasks such as generating free-form answers to contextual questions about egocentric video, by formulating video Q&A as short story Q&A, i.e. summarizing the video into a short story, then answering questions about it. Additionally, SMs can generate captions for Internet images, and are competitive with state-of-the-art on zero-shot video-to-text retrieval with 42.8 R@1 on MSR-VTT 1k-A. SMs demonstrate how to compose foundation models zero-shot to capture new multimodal functionalities, without domain-specific data collection. Prototypes are available at socraticmodels.github.io.", "paper_url": "http://arxiv.org/abs/2204.00598v1", "pdf_url": "http://arxiv.org/pdf/2204.00598v1", "repo_url": null}, "2204.00597": {"publish_time": "2022-04-01", "title": "Fast and Automatic Object Registration for Human-Robot Collaboration in Industrial Manufacturing", "author": "Manuela Gei\u00df et.al.", "abstract": "We present an end-to-end framework for fast retraining of object detection models in human-robot-collaboration. Our Faster R-CNN based setup covers the whole workflow of automatic image generation and labeling, model retraining on-site as well as inference on a FPGA edge device. The intervention of a human operator reduces to providing the new object together with its label and starting the training process. Moreover, we present a new loss, the intraspread-objectosphere loss, to tackle the problem of open world recognition. Though it fails to completely solve the problem, it significantly reduces the number of false positive detections of unknown objects.", "paper_url": "http://arxiv.org/abs/2204.00597v1", "pdf_url": "http://arxiv.org/pdf/2204.00597v1", "repo_url": null}, "2204.01697": {"publish_time": "2022-04-04", "title": "MaxViT: Multi-Axis Vision Transformer", "author": "Zhengzhong Tu et.al.", "abstract": "Transformers have recently gained significant attention in the computer vision community. However, the lack of scalability of self-attention mechanisms with respect to image size has limited their wide adoption in state-of-the-art vision backbones. In this paper we introduce an efficient and scalable attention model we call multi-axis attention, which consists of two aspects: blocked local and dilated global attention. These design choices allow global-local spatial interactions on arbitrary input resolutions with only linear complexity. We also present a new architectural element by effectively blending our proposed attention model with convolutions, and accordingly propose a simple hierarchical vision backbone, dubbed MaxViT, by simply repeating the basic building block over multiple stages. Notably, MaxViT is able to \"see\" globally throughout the entire network, even in earlier, high-resolution stages. We demonstrate the effectiveness of our model on a broad spectrum of vision tasks. On image classification, MaxViT achieves state-of-the-art performance under various settings: without extra data, MaxViT attains 86.5\\% ImageNet-1K top-1 accuracy; with ImageNet-21K pre-training, our model achieves 88.7\\% top-1 accuracy. For downstream tasks, MaxViT as a backbone delivers favorable performance on object detection as well as visual aesthetic assessment. We also show that our proposed model expresses strong generative modeling capability on ImageNet, demonstrating the superior potential of MaxViT blocks as a universal vision module. We will make the code and models publicly available.", "paper_url": "http://arxiv.org/abs/2204.01697v1", "pdf_url": "http://arxiv.org/pdf/2204.01697v1", "repo_url": null}, "2204.00628": {"publish_time": "2022-04-04", "title": "Learning Neural Acoustic Fields", "author": "Andrew Luo et.al.", "abstract": "Our environment is filled with rich and dynamic acoustic information. When we walk into a cathedral, the reverberations as much as appearance inform us of the sanctuary's wide open space. Similarly, as an object moves around us, we expect the sound emitted to also exhibit this movement. While recent advances in learned implicit functions have led to increasingly higher quality representations of the visual world, there have not been commensurate advances in learning spatial auditory representations. To address this gap, we introduce Neural Acoustic Fields (NAFs), an implicit representation that captures how sounds propagate in a physical scene. By modeling acoustic propagation in a scene as a linear time-invariant system, NAFs learn to continuously map all emitter and listener location pairs to a neural impulse response function that can then be applied to arbitrary sounds. We demonstrate that the continuous nature of NAFs enables us to render spatial acoustics for a listener at an arbitrary location, and can predict sound propagation at novel locations. We further show that the representation learned by NAFs can help improve visual learning with sparse views. Finally, we show that a representation informative of scene structure emerges during the learning of NAFs.", "paper_url": "http://arxiv.org/abs/2204.00628v1", "pdf_url": "http://arxiv.org/pdf/2204.00628v1", "repo_url": null}, "2204.01695": {"publish_time": "2022-04-04", "title": "LISA: Learning Implicit Shape and Appearance of Hands", "author": "Enric Corona et.al.", "abstract": "This paper proposes a do-it-all neural model of human hands, named LISA. The model can capture accurate hand shape and appearance, generalize to arbitrary hand subjects, provide dense surface correspondences, be reconstructed from images in the wild and easily animated. We train LISA by minimizing the shape and appearance losses on a large set of multi-view RGB image sequences annotated with coarse 3D poses of the hand skeleton. For a 3D point in the hand local coordinate, our model predicts the color and the signed distance with respect to each hand bone independently, and then combines the per-bone predictions using predicted skinning weights. The shape, color and pose representations are disentangled by design, allowing to estimate or animate only selected parameters. We experimentally demonstrate that LISA can accurately reconstruct a dynamic hand from monocular or multi-view sequences, achieving a noticeably higher quality of reconstructed hand shapes compared to baseline approaches. Project page: https://www.iri.upc.edu/people/ecorona/lisa/.", "paper_url": "http://arxiv.org/abs/2204.01695v1", "pdf_url": "http://arxiv.org/pdf/2204.01695v1", "repo_url": null}, "2204.01696": {"publish_time": "2022-04-04", "title": "Joint Hand Motion and Interaction Hotspots Prediction from Egocentric Videos", "author": "Shaowei Liu et.al.", "abstract": "We propose to forecast future hand-object interactions given an egocentric video. Instead of predicting action labels or pixels, we directly predict the hand motion trajectory and the future contact points on the next active object (i.e., interaction hotspots). This relatively low-dimensional representation provides a concrete description of future interactions. To tackle this task, we first provide an automatic way to collect trajectory and hotspots labels on large-scale data. We then use this data to train an Object-Centric Transformer (OCT) model for prediction. Our model performs hand and object interaction reasoning via the self-attention mechanism in Transformers. OCT also provides a probabilistic framework to sample the future trajectory and hotspots to handle uncertainty in prediction. We perform experiments on the Epic-Kitchens-55, Epic-Kitchens-100, and EGTEA Gaze+ datasets, and show that OCT significantly outperforms state-of-the-art approaches by a large margin. Project page is available at https://stevenlsw.github.io/hoi-forecast .", "paper_url": "http://arxiv.org/abs/2204.01696v1", "pdf_url": "http://arxiv.org/pdf/2204.01696v1", "repo_url": null}, "2204.01694": {"publish_time": "2022-04-04", "title": "\"This is my unicorn, Fluffy\": Personalizing frozen vision-language representations", "author": "Niv Cohen et.al.", "abstract": "Large Vision & Language models pretrained on web-scale data provide representations that are invaluable for numerous V&L problems. However, it is unclear how they can be used for reasoning about user-specific visual concepts in unstructured language. This problem arises in multiple domains, from personalized image retrieval to personalized interaction with smart devices. We introduce a new learning setup called Personalized Vision & Language (PerVL) with two new benchmark datasets for retrieving and segmenting user-specific \"personalized\" concepts \"in the wild\". In PerVL, one should learn personalized concepts (1) independently of the downstream task (2) allowing a pretrained model to reason about them with free language, and (3) does not require personalized negative examples. We propose an architecture for solving PerVL that operates by extending the input vocabulary of a pretrained model with new word embeddings for the new personalized concepts. The model can then reason about them by simply using them in a sentence. We demonstrate that our approach learns personalized visual concepts from a few examples and can effectively apply them in image retrieval and semantic segmentation using rich textual queries.", "paper_url": "http://arxiv.org/abs/2204.01694v1", "pdf_url": "http://arxiv.org/pdf/2204.01694v1", "repo_url": null}, "2204.02397": {"publish_time": "2022-04-05", "title": "SALISA: Saliency-based Input Sampling for Efficient Video Object Detection", "author": "Babak Ehteshami Bejnordi et.al.", "abstract": "High-resolution images are widely adopted for high-performance object detection in videos. However, processing high-resolution inputs comes with high computation costs, and naive down-sampling of the input to reduce the computation costs quickly degrades the detection performance. In this paper, we propose SALISA, a novel non-uniform SALiency-based Input SAmpling technique for video object detection that allows for heavy down-sampling of unimportant background regions while preserving the fine-grained details of a high-resolution image. The resulting image is spatially smaller, leading to reduced computational costs while enabling a performance comparable to a high-resolution input. To achieve this, we propose a differentiable resampling module based on a thin plate spline spatial transformer network (TPS-STN). This module is regularized by a novel loss to provide an explicit supervision signal to learn to \"magnify\" salient regions. We report state-of-the-art results in the low compute regime on the ImageNet-VID and UA-DETRAC video object detection datasets. We demonstrate that on both datasets, the mAP of an EfficientDet-D1 (EfficientDet-D2) gets on par with EfficientDet-D2 (EfficientDet-D3) at a much lower computational cost. We also show that SALISA significantly improves the detection of small objects. In particular, SALISA with an EfficientDet-D1 detector improves the detection of small objects by $77\\%$, and remarkably also outperforms EfficientDetD3 baseline.", "paper_url": "http://arxiv.org/abs/2204.02397v1", "pdf_url": "http://arxiv.org/pdf/2204.02397v1", "repo_url": null}, "2204.02394": {"publish_time": "2022-04-05", "title": "SE(3)-Equivariant Attention Networks for Shape Reconstruction in Function Space", "author": "Evangelos Chatzipantazis et.al.", "abstract": "We propose the first SE(3)-equivariant coordinate-based network for learning occupancy fields from point clouds. In contrast to previous shape reconstruction methods that align the input to a regular grid, we operate directly on the irregular, unoriented point cloud. We leverage attention mechanisms in order to preserve the set structure (permutation equivariance and variable length) of the input. At the same time, attention layers enable local shape modelling, a crucial property for scalability to large scenes. In contrast to architectures that create a global signature for the shape, we operate on local tokens. Given an unoriented, sparse, noisy point cloud as input, we produce equivariant features for each point. These serve as keys and values for the subsequent equivariant cross-attention blocks that parametrize the occupancy field. By querying an arbitrary point in space, we predict its occupancy score. We show that our method outperforms previous SO(3)-equivariant methods, as well as non-equivariant methods trained on SO(3)-augmented datasets. More importantly, local modelling together with SE(3)-equivariance create an ideal setting for SE(3) scene reconstruction. We show that by training only on single objects and without any pre-segmentation, we can reconstruct a novel scene with single-object performance.", "paper_url": "http://arxiv.org/abs/2204.02394v1", "pdf_url": "http://arxiv.org/pdf/2204.02394v1", "repo_url": null}, "2204.02393": {"publish_time": "2022-04-05", "title": "Action-Conditioned Contrastive Policy Pretraining", "author": "Qihang Zhang et.al.", "abstract": "Deep visuomotor policy learning achieves promising results in control tasks such as robotic manipulation and autonomous driving, where the action is generated from the visual input by the neural policy. However, it requires a huge number of online interactions with the training environment, which limits its real-world application. Compared to the popular unsupervised feature learning for visual recognition, feature pretraining for visuomotor control tasks is much less explored. In this work, we aim to pretrain policy representations for driving tasks using hours-long uncurated YouTube videos. A new contrastive policy pretraining method is developed to learn action-conditioned features from video frames with action pseudo labels. Experiments show that the resulting action-conditioned features bring substantial improvements to the downstream reinforcement learning and imitation learning tasks, outperforming the weights pretrained from previous unsupervised learning methods. Code and models will be made publicly available.", "paper_url": "http://arxiv.org/abs/2204.02393v1", "pdf_url": "http://arxiv.org/pdf/2204.02393v1", "repo_url": null}, "2204.02390": {"publish_time": "2022-04-05", "title": "Learning Pneumatic Non-Prehensile Manipulation with a Mobile Blower", "author": "Jimmy Wu et.al.", "abstract": "We investigate pneumatic non-prehensile manipulation (i.e., blowing) as a means of efficiently moving scattered objects into a target receptacle. Due to the chaotic nature of aerodynamic forces, a blowing controller must (i) continually adapt to unexpected changes from its actions, (ii) maintain fine-grained control, since the slightest misstep can result in large unintended consequences (e.g., scatter objects already in a pile), and (iii) infer long-range plans (e.g., move the robot to strategic blowing locations). We tackle these challenges in the context of deep reinforcement learning, introducing a multi-frequency version of the spatial action maps framework. This allows for efficient learning of vision-based policies that effectively combine high-level planning and low-level closed-loop control for dynamic mobile manipulation. Experiments show that our system learns efficient behaviors for the task, demonstrating in particular that blowing achieves better downstream performance than pushing, and that our policies improve performance over baselines. Moreover, we show that our system naturally encourages emergent specialization between the different subpolicies spanning low-level fine-grained control and high-level planning. On a real mobile robot equipped with a miniature air blower, we show that our simulation-trained policies transfer well to a real environment and can generalize to novel objects.", "paper_url": "http://arxiv.org/abs/2204.02390v1", "pdf_url": "http://arxiv.org/pdf/2204.02390v1", "repo_url": null}, "2204.02389": {"publish_time": "2022-04-05", "title": "ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer", "author": "Ruohan Gao et.al.", "abstract": "Objects play a crucial role in our everyday activities. Though multisensory object-centric learning has shown great potential lately, the modeling of objects in prior work is rather unrealistic. ObjectFolder 1.0 is a recent dataset that introduces 100 virtualized objects with visual, acoustic, and tactile sensory data. However, the dataset is small in scale and the multisensory data is of limited quality, hampering generalization to real-world scenarios. We present ObjectFolder 2.0, a large-scale, multisensory dataset of common household objects in the form of implicit neural representations that significantly enhances ObjectFolder 1.0 in three aspects. First, our dataset is 10 times larger in the amount of objects and orders of magnitude faster in rendering time. Second, we significantly improve the multisensory rendering quality for all three modalities. Third, we show that models learned from virtual objects in our dataset successfully transfer to their real-world counterparts in three challenging tasks: object scale estimation, contact localization, and shape reconstruction. ObjectFolder 2.0 offers a new path and testbed for multisensory learning in computer vision and robotics. The dataset is available at https://github.com/rhgao/ObjectFolder.", "paper_url": "http://arxiv.org/abs/2204.02389v1", "pdf_url": "http://arxiv.org/pdf/2204.02389v1", "repo_url": "https://github.com/rhgao/objectfolder"}, "2204.02968": {"publish_time": "2022-04-06", "title": "Temporal Alignment Networks for Long-term Video", "author": "Tengda Han et.al.", "abstract": "The objective of this paper is a temporal alignment network that ingests long term video sequences, and associated text sentences, in order to: (1) determine if a sentence is alignable with the video; and (2) if it is alignable, then determine its alignment. The challenge is to train such networks from large-scale datasets, such as HowTo100M, where the associated text sentences have significant noise, and are only weakly aligned when relevant. Apart from proposing the alignment network, we also make four contributions: (i) we describe a novel co-training method that enables to denoise and train on raw instructional videos without using manual annotation, despite the considerable noise; (ii) to benchmark the alignment performance, we manually curate a 10-hour subset of HowTo100M, totalling 80 videos, with sparse temporal descriptions. Our proposed model, trained on HowTo100M, outperforms strong baselines (CLIP, MIL-NCE) on this alignment dataset by a significant margin; (iii) we apply the trained model in the zero-shot settings to multiple downstream video understanding tasks and achieve state-of-the-art results, including text-video retrieval on YouCook2, and weakly supervised video action segmentation on Breakfast-Action; (iv) we use the automatically aligned HowTo100M annotations for end-to-end finetuning of the backbone model, and obtain improved performance on downstream action recognition tasks.", "paper_url": "http://arxiv.org/abs/2204.02968v1", "pdf_url": "http://arxiv.org/pdf/2204.02968v1", "repo_url": null}, "2204.02965": {"publish_time": "2022-04-06", "title": "LilNetX: Lightweight Networks with EXtreme Model Compression and Structured Sparsification", "author": "Sharath Girish et.al.", "abstract": "We introduce LilNetX, an end-to-end trainable technique for neural networks that enables learning models with specified accuracy-rate-computation trade-off. Prior works approach these problems one at a time and often require post-processing or multistage training which become less practical and do not scale very well for large datasets or architectures. Our method constructs a joint training objective that penalizes the self-information of network parameters in a reparameterized latent space to encourage small model size while also introducing priors to increase structured sparsity in the parameter space to reduce computation. We achieve up to 50% smaller model size and 98% model sparsity on ResNet-20 while retaining the same accuracy on the CIFAR-10 dataset as well as 35% smaller model size and 42% structured sparsity on ResNet-50 trained on ImageNet, when compared to existing state-of-the-art model compression methods. Code is available at https://github.com/Sharath-girish/LilNetX.", "paper_url": "http://arxiv.org/abs/2204.02965v1", "pdf_url": "http://arxiv.org/pdf/2204.02965v1", "repo_url": "https://github.com/sharath-girish/lilnetx"}, "2204.02964": {"publish_time": "2022-04-06", "title": "Unleashing Vanilla Vision Transformer with Masked Image Modeling for Object Detection", "author": "Yuxin Fang et.al.", "abstract": "We present an approach to efficiently and effectively adapt a masked image modeling (MIM) pre-trained vanilla Vision Transformer (ViT) for object detection, which is based on our two novel observations: (i) A MIM pre-trained vanilla ViT can work surprisingly well in the challenging object-level recognition scenario even with random sampled partial observations, e.g., only 25% ~ 50% of the input sequence. (ii) In order to construct multi-scale representations for object detection, a random initialized compact convolutional stem supplants the pre-trained large kernel patchify stem, and its intermediate features can naturally serve as the higher resolution inputs of a feature pyramid without upsampling. While the pre-trained ViT is only regarded as the third-stage of our detector's backbone instead of the whole feature extractor, resulting in a ConvNet-ViT hybrid architecture. The proposed detector, named MIMDet, enables a MIM pre-trained vanilla ViT to outperform hierarchical Swin Transformer by 2.3 box AP and 2.5 mask AP on COCO, and achieve even better results compared with other adapted vanilla ViT using a more modest fine-tuning recipe while converging 2.8x faster. Code and pre-trained models are available at \\url{https://github.com/hustvl/MIMDet}.", "paper_url": "http://arxiv.org/abs/2204.02964v1", "pdf_url": "http://arxiv.org/pdf/2204.02964v1", "repo_url": "https://github.com/hustvl/mimdet"}, "2204.02961": {"publish_time": "2022-04-06", "title": "SMU-Net: Style matching U-Net for brain tumor segmentation with missing modalities", "author": "Reza Azad et.al.", "abstract": "Gliomas are one of the most prevalent types of primary brain tumours, accounting for more than 30\\% of all cases and they develop from the glial stem or progenitor cells. In theory, the majority of brain tumours could well be identified exclusively by the use of Magnetic Resonance Imaging (MRI). Each MRI modality delivers distinct information on the soft tissue of the human brain and integrating all of them would provide comprehensive data for the accurate segmentation of the glioma, which is crucial for the patient's prognosis, diagnosis, and determining the best follow-up treatment. Unfortunately, MRI is prone to artifacts for a variety of reasons, which might result in missing one or more MRI modalities. Various strategies have been proposed over the years to synthesize the missing modality or compensate for the influence it has on automated segmentation models. However, these methods usually fail to model the underlying missing information. In this paper, we propose a style matching U-Net (SMU-Net) for brain tumour segmentation on MRI images. Our co-training approach utilizes a content and style-matching mechanism to distill the informative features from the full-modality network into a missing modality network. To do so, we encode both full-modality and missing-modality data into a latent space, then we decompose the representation space into a style and content representation. Our style matching module adaptively recalibrates the representation space by learning a matching function to transfer the informative and textural features from a full-modality path into a missing-modality path. Moreover, by modelling the mutual information, our content module surpasses the less informative features and re-calibrates the representation space based on discriminative semantic features. The evaluation process on the BraTS 2018 dataset shows a significant results.", "paper_url": "http://arxiv.org/abs/2204.02961v1", "pdf_url": "http://arxiv.org/pdf/2204.02961v1", "repo_url": "https://github.com/rezazad68/smunet"}, "2204.02960": {"publish_time": "2022-04-06", "title": "Simple and Effective Synthesis of Indoor 3D Scenes", "author": "Jing Yu Koh et.al.", "abstract": "We study the problem of synthesizing immersive 3D indoor scenes from one or more images. Our aim is to generate high-resolution images and videos from novel viewpoints, including viewpoints that extrapolate far beyond the input images while maintaining 3D consistency. Existing approaches are highly complex, with many separately trained stages and components. We propose a simple alternative: an image-to-image GAN that maps directly from reprojections of incomplete point clouds to full high-resolution RGB-D images. On the Matterport3D and RealEstate10K datasets, our approach significantly outperforms prior work when evaluated by humans, as well as on FID scores. Further, we show that our model is useful for generative data augmentation. A vision-and-language navigation (VLN) agent trained with trajectories spatially-perturbed by our model improves success rate by up to 1.5% over a state of the art baseline on the R2R benchmark. Our code will be made available to facilitate generative data augmentation and applications to downstream robotics and embodied AI tasks.", "paper_url": "http://arxiv.org/abs/2204.02960v1", "pdf_url": "http://arxiv.org/pdf/2204.02960v1", "repo_url": null}, "2204.03649": {"publish_time": "2022-04-07", "title": "Unsupervised Prompt Learning for Vision-Language Models", "author": "Tony Huang et.al.", "abstract": "Contrastive vision-language models like CLIP have shown great progress in zero-shot transfer learning. This new paradigm uses large-scale image-text pairs for training and aligns images and texts in a common embedding space. In the inference stage, the proper text description, known as prompt, needs to be carefully designed for zero-shot transfer. To avoid laborious prompt engineering and simultaneously improve transfer performance, recent works such as CoOp, CLIP-Adapter and Tip-Adapter propose to adapt vision-language models for downstream image recognition tasks by either optimizing the continuous prompt representations or training an additional adapter network on top of the pre-trained vision-language models on a small set of labeled data. Though promising improvements are achieved, using labeled images from target datasets may violate the intention of zero-shot transfer of pre-trained vision-language models. In this paper, we propose an unsupervised prompt learning (UPL) framework, which does not require any annotations of the target dataset, to improve the zero-shot transfer of CLIP-like vision-language models. Experimentally, for zero-shot transfer, our UPL outperforms original CLIP with prompt engineering and on ImageNet as well as other 10 datasets. An enhanced version of UPL is even on par with the 8-shot CoOp and the 8-shot TIP-Adapter on most datasets while our method does not need any labeled images for training. Code and models are available at https://github.com/tonyhuang2022/UPL.", "paper_url": "http://arxiv.org/abs/2204.03649v1", "pdf_url": "http://arxiv.org/pdf/2204.03649v1", "repo_url": "https://github.com/tonyhuang2022/upl"}, "2204.03648": {"publish_time": "2022-04-07", "title": "SunStage: Portrait Reconstruction and Relighting using the Sun as a Light Stage", "author": "Yifan Wang et.al.", "abstract": "Outdoor portrait photographs are often marred by the harsh shadows cast under direct sunlight. To resolve this, one can use post-capture lighting manipulation techniques, but these methods either require complex hardware (e.g., a light stage) to capture each individual, or rely on image-based priors and thus fail to reconstruct many of the subtle facial details that vary from person to person. In this paper, we present SunStage, a system for accurate, individually-tailored, and lightweight reconstruction of facial geometry and reflectance that can be used for general portrait relighting with cast shadows. Our method only requires the user to capture a selfie video outdoors, rotating in place, and uses the varying angles between the sun and the face as constraints in the joint reconstruction of facial geometry, reflectance properties, and lighting parameters. Aside from relighting, we show that our reconstruction can be used for applications like reflectance editing and view synthesis. Results and interactive demos are available at https://grail.cs.washington.edu/projects/sunstage/.", "paper_url": "http://arxiv.org/abs/2204.03648v1", "pdf_url": "http://arxiv.org/pdf/2204.03648v1", "repo_url": null}, "2204.03647": {"publish_time": "2022-04-07", "title": "Adapting CLIP For Phrase Localization Without Further Training", "author": "Jiahao Li et.al.", "abstract": "Supervised or weakly supervised methods for phrase localization (textual grounding) either rely on human annotations or some other supervised models, e.g., object detectors. Obtaining these annotations is labor-intensive and may be difficult to scale in practice. We propose to leverage recent advances in contrastive language-vision models, CLIP, pre-trained on image and caption pairs collected from the internet. In its original form, CLIP only outputs an image-level embedding without any spatial resolution. We adapt CLIP to generate high-resolution spatial feature maps. Importantly, we can extract feature maps from both ViT and ResNet CLIP model while maintaining the semantic properties of an image embedding. This provides a natural framework for phrase localization. Our method for phrase localization requires no human annotations or additional training. Extensive experiments show that our method outperforms existing no-training methods in zero-shot phrase localization, and in some cases, it even outperforms supervised methods. Code is available at https://github.com/pals-ttic/adapting-CLIP .", "paper_url": "http://arxiv.org/abs/2204.03647v1", "pdf_url": "http://arxiv.org/pdf/2204.03647v1", "repo_url": "https://github.com/pals-ttic/adapting-clip"}, "2204.03645": {"publish_time": "2022-04-07", "title": "DaViT: Dual Attention Vision Transformers", "author": "Mingyu Ding et.al.", "abstract": "In this work, we introduce Dual Attention Vision Transformers (DaViT), a simple yet effective vision transformer architecture that is able to capture global context while maintaining computational efficiency. We propose approaching the problem from an orthogonal angle: exploiting self-attention mechanisms with both \"spatial tokens\" and \"channel tokens\". With spatial tokens, the spatial dimension defines the token scope, and the channel dimension defines the token feature dimension. With channel tokens, we have the inverse: the channel dimension defines the token scope, and the spatial dimension defines the token feature dimension. We further group tokens along the sequence direction for both spatial and channel tokens to maintain the linear complexity of the entire model. We show that these two self-attentions complement each other: (i) since each channel token contains an abstract representation of the entire image, the channel attention naturally captures global interactions and representations by taking all spatial positions into account when computing attention scores between channels; (ii) the spatial attention refines the local representations by performing fine-grained interactions across spatial locations, which in turn helps the global information modeling in channel attention. Extensive experiments show our DaViT achieves state-of-the-art performance on four different tasks with efficient computations. Without extra data, DaViT-Tiny, DaViT-Small, and DaViT-Base achieve 82.8%, 84.2%, and 84.6% top-1 accuracy on ImageNet-1K with 28.3M, 49.7M, and 87.9M parameters, respectively. When we further scale up DaViT with 1.5B weakly supervised image and text pairs, DaViT-Gaint reaches 90.4% top-1 accuracy on ImageNet-1K. Code is available at https://github.com/dingmyu/davit.", "paper_url": "http://arxiv.org/abs/2204.03645v1", "pdf_url": "http://arxiv.org/pdf/2204.03645v1", "repo_url": "https://github.com/dingmyu/davit"}, "2204.03646": {"publish_time": "2022-04-07", "title": "FineDiving: A Fine-grained Dataset for Procedure-aware Action Quality Assessment", "author": "Jinglin Xu et.al.", "abstract": "Most existing action quality assessment methods rely on the deep features of an entire video to predict the score, which is less reliable due to the non-transparent inference process and poor interpretability. We argue that understanding both high-level semantics and internal temporal structures of actions in competitive sports videos is the key to making predictions accurate and interpretable. Towards this goal, we construct a new fine-grained dataset, called FineDiving, developed on diverse diving events with detailed annotations on action procedures. We also propose a procedure-aware approach for action quality assessment, learned by a new Temporal Segmentation Attention module. Specifically, we propose to parse pairwise query and exemplar action instances into consecutive steps with diverse semantic and temporal correspondences. The procedure-aware cross-attention is proposed to learn embeddings between query and exemplar steps to discover their semantic, spatial, and temporal correspondences, and further serve for fine-grained contrastive regression to derive a reliable scoring mechanism. Extensive experiments demonstrate that our approach achieves substantial improvements over state-of-the-art methods with better interpretability. The dataset and code are available at \\url{https://github.com/xujinglin/FineDiving}.", "paper_url": "http://arxiv.org/abs/2204.03646v1", "pdf_url": "http://arxiv.org/pdf/2204.03646v1", "repo_url": "https://github.com/xujinglin/finediving"}, "2204.04210": {"publish_time": "2022-04-08", "title": "Dancing under the stars: video denoising in starlight", "author": "Kristina Monakhova et.al.", "abstract": "Imaging in low light is extremely challenging due to low photon counts. Using sensitive CMOS cameras, it is currently possible to take videos at night under moonlight (0.05-0.3 lux illumination). In this paper, we demonstrate photorealistic video under starlight (no moon present, $<$0.001 lux) for the first time. To enable this, we develop a GAN-tuned physics-based noise model to more accurately represent camera noise at the lowest light levels. Using this noise model, we train a video denoiser using a combination of simulated noisy video clips and real noisy still images. We capture a 5-10 fps video dataset with significant motion at approximately 0.6-0.7 millilux with no active illumination. Comparing against alternative methods, we achieve improved video quality at the lowest light levels, demonstrating photorealistic video denoising in starlight for the first time.", "paper_url": "http://arxiv.org/abs/2204.04210v1", "pdf_url": "http://arxiv.org/pdf/2204.04210v1", "repo_url": null}, "2204.04199": {"publish_time": "2022-04-08", "title": "Underwater Image Enhancement Using Pre-trained Transformer", "author": "Abderrahmene Boudiaf et.al.", "abstract": "The goal of this work is to apply a denoising image transformer to remove the distortion from underwater images and compare it with other similar approaches. Automatic restoration of underwater images plays an important role since it allows to increase the quality of the images, without the need for more expensive equipment. This is a critical example of the important role of the machine learning algorithms to support marine exploration and monitoring, reducing the need for human intervention like the manual processing of the images, thus saving time, effort, and cost. This paper is the first application of the image transformer-based approach called \"Pre-Trained Image Processing Transformer\" to underwater images. This approach is tested on the UFO-120 dataset, containing 1500 images with the corresponding clean images.", "paper_url": "http://arxiv.org/abs/2204.04199v1", "pdf_url": "http://arxiv.org/pdf/2204.04199v1", "repo_url": null}, "2204.04153": {"publish_time": "2022-04-08", "title": "Particle Videos Revisited: Tracking Through Occlusions Using Point Trajectories", "author": "Adam W. Harley et.al.", "abstract": "Tracking pixels in videos is typically studied as an optical flow estimation problem, where every pixel is described with a displacement vector that locates it in the next frame. Even though wider temporal context is freely available, prior efforts to take this into account have yielded only small gains over 2-frame methods. In this paper, we revisit Sand and Teller's \"particle video\" approach, and study pixel tracking as a long-range motion estimation problem, where every pixel is described with a trajectory that locates it in multiple future frames. We re-build this classic approach using components that drive the current state-of-the-art in flow and object tracking, such as dense cost maps, iterative optimization, and learned appearance updates. We train our models using long-range amodal point trajectories mined from existing optical flow datasets that we synthetically augment with occlusions. We test our approach in trajectory estimation benchmarks and in keypoint label propagation tasks, and compare favorably against state-of-the-art optical flow and feature tracking methods.", "paper_url": "http://arxiv.org/abs/2204.04153v1", "pdf_url": "http://arxiv.org/pdf/2204.04153v1", "repo_url": null}, "2204.04151": {"publish_time": "2022-04-08", "title": "A Video Anomaly Detection Framework based on Appearance-Motion Semantics Representation Consistency", "author": "Xiangyu Huang et.al.", "abstract": "Video anomaly detection refers to the identification of events that deviate from the expected behavior. Due to the lack of anomalous samples in training, video anomaly detection becomes a very challenging task. Existing methods almost follow a reconstruction or future frame prediction mode. However, these methods ignore the consistency between appearance and motion information of samples, which limits their anomaly detection performance. Anomalies only occur in the moving foreground of surveillance videos, so the semantics expressed by video frame sequences and optical flow without background information in anomaly detection should be highly consistent and significant for anomaly detection. Based on this idea, we propose Appearance-Motion Semantics Representation Consistency (AMSRC), a framework that uses normal data's appearance and motion semantic representation consistency to handle anomaly detection. Firstly, we design a two-stream encoder to encode the appearance and motion information representations of normal samples and introduce constraints to further enhance the consistency of the feature semantics between appearance and motion information of normal samples so that abnormal samples with low consistency appearance and motion feature representation can be identified. Moreover, the lower consistency of appearance and motion features of anomalous samples can be used to generate predicted frames with larger reconstruction error, which makes anomalies easier to spot. Experimental results demonstrate the effectiveness of the proposed method.", "paper_url": "http://arxiv.org/abs/2204.04151v1", "pdf_url": "http://arxiv.org/pdf/2204.04151v1", "repo_url": null}, "2204.04145": {"publish_time": "2022-04-08", "title": "Constrained Bundle Adjustment for Structure From Motion Using Uncalibrated Multi-Camera Systems", "author": "Debao Huang et.al.", "abstract": "Structure from motion using uncalibrated multi-camera systems is a challenging task. This paper proposes a bundle adjustment solution that implements a baseline constraint respecting that these cameras are static to each other. We assume these cameras are mounted on a mobile platform, uncalibrated, and coarsely synchronized. To this end, we propose the baseline constraint that is formulated for the scenario in which the cameras have overlapping views. The constraint is incorporated in the bundle adjustment solution to keep the relative motion of different cameras static. Experiments were conducted using video frames of two collocated GoPro cameras mounted on a vehicle with no system calibration. These two cameras were placed capturing overlapping contents. We performed our bundle adjustment using the proposed constraint and then produced 3D dense point clouds. Evaluations were performed by comparing these dense point clouds against LiDAR reference data. We showed that, as compared to traditional bundle adjustment, our proposed method achieved an improvement of 29.38%.", "paper_url": "http://arxiv.org/abs/2204.04145v1", "pdf_url": "http://arxiv.org/pdf/2204.04145v1", "repo_url": null}, "2204.05308": {"publish_time": "2022-04-11", "title": "On the Generalization of BasicVSR++ to Video Deblurring and Denoising", "author": "Kelvin C. K. Chan et.al.", "abstract": "The exploitation of long-term information has been a long-standing problem in video restoration. The recent BasicVSR and BasicVSR++ have shown remarkable performance in video super-resolution through long-term propagation and effective alignment. Their success has led to a question of whether they can be transferred to different video restoration tasks. In this work, we extend BasicVSR++ to a generic framework for video restoration tasks. In tasks where inputs and outputs possess identical spatial size, the input resolution is reduced by strided convolutions to maintain efficiency. With only minimal changes from BasicVSR++, the proposed framework achieves compelling performance with great efficiency in various video restoration tasks including video deblurring and denoising. Notably, BasicVSR++ achieves comparable performance to Transformer-based approaches with up to 79% of parameter reduction and 44x speedup. The promising results demonstrate the importance of propagation and alignment in video restoration tasks beyond just video super-resolution. Code and models are available at https://github.com/ckkelvinchan/BasicVSR_PlusPlus.", "paper_url": "http://arxiv.org/abs/2204.05308v1", "pdf_url": "http://arxiv.org/pdf/2204.05308v1", "repo_url": null}, "2204.05306": {"publish_time": "2022-04-11", "title": "Full-Spectrum Out-of-Distribution Detection", "author": "Jingkang Yang et.al.", "abstract": "Existing out-of-distribution (OOD) detection literature clearly defines semantic shift as a sign of OOD but does not have a consensus over covariate shift. Samples experiencing covariate shift but not semantic shift are either excluded from the test set or treated as OOD, which contradicts the primary goal in machine learning -- being able to generalize beyond the training distribution. In this paper, we take into account both shift types and introduce full-spectrum OOD (FS-OOD) detection, a more realistic problem setting that considers both detecting semantic shift and being tolerant to covariate shift; and designs three benchmarks. These new benchmarks have a more fine-grained categorization of distributions (i.e., training ID, covariate-shifted ID, near-OOD, and far-OOD) for the purpose of more comprehensively evaluating the pros and cons of algorithms. To address the FS-OOD detection problem, we propose SEM, a simple feature-based semantics score function. SEM is mainly composed of two probability measures: one is based on high-level features containing both semantic and non-semantic information, while the other is based on low-level feature statistics only capturing non-semantic image styles. With a simple combination, the non-semantic part is cancelled out, which leaves only semantic information in SEM that can better handle FS-OOD detection. Extensive experiments on the three new benchmarks show that SEM significantly outperforms current state-of-the-art methods. Our code and benchmarks are released in https://github.com/Jingkang50/OpenOOD.", "paper_url": "http://arxiv.org/abs/2204.05306v1", "pdf_url": "http://arxiv.org/pdf/2204.05306v1", "repo_url": "https://github.com/jingkang50/openood"}, "2204.05300": {"publish_time": "2022-04-11", "title": "Single-Photon Structured Light", "author": "Varun Sundar et.al.", "abstract": "We present a novel structured light technique that uses Single Photon Avalanche Diode (SPAD) arrays to enable 3D scanning at high-frame rates and low-light levels. This technique, called \"Single-Photon Structured Light\", works by sensing binary images that indicates the presence or absence of photon arrivals during each exposure; the SPAD array is used in conjunction with a high-speed binary projector, with both devices operated at speeds as high as 20~kHz. The binary images that we acquire are heavily influenced by photon noise and are easily corrupted by ambient sources of light. To address this, we develop novel temporal sequences using error correction codes that are designed to be robust to short-range effects like projector and camera defocus as well as resolution mismatch between the two devices. Our lab prototype is capable of 3D imaging in challenging scenarios involving objects with extremely low albedo or undergoing fast motion, as well as scenes under strong ambient illumination.", "paper_url": "http://arxiv.org/abs/2204.05300v1", "pdf_url": "http://arxiv.org/pdf/2204.05300v1", "repo_url": null}, "2204.05289": {"publish_time": "2022-04-11", "title": "Towards Online Domain Adaptive Object Detection", "author": "Vibashan VS et.al.", "abstract": "Existing object detection models assume both the training and test data are sampled from the same source domain. This assumption does not hold true when these detectors are deployed in real-world applications, where they encounter new visual domain. Unsupervised Domain Adaptation (UDA) methods are generally employed to mitigate the adverse effects caused by domain shift. Existing UDA methods operate in an offline manner where the model is first adapted towards the target domain and then deployed in real-world applications. However, this offline adaptation strategy is not suitable for real-world applications as the model frequently encounters new domain shifts. Hence, it becomes critical to develop a feasible UDA method that generalizes to these domain shifts encountered during deployment time in a continuous online manner. To this end, we propose a novel unified adaptation framework that adapts and improves generalization on the target domain in online settings. In particular, we introduce MemXformer - a cross-attention transformer-based memory module where items in the memory take advantage of domain shifts and record prototypical patterns of the target distribution. Further, MemXformer produces strong positive and negative pairs to guide a novel contrastive loss, which enhances target specific representation learning. Experiments on diverse detection benchmarks show that the proposed strategy can produce state-of-the-art performance in both online and offline settings. To the best of our knowledge, this is the first work to address online and offline adaptation settings for object detection. Code at https://github.com/Vibashan/online-od", "paper_url": "http://arxiv.org/abs/2204.05289v1", "pdf_url": "http://arxiv.org/pdf/2204.05289v1", "repo_url": "https://github.com/vibashan/online-od"}, "2204.05281": {"publish_time": "2022-04-11", "title": "Physically Disentangled Representations", "author": "Tzofi Klinghoffer et.al.", "abstract": "State-of-the-art methods in generative representation learning yield semantic disentanglement, but typically do not consider physical scene parameters, such as geometry, albedo, lighting, or camera. We posit that inverse rendering, a way to reverse the rendering process to recover scene parameters from an image, can also be used to learn physically disentangled representations of scenes without supervision. In this paper, we show the utility of inverse rendering in learning representations that yield improved accuracy on downstream clustering, linear classification, and segmentation tasks with the help of our novel Leave-One-Out, Cycle Contrastive loss (LOOCC), which improves disentanglement of scene parameters and robustness to out-of-distribution lighting and viewpoints. We perform a comparison of our method with other generative representation learning methods across a variety of downstream tasks, including face attribute classification, emotion recognition, identification, face segmentation, and car classification. Our physically disentangled representations yield higher accuracy than semantically disentangled alternatives across all tasks and by as much as 18%. We hope that this work will motivate future research in applying advances in inverse rendering and 3D understanding to representation learning.", "paper_url": "http://arxiv.org/abs/2204.05281v1", "pdf_url": "http://arxiv.org/pdf/2204.05281v1", "repo_url": null}, "2204.05994": {"publish_time": "2022-04-12", "title": "Malceiver: Perceiver with Hierarchical and Multi-modal Features for Android Malware Detection", "author": "Niall McLaughlin et.al.", "abstract": "We propose the Malceiver, a hierarchical Perceiver model for Android malware detection that makes use of multi-modal features. The primary inputs are the opcode sequence and the requested permissions of a given Android APK file. To reach a malware classification decision the model combines hierarchical features extracted from the opcode sequence together with the requested permissions. The model's architecture is based on the Perceiver/PerceiverIO which allows for very long opcode sequences to be processed efficiently. Our proposed model can be easily extended to use multi-modal features. We show experimentally that this model outperforms a conventional CNN architecture for opcode sequence based malware detection. We then show that using additional modalities improves performance. Our proposed architecture opens new avenues for the use of Transformer-style networks in malware research.", "paper_url": "http://arxiv.org/abs/2204.05994v1", "pdf_url": "http://arxiv.org/pdf/2204.05994v1", "repo_url": null}, "2204.05991": {"publish_time": "2022-04-12", "title": "ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension", "author": "Sanjay Subramanian et.al.", "abstract": "Training a referring expression comprehension (ReC) model for a new visual domain requires collecting referring expressions, and potentially corresponding bounding boxes, for images in the domain. While large-scale pre-trained models are useful for image classification across domains, it remains unclear if they can be applied in a zero-shot manner to more complex tasks like ReC. We present ReCLIP, a simple but strong zero-shot baseline that repurposes CLIP, a state-of-the-art large-scale model, for ReC. Motivated by the close connection between ReC and CLIP's contrastive pre-training objective, the first component of ReCLIP is a region-scoring method that isolates object proposals via cropping and blurring, and passes them to CLIP. However, through controlled experiments on a synthetic dataset, we find that CLIP is largely incapable of performing spatial reasoning off-the-shelf. Thus, the second component of ReCLIP is a spatial relation resolver that handles several types of spatial relations. We reduce the gap between zero-shot baselines from prior work and supervised models by as much as 29% on RefCOCOg, and on RefGTA (video game imagery), ReCLIP's relative improvement over supervised ReC models trained on real images is 8%.", "paper_url": "http://arxiv.org/abs/2204.05991v1", "pdf_url": "http://arxiv.org/pdf/2204.05991v1", "repo_url": "https://github.com/allenai/reclip"}, "2204.05986": {"publish_time": "2022-04-12", "title": "Machine Learning Security against Data Poisoning: Are We There Yet?", "author": "Antonio Emanuele Cin\u00e0 et.al.", "abstract": "The recent success of machine learning has been fueled by the increasing availability of computing power and large amounts of data in many different applications. However, the trustworthiness of the resulting models can be compromised when such data is maliciously manipulated to mislead the learning process. In this article, we first review poisoning attacks that compromise the training data used to learn machine-learning models, including attacks that aim to reduce the overall performance, manipulate the predictions on specific test samples, and even implant backdoors in the model. We then discuss how to mitigate these attacks before, during, and after model training. We conclude our article by formulating some relevant open challenges which are hindering the development of testing methods and benchmarks suitable for assessing and improving the trustworthiness of machine-learning models against data poisoning attacks.", "paper_url": "http://arxiv.org/abs/2204.05986v1", "pdf_url": "http://arxiv.org/pdf/2204.05986v1", "repo_url": null}, "2204.05976": {"publish_time": "2022-04-12", "title": "Video Captioning: a comparative review of where we are and which could be the route", "author": "Daniela Moctezuma et.al.", "abstract": "Video captioning is the process of describing the content of a sequence of images capturing its semantic relationships and meanings. Dealing with this task with a single image is arduous, not to mention how difficult it is for a video (or images sequence). The amount and relevance of the applications of video captioning are vast, mainly to deal with a significant amount of video recordings in video surveillance, or assisting people visually impaired, to mention a few. To analyze where the efforts of our community to solve the video captioning task are, as well as what route could be better to follow, this manuscript presents an extensive review of more than 105 papers for the period of 2016 to 2021. As a result, the most-used datasets and metrics are identified. Also, the main approaches used and the best ones. We compute a set of rankings based on several performance metrics to obtain, according to its performance, the best method with the best result on the video captioning task. Finally, some insights are concluded about which could be the next steps or opportunity areas to improve dealing with this complex task.", "paper_url": "http://arxiv.org/abs/2204.05976v1", "pdf_url": "http://arxiv.org/pdf/2204.05976v1", "repo_url": null}, "2204.05957": {"publish_time": "2022-04-12", "title": "Localization Distillation for Object Detection", "author": "Zhaohui Zheng et.al.", "abstract": "Previous knowledge distillation (KD) methods for object detection mostly focus on feature imitation instead of mimicking the classification logits due to its inefficiency in distilling the localization information. In this paper, we investigate whether logit mimicking always lags behind feature imitation. Towards this goal, we first present a novel localization distillation (LD) method which can efficiently transfer the localization knowledge from the teacher to the student. Second, we introduce the concept of valuable localization region that can aid to selectively distill the classification and localization knowledge for a certain region. Combining these two new components, for the first time, we show that logit mimicking can outperform feature imitation and the absence of localization distillation is a critical reason for why logit mimicking underperforms for years. The thorough studies exhibit the great potential of logit mimicking that can significantly alleviate the localization ambiguity, learn robust feature representation, and ease the training difficulty in the early stage. We also provide the theoretical connection between the proposed LD and the classification KD, that they share the equivalent optimization effect. Our distillation scheme is simple as well as effective and can be easily applied to both dense horizontal object detectors and rotated object detectors. Extensive experiments on the MS COCO, PASCAL VOC, and DOTA benchmarks demonstrate that our method can achieve considerable AP improvement without any sacrifice on the inference speed. Our source code and pretrained models are publicly available at https://github.com/HikariTJU/LD.", "paper_url": "http://arxiv.org/abs/2204.05957v1", "pdf_url": "http://arxiv.org/pdf/2204.05957v1", "repo_url": "https://github.com/HikariTJU/LD"}}}, "NLP": {"NLP": {"2202.12875": {"publish_time": "2022-02-25", "title": "DataLab: A Platform for Data Analysis and Intervention", "author": "Yang Xiao et.al.", "abstract": "Despite data's crucial role in machine learning, most existing tools and research tend to focus on systems on top of existing data rather than how to interpret and manipulate data. In this paper, we propose DataLab, a unified data-oriented platform that not only allows users to interactively analyze the characteristics of data, but also provides a standardized interface for different data processing operations. Additionally, in view of the ongoing proliferation of datasets, \\toolname has features for dataset recommendation and global vision analysis that help researchers form a better view of the data ecosystem. So far, DataLab covers 1,715 datasets and 3,583 of its transformed version (e.g., hyponyms replacement), where 728 datasets support various analyses (e.g., with respect to gender bias) with the help of 140M samples annotated by 318 feature functions. DataLab is under active development and will be supported going forward. We have released a web platform, web API, Python SDK, PyPI published package and online documentation, which hopefully, can meet the diverse needs of researchers.", "paper_url": "http://arxiv.org/abs/2202.12875v1", "pdf_url": "http://arxiv.org/pdf/2202.12875v1", "repo_url": null}, "2202.12837": {"publish_time": "2022-02-25", "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?", "author": "Sewon Min et.al.", "abstract": "Large language models (LMs) are able to in-context learn -- perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required -- randomly replacing labels in the demonstrations barely hurts performance, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.", "paper_url": "http://arxiv.org/abs/2202.12837v1", "pdf_url": "http://arxiv.org/pdf/2202.12837v1", "repo_url": null}, "2202.12832": {"publish_time": "2022-02-25", "title": "Morphology Without Borders: Clause-Level Morphological Annotation", "author": "Omer Goldman et.al.", "abstract": "Morphological tasks use large multi-lingual datasets that organize words into inflection tables, which then serve as training and evaluation data for various tasks. However, a closer inspection of these data reveals profound cross-linguistic inconsistencies, that arise from the lack of a clear linguistic and operational definition of what is a word, and that severely impair the universality of the derived tasks. To overcome this deficiency, we propose to view morphology as a clause-level phenomenon, rather than word-level. It is anchored in a fixed yet inclusive set of features homogeneous across languages, that encapsulates all functions realized in a saturated clause. We deliver MightyMorph, a novel dataset for clause-level morphology covering 4 typologically-different languages: English, German, Turkish and Hebrew. We use this dataset to derive 3 clause-level morphological tasks: inflection, reinflection and analysis. Our experiments show that the clause-level tasks are substantially harder than the respective word-level tasks, while having comparable complexity across languages. Furthermore, redefining morphology to the clause-level provides a neat interface with contextualized language models (LMs) and can be used to probe LMs capacity to encode complex morphology. Taken together, this work opens up new horizons in the study of computational morphology, leaving ample space for studying neural morphological modeling cross-linguistically.", "paper_url": "http://arxiv.org/abs/2202.12832v1", "pdf_url": "http://arxiv.org/pdf/2202.12832v1", "repo_url": null}, "2202.12814": {"publish_time": "2022-02-25", "title": "The Reality of Multi-Lingual Machine Translation", "author": "Tom Kocmi et.al.", "abstract": "Our book \"The Reality of Multi-Lingual Machine Translation\" discusses the benefits and perils of using more than two languages in machine translation systems. While focused on the particular task of sequence-to-sequence processing and multi-task learning, the book targets somewhat beyond the area of natural language processing. Machine translation is for us a prime example of deep learning applications where human skills and learning capabilities are taken as a benchmark that many try to match and surpass. We document that some of the gains observed in multi-lingual translation may result from simpler effects than the assumed cross-lingual transfer of knowledge.   In the first, rather general part, the book will lead you through the motivation for multi-linguality, the versatility of deep neural networks especially in sequence-to-sequence tasks to complications of this learning. We conclude the general part with warnings against too optimistic and unjustified explanations of the gains that neural networks demonstrate.   In the second part, we fully delve into multi-lingual models, with a particularly careful examination of transfer learning as one of the more straightforward approaches utilizing additional languages. The recent multi-lingual techniques, including massive models, are surveyed and practical aspects of deploying systems for many languages are discussed. The conclusion highlights the open problem of machine understanding and reminds of two ethical aspects of building large-scale models: the inclusivity of research and its ecological trace.", "paper_url": "http://arxiv.org/abs/2202.12814v1", "pdf_url": "http://arxiv.org/pdf/2202.12814v1", "repo_url": null}, "2202.12801": {"publish_time": "2022-02-25", "title": "On the data requirements of probing", "author": "Zining Zhu et.al.", "abstract": "As large and powerful neural language models are developed, researchers have been increasingly interested in developing diagnostic tools to probe them. There are many papers with conclusions of the form \"observation X is found in model Y\", using their own datasets with varying sizes. Larger probing datasets bring more reliability, but are also expensive to collect. There is yet to be a quantitative method for estimating reasonable probing dataset sizes. We tackle this omission in the context of comparing two probing configurations: after we have collected a small dataset from a pilot study, how many additional data samples are sufficient to distinguish two different configurations? We present a novel method to estimate the required number of data samples in such experiments and, across several case studies, we verify that our estimations have sufficient statistical power. Our framework helps to systematically construct probing datasets to diagnose neural NLP models.", "paper_url": "http://arxiv.org/abs/2202.12801v1", "pdf_url": "http://arxiv.org/pdf/2202.12801v1", "repo_url": null}, "2202.14035": {"publish_time": "2022-02-28", "title": "ParaNames: A Massively Multilingual Entity Name Corpus", "author": "Jonne S\u00e4lev\u00e4 et.al.", "abstract": "This preprint describes work in progress on ParaNames, a multilingual parallel name resource consisting of names for approximately 14 million entities. The included names span over 400 languages, and almost all entities are mapped to standardized entity types (PER/LOC/ORG). Using Wikidata as a source, we create the largest resource of this type to-date. We describe our approach to filtering and standardizing the data to provide the best quality possible. ParaNames is useful for multilingual language processing, both in defining tasks for name translation/transliteration and as supplementary data for tasks such as named entity recognition and linking. Our resource is released on GitHub (https://github.com/bltlab/paranames) under a Creative Commons license (CC BY 4.0).", "paper_url": "http://arxiv.org/abs/2202.14035v1", "pdf_url": "http://arxiv.org/pdf/2202.14035v1", "repo_url": null}, "2202.13972": {"publish_time": "2022-02-28", "title": "The impact of lexical and grammatical processing on generating code from natural language", "author": "Nathana\u00ebl Beau et.al.", "abstract": "Considering the seq2seq architecture of TranX for natural language to code translation, we identify four key components of importance: grammatical constraints, lexical preprocessing, input representations, and copy mechanisms. To study the impact of these components, we use a state-of-the-art architecture that relies on BERT encoder and a grammar-based decoder for which a formalization is provided. The paper highlights the importance of the lexical substitution component in the current natural language to code systems.", "paper_url": "http://arxiv.org/abs/2202.13972v1", "pdf_url": "http://arxiv.org/pdf/2202.13972v1", "repo_url": null}, "2202.13914": {"publish_time": "2022-02-28", "title": "Combining Modular Skills in Multitask Learning", "author": "Edoardo M. Ponti et.al.", "abstract": "A modular design encourages neural models to disentangle and recombine different facets of knowledge to generalise more systematically to new tasks. In this work, we assume that each task is associated with a subset of latent discrete skills from a (potentially small) inventory. In turn, skills correspond to parameter-efficient (sparse / low-rank) model parameterisations. By jointly learning these and a task-skill allocation matrix, the network for each task is instantiated as the average of the parameters of active skills. To favour non-trivial soft partitions of skills across tasks, we experiment with a series of inductive biases, such as an Indian Buffet Process prior and a two-speed learning rate. We evaluate our latent-skill model on two main settings: 1) multitask reinforcement learning for grounded instruction following on 8 levels of the BabyAI platform; and 2) few-shot adaptation of pre-trained text-to-text generative models on CrossFit, a benchmark comprising 160 NLP tasks. We find that the modular design of a network significantly increases sample efficiency in reinforcement learning and few-shot generalisation in supervised learning, compared to baselines with fully shared, task-specific, or conditionally generated parameters where knowledge is entangled across tasks. In addition, we show how discrete skills help interpretability, as they yield an explicit hierarchy of tasks.", "paper_url": "http://arxiv.org/abs/2202.13914v1", "pdf_url": "http://arxiv.org/pdf/2202.13914v1", "repo_url": null}, "2202.13887": {"publish_time": "2022-02-28", "title": "Probing the Robustness of Trained Metrics for Conversational Dialogue Systems", "author": "Jan Deriu et.al.", "abstract": "This paper introduces an adversarial method to stress-test trained metrics to evaluate conversational dialogue systems. The method leverages Reinforcement Learning to find response strategies that elicit optimal scores from the trained metrics. We apply our method to test recently proposed trained metrics. We find that they all are susceptible to giving high scores to responses generated by relatively simple and obviously flawed strategies that our method converges on. For instance, simply copying parts of the conversation context to form a response yields competitive scores or even outperforms responses written by humans.", "paper_url": "http://arxiv.org/abs/2202.13887v1", "pdf_url": "http://arxiv.org/pdf/2202.13887v1", "repo_url": null}, "2202.13876": {"publish_time": "2022-02-28", "title": "PMC-Patients: A Large-scale Dataset of Patient Notes and Relations Extracted from Case Reports in PubMed Central", "author": "Zhengyun Zhao et.al.", "abstract": "We present PMC-Patients, a dataset consisting of 167k patient notes with 3.1M relevant article annotations and 293k similar patient annotations. The patient notes are extracted by identifying certain sections from case reports in PubMed Central, and those with at least CC BY-NC-SA license are re-distributed. Patient-article relevance and patient-patient similarity are defined by citation relationships in PubMed. We also perform four tasks with PMC-Patients to demonstrate its utility, including Patient Note Recognition (PNR), Patient-Patient Similarity (PPS), Patient-Patient Retrieval (PPR), and Patient-Article Retrieval (PAR). In summary, PMC-Patients provides the largest-scale patient notes with high quality, diverse conditions, easy access, and rich annotations.", "paper_url": "http://arxiv.org/abs/2202.13876v1", "pdf_url": "http://arxiv.org/pdf/2202.13876v1", "repo_url": "https://github.com/zhao-zy15/pmc-patients"}, "2203.00674": {"publish_time": "2022-03-01", "title": "Advancing an Interdisciplinary Science of Conversation: Insights from a Large Multimodal Corpus of Human Speech", "author": "Andrew Reece et.al.", "abstract": "People spend a substantial portion of their lives engaged in conversation, and yet our scientific understanding of conversation is still in its infancy. In this report we advance an interdisciplinary science of conversation, with findings from a large, novel, multimodal corpus of 1,656 recorded conversations in spoken English. This 7+ million word, 850 hour corpus totals over 1TB of audio, video, and transcripts, with moment-to-moment measures of vocal, facial, and semantic expression, along with an extensive survey of speaker post conversation reflections. We leverage the considerable scope of the corpus to (1) extend key findings from the literature, such as the cooperativeness of human turn-taking; (2) define novel algorithmic procedures for the segmentation of speech into conversational turns; (3) apply machine learning insights across various textual, auditory, and visual features to analyze what makes conversations succeed or fail; and (4) explore how conversations are related to well-being across the lifespan. We also report (5) a comprehensive mixed-method report, based on quantitative analysis and qualitative review of each recording, that showcases how individuals from diverse backgrounds alter their communication patterns and find ways to connect. We conclude with a discussion of how this large-scale public dataset may offer new directions for future research, especially across disciplinary boundaries, as scholars from a variety of fields appear increasingly interested in the study of conversation.", "paper_url": "http://arxiv.org/abs/2203.00674v1", "pdf_url": "http://arxiv.org/pdf/2203.00674v1", "repo_url": null}, "2203.00648": {"publish_time": "2022-03-01", "title": "Measuring the Impact of Individual Domain Factors in Self-Supervised Pre-Training", "author": "Ramon Sanabria et.al.", "abstract": "Human speech data comprises a rich set of domain factors such as accent, syntactic and semantic variety, or acoustic environment. Previous work explores the effect of domain mismatch in automatic speech recognition between pre-training and fine-tuning as a whole but does not dissect the contribution of individual factors. In this paper, we present a controlled study to better understand the effect of such factors on the performance of pre-trained representations. To do so, we pre-train models either on modified natural speech or synthesized audio, with a single domain factor modified, and then measure performance on automatic speech recognition after fine tuning. Results show that phonetic domain factors play an important role during pre-training while grammatical and syntactic factors are far less important. To our knowledge, this is the first study to better understand the domain characteristics in self-supervised pre-training for speech.", "paper_url": "http://arxiv.org/abs/2203.00648v1", "pdf_url": "http://arxiv.org/pdf/2203.00648v1", "repo_url": null}, "2203.00633": {"publish_time": "2022-03-01", "title": "Transformer Grammars: Augmenting Transformer Language Models with Syntactic Inductive Biases at Scale", "author": "Laurent Sartran et.al.", "abstract": "Transformer language models that are trained on vast amounts of data have achieved remarkable success at various NLP benchmarks. Intriguingly, this success is achieved by models that lack an explicit modeling of hierarchical syntactic structures, which were hypothesized by decades of linguistic research to be necessary for good generalization. This naturally leaves a question: to what extent can we further improve the performance of Transformer language models, through an inductive bias that encourages the model to explain the data through the lens of recursive syntactic compositions? Although the benefits of modeling recursive syntax have been shown at the small data and model scales, it remains an open question whether -- and to what extent -- a similar design principle is still beneficial in the case of powerful Transformer language models that work well at scale. To answer these questions, we introduce Transformer Grammars -- a novel class of Transformer language models that combine: (i) the expressive power, scalability, and strong performance of Transformers, and (ii) recursive syntactic compositions, which here are implemented through a special attention mask. We find that Transformer Grammars outperform various strong baselines on multiple syntax-sensitive language modeling evaluation metrics, in addition to sentence-level language modeling perplexity. Nevertheless, we find that the recursive syntactic composition bottleneck harms perplexity on document-level modeling, providing evidence that a different kind of memory mechanism -- that works independently of syntactic structures -- plays an important role in the processing of long-form text.", "paper_url": "http://arxiv.org/abs/2203.00633v1", "pdf_url": "http://arxiv.org/pdf/2203.00633v1", "repo_url": null}, "2203.00613": {"publish_time": "2022-03-01", "title": "Towards a Common Speech Analysis Engine", "author": "Hagai Aronowitz et.al.", "abstract": "Recent innovations in self-supervised representation learning have led to remarkable advances in natural language processing. That said, in the speech processing domain, self-supervised representation learning-based systems are not yet considered state-of-the-art. We propose leveraging recent advances in self-supervised-based speech processing to create a common speech analysis engine. Such an engine should be able to handle multiple speech processing tasks, using a single architecture, to obtain state-of-the-art accuracy. The engine must also enable support for new tasks with small training datasets. Beyond that, a common engine should be capable of supporting distributed training with client in-house private data. We present the architecture for a common speech analysis engine based on the HuBERT self-supervised speech representation. Based on experiments, we report our results for language identification and emotion recognition on the standard evaluations NIST-LRE 07 and IEMOCAP. Our results surpass the state-of-the-art performance reported so far on these tasks. We also analyzed our engine on the emotion recognition task using reduced amounts of training data and show how to achieve improved results.", "paper_url": "http://arxiv.org/abs/2203.00613v1", "pdf_url": "http://arxiv.org/pdf/2203.00613v1", "repo_url": null}, "2203.00588": {"publish_time": "2022-03-01", "title": "Structural invariants and semantic fingerprints in the \"ego network\" of words", "author": "Kilian Ollivier et.al.", "abstract": "Well-established cognitive models coming from anthropology have shown that, due to the cognitive constraints that limit our \"bandwidth\" for social interactions, humans organize their social relations according to a regular structure. In this work, we postulate that similar regularities can be found in other cognitive processes, such as those involving language production. In order to investigate this claim, we analyse a dataset containing tweets of a heterogeneous group of Twitter users (regular users and professional writers). Leveraging a methodology similar to the one used to uncover the well-established social cognitive constraints, we find regularities at both the structural and semantic level. At the former, we find that a concentric layered structure (which we call ego network of words, in analogy to the ego network of social relationships) very well captures how individuals organise the words they use. The size of the layers in this structure regularly grows (approximately 2-3 times with respect to the previous one) when moving outwards, and the two penultimate external layers consistently account for approximately 60% and 30% of the used words, irrespective of the number of the total number of layers of the user. For the semantic analysis, each ring of each ego network is described by a semantic profile, which captures the topics associated with the words in the ring. We find that ring #1 has a special role in the model. It is semantically the most dissimilar and the most diverse among the rings. We also show that the topics that are important in the innermost ring also have the characteristic of being predominant in each of the other rings, as well as in the entire ego network. In this respect, ring #1 can be seen as the semantic fingerprint of the ego network of words.", "paper_url": "http://arxiv.org/abs/2203.00588v1", "pdf_url": "http://arxiv.org/pdf/2203.00588v1", "repo_url": null}, "2203.01311": {"publish_time": "2022-03-02", "title": "HighMMT: Towards Modality and Task Generalization for High-Modality Representation Learning", "author": "Paul Pu Liang et.al.", "abstract": "Learning multimodal representations involves discovering correspondences and integrating information from multiple heterogeneous sources of data. While recent research has begun to explore the design of more general-purpose multimodal models (contrary to prior focus on domain and modality-specific architectures), these methods are still largely focused on a small set of modalities in the language, vision, and audio space. In order to accelerate generalization towards diverse and understudied modalities, we investigate methods for high-modality (a large set of diverse modalities) and partially-observable (each task only defined on a small subset of modalities) scenarios. To tackle these challenges, we design a general multimodal model that enables multitask and transfer learning: multitask learning with shared parameters enables stable parameter counts (addressing scalability), and cross-modal transfer learning enables information sharing across modalities and tasks (addressing partial observability). Our resulting model generalizes across text, image, video, audio, time-series, sensors, tables, and set modalities from different research areas, improves the tradeoff between performance and efficiency, transfers to new modalities and tasks, and reveals surprising insights on the nature of information sharing in multitask models. We release our code and benchmarks which we hope will present a unified platform for subsequent theoretical and empirical analysis: https://github.com/pliang279/HighMMT.", "paper_url": "http://arxiv.org/abs/2203.01311v1", "pdf_url": "http://arxiv.org/pdf/2203.01311v1", "repo_url": null}, "2203.01294": {"publish_time": "2022-03-02", "title": "Providing Insights for Open-Response Surveys via End-to-End Context-Aware Clustering", "author": "Soheil Esmaeilzadeh et.al.", "abstract": "Teachers often conduct surveys in order to collect data from a predefined group of students to gain insights into topics of interest. When analyzing surveys with open-ended textual responses, it is extremely time-consuming, labor-intensive, and difficult to manually process all the responses into an insightful and comprehensive report. In the analysis step, traditionally, the teacher has to read each of the responses and decide on how to group them in order to extract insightful information. Even though it is possible to group the responses only using certain keywords, such an approach would be limited since it not only fails to account for embedded contexts but also cannot detect polysemous words or phrases and semantics that are not expressible in single words. In this work, we present a novel end-to-end context-aware framework that extracts, aggregates, and abbreviates embedded semantic patterns in open-response survey data. Our framework relies on a pre-trained natural language model in order to encode the textual data into semantic vectors. The encoded vectors then get clustered either into an optimally tuned number of groups or into a set of groups with pre-specified titles. In the former case, the clusters are then further analyzed to extract a representative set of keywords or summary sentences that serve as the labels of the clusters. In our framework, for the designated clusters, we finally provide context-aware wordclouds that demonstrate the semantically prominent keywords within each group. Honoring user privacy, we have successfully built the on-device implementation of our framework suitable for real-time analysis on mobile devices and have tested it on a synthetic dataset. Our framework reduces the costs at-scale by automating the process of extracting the most insightful information pieces from survey data.", "paper_url": "http://arxiv.org/abs/2203.01294v1", "pdf_url": "http://arxiv.org/pdf/2203.01294v1", "repo_url": null}, "2203.01282": {"publish_time": "2022-03-02", "title": "$\\texttt{py-irt}$: A Scalable Item Response Theory Library for Python", "author": "John P. Lalor et.al.", "abstract": "$\\texttt{py-irt}$ is a Python library for fitting Bayesian Item Response Theory (IRT) models. $\\texttt{py-irt}$ estimates latent traits of subjects and items, making it appropriate for use in IRT tasks as well as ideal-point models. $\\texttt{py-irt}$ is built on top of the Pyro and PyTorch frameworks and uses GPU-accelerated training to scale to large data sets. Code, documentation, and examples can be found at https://github.com/nd-ball/py-irt. $\\texttt{py-irt}$ can be installed from the GitHub page or the Python Package Index (PyPI).", "paper_url": "http://arxiv.org/abs/2203.01282v1", "pdf_url": "http://arxiv.org/pdf/2203.01282v1", "repo_url": "https://github.com/nd-ball/py-irt"}, "2203.01215": {"publish_time": "2022-03-02", "title": "Mukayese: Turkish NLP Strikes Back", "author": "Ali Safaya et.al.", "abstract": "Having sufficient resources for language X lifts it from the under-resourced languages class, but not necessarily from the under-researched class. In this paper, we address the problem of the absence of organized benchmarks in the Turkish language. We demonstrate that languages such as Turkish are left behind the state-of-the-art in NLP applications. As a solution, we present Mukayese, a set of NLP benchmarks for the Turkish language that contains several NLP tasks. We work on one or more datasets for each benchmark and present two or more baselines. Moreover, we present four new benchmarking datasets in Turkish for language modeling, sentence segmentation, and spell checking. All datasets and baselines are available under: https://github.com/alisafaya/mukayese", "paper_url": "http://arxiv.org/abs/2203.01215v1", "pdf_url": "http://arxiv.org/pdf/2203.01215v1", "repo_url": "https://github.com/alisafaya/mukayese"}, "2203.01111": {"publish_time": "2022-03-02", "title": "Large-Scale Hate Speech Detection with Cross-Domain Transfer", "author": "Cagri Toraman et.al.", "abstract": "The performance of hate speech detection models relies on the datasets on which the models are trained. Existing datasets are mostly prepared with a limited number of instances or hate domains that define hate topics. This hinders large-scale analysis and transfer learning with respect to hate domains. In this study, we construct large-scale tweet datasets for hate speech detection in English and a low-resource language, Turkish, consisting of human-labeled 100k tweets per each. Our datasets are designed to have equal number of tweets distributed over five domains. The experimental results supported by statistical tests show that Transformer-based language models outperform conventional bag-of-words and neural models by at least 5% in English and 10% in Turkish for large-scale hate speech detection. The performance is also scalable to different training sizes, such that 98% of performance in English, and 97% in Turkish, are recovered when 20% of training instances are used. We further examine the generalization ability of cross-domain transfer among hate domains. We show that 96% of the performance of a target domain in average is recovered by other domains for English, and 92% for Turkish. Gender and religion are more successful to generalize to other domains, while sports fail most.", "paper_url": "http://arxiv.org/abs/2203.01111v1", "pdf_url": "http://arxiv.org/pdf/2203.01111v1", "repo_url": "https://github.com/avaapm/hatespeech"}, "2203.01927": {"publish_time": "2022-03-03", "title": "As Little as Possible, as Much as Necessary: Detecting Over- and Undertranslations with Contrastive Conditioning", "author": "Jannis Vamvas et.al.", "abstract": "Omission and addition of content is a typical issue in neural machine translation. We propose a method for detecting such phenomena with off-the-shelf translation models. Using contrastive conditioning, we compare the likelihood of a full sequence under a translation model to the likelihood of its parts, given the corresponding source or target sequence. This allows to pinpoint superfluous words in the translation and untranslated words in the source even in the absence of a reference translation. The accuracy of our method is comparable to a supervised method that requires a custom quality estimation model.", "paper_url": "http://arxiv.org/abs/2203.01927v1", "pdf_url": "http://arxiv.org/pdf/2203.01927v1", "repo_url": "https://github.com/zurichnlp/coverage-contrastive-conditioning"}, "2203.01922": {"publish_time": "2022-03-03", "title": "Vision-Language Intelligence: Tasks, Representation Learning, and Large Models", "author": "Feng Li et.al.", "abstract": "This paper presents a comprehensive survey of vision-language (VL) intelligence from the perspective of time. This survey is inspired by the remarkable progress in both computer vision and natural language processing, and recent trends shifting from single modality processing to multiple modality comprehension. We summarize the development in this field into three time periods, namely task-specific methods, vision-language pre-training (VLP) methods, and larger models empowered by large-scale weakly-labeled data. We first take some common VL tasks as examples to introduce the development of task-specific methods. Then we focus on VLP methods and comprehensively review key components of the model structures and training methods. After that, we show how recent work utilizes large-scale raw image-text data to learn language-aligned visual representations that generalize better on zero or few shot learning tasks. Finally, we discuss some potential future trends towards modality cooperation, unified representation, and knowledge incorporation. We believe that this review will be of help for researchers and practitioners of AI and ML, especially those interested in computer vision and natural language processing.", "paper_url": "http://arxiv.org/abs/2203.01922v1", "pdf_url": "http://arxiv.org/pdf/2203.01922v1", "repo_url": null}, "2203.01849": {"publish_time": "2022-03-03", "title": "Context Enhanced Short Text Matching using Clickthrough Data", "author": "Mao Yan Chen et.al.", "abstract": "The short text matching task employs a model to determine whether two short texts have the same semantic meaning or intent. Existing short text matching models usually rely on the content of short texts which are lack information or missing some key clues. Therefore, the short texts need external knowledge to complete their semantic meaning. To address this issue, we propose a new short text matching framework for introducing external knowledge to enhance the short text contextual representation. In detail, we apply a self-attention mechanism to enrich short text representation with external contexts. Experiments on two Chinese datasets and one English dataset demonstrate that our framework outperforms the state-of-the-art short text matching models.", "paper_url": "http://arxiv.org/abs/2203.01849v1", "pdf_url": "http://arxiv.org/pdf/2203.01849v1", "repo_url": null}, "2203.01769": {"publish_time": "2022-03-03", "title": "PeerSum: A Peer Review Dataset for Abstractive Multi-document Summarization", "author": "Miao Li et.al.", "abstract": "We present PeerSum, a new MDS dataset using peer reviews of scientific publications. Our dataset differs from the existing MDS datasets in that our summaries (i.e., the meta-reviews) are highly abstractive and they are real summaries of the source documents (i.e., the reviews) and it also features disagreements among source documents. We found that current state-of-the-art MDS models struggle to generate high-quality summaries for PeerSum, offering new research opportunities.", "paper_url": "http://arxiv.org/abs/2203.01769v1", "pdf_url": "http://arxiv.org/pdf/2203.01769v1", "repo_url": "https://github.com/oaimli/peersum"}, "2203.01677": {"publish_time": "2022-03-03", "title": "Detection of Word Adversarial Examples in Text Classification: Benchmark and Baseline via Robust Density Estimation", "author": "KiYoon Yoo et.al.", "abstract": "Word-level adversarial attacks have shown success in NLP models, drastically decreasing the performance of transformer-based models in recent years. As a countermeasure, adversarial defense has been explored, but relatively few efforts have been made to detect adversarial examples. However, detecting adversarial examples may be crucial for automated tasks (e.g. review sentiment analysis) that wish to amass information about a certain population and additionally be a step towards a robust defense system. To this end, we release a dataset for four popular attack methods on four datasets and four models to encourage further research in this field. Along with it, we propose a competitive baseline based on density estimation that has the highest AUC on 29 out of 30 dataset-attack-model combinations. Source code is available in https://github.com/anoymous92874838/text-adv-detection.", "paper_url": "http://arxiv.org/abs/2203.01677v1", "pdf_url": "http://arxiv.org/pdf/2203.01677v1", "repo_url": "https://github.com/anoymous92874838/text-adv-detection"}, "2203.02459": {"publish_time": "2022-03-04", "title": "From Simultaneous to Streaming Machine Translation by Leveraging Streaming History", "author": "Javier Iranzo-S\u00e1nchez et.al.", "abstract": "Simultaneous Machine Translation is the task of incrementally translating an input sentence before it is fully available. Currently, simultaneous translation is carried out by translating each sentence independently of the previously translated text. More generally, Streaming MT can be understood as an extension of Simultaneous MT to the incremental translation of a continuous input text stream. In this work, a state-of-the-art simultaneous sentence-level MT system is extended to the streaming setup by leveraging the streaming history. Extensive empirical results are reported on IWSLT Translation Tasks, showing that leveraging the streaming history leads to significant quality gains. In particular, the proposed system proves to compare favorably to the best performing systems.", "paper_url": "http://arxiv.org/abs/2203.02459v1", "pdf_url": "http://arxiv.org/pdf/2203.02459v1", "repo_url": null}, "2203.02458": {"publish_time": "2022-03-04", "title": "Comprehension of Subtitles from Re-Translating Simultaneous Speech Translation", "author": "D\u00e1vid Javorsk\u00fd et.al.", "abstract": "In simultaneous speech translation, one can vary the size of the output window, system latency and sometimes the allowed level of rewriting. The effect of these properties on readability and comprehensibility has not been tested with modern neural translation systems. In this work, we propose an evaluation method and investigate the effects on comprehension and user preferences. It is a pilot study with 14 users on 2 hours of German documentaries or speeches with online translations into Czech. We collect continuous feedback and answers on factual questions. Our results show that the subtitling layout or flicker have a little effect on comprehension, in contrast to machine translation itself and individual competence. Other results show that users with a limited knowledge of the source language have different preferences to stability and latency than the users with zero knowledge. The results are statistically insignificant, however, we show that our method works and can be reproduced in larger volume.", "paper_url": "http://arxiv.org/abs/2203.02458v1", "pdf_url": "http://arxiv.org/pdf/2203.02458v1", "repo_url": null}, "2203.02392": {"publish_time": "2022-03-04", "title": "Beyond Plain Toxic: Detection of Inappropriate Statements on Flammable Topics for the Russian Language", "author": "Nikolay Babakov et.al.", "abstract": "Toxicity on the Internet, such as hate speech, offenses towards particular users or groups of people, or the use of obscene words, is an acknowledged problem. However, there also exist other types of inappropriate messages which are usually not viewed as toxic, e.g. as they do not contain explicit offences. Such messages can contain covered toxicity or generalizations, incite harmful actions (crime, suicide, drug use), provoke \"heated\" discussions. Such messages are often related to particular sensitive topics, e.g. on politics, sexual minorities, social injustice which more often than other topics, e.g. cars or computing, yield toxic emotional reactions. At the same time, clearly not all messages within such flammable topics are inappropriate.   Towards this end, in this work, we present two text collections labelled according to binary notion of inapropriateness and a multinomial notion of sensitive topic. Assuming that the notion of inappropriateness is common among people of the same culture, we base our approach on human intuitive understanding of what is not acceptable and harmful. To objectivise the notion of inappropriateness, we define it in a data-driven way though crowdsourcing. Namely we run a large-scale annotation study asking workers if a given chatbot textual statement could harm reputation of a company created it. Acceptably high values of inter-annotator agreement suggest that the notion of inappropriateness exists and can be uniformly understood by different people. To define the notion of sensitive topics in an objective way we use on guidelines suggested commonly by specialists of legal and PR department of a large public company as potentially harmful.", "paper_url": "http://arxiv.org/abs/2203.02392v1", "pdf_url": "http://arxiv.org/pdf/2203.02392v1", "repo_url": null}, "2203.02385": {"publish_time": "2022-03-04", "title": "MM-DFN: Multimodal Dynamic Fusion Network for Emotion Recognition in Conversations", "author": "Dou Hu et.al.", "abstract": "Emotion Recognition in Conversations (ERC) has considerable prospects for developing empathetic machines. For multimodal ERC, it is vital to understand context and fuse modality information in conversations. Recent graph-based fusion methods generally aggregate multimodal information by exploring unimodal and cross-modal interactions in a graph. However, they accumulate redundant information at each layer, limiting the context understanding between modalities. In this paper, we propose a novel Multimodal Dynamic Fusion Network (MM-DFN) to recognize emotions by fully understanding multimodal conversational context. Specifically, we design a new graph-based dynamic fusion module to fuse multimodal contextual features in a conversation. The module reduces redundancy and enhances complementarity between modalities by capturing the dynamics of contextual information in different semantic spaces. Extensive experiments on two public benchmark datasets demonstrate the effectiveness and superiority of MM-DFN.", "paper_url": "http://arxiv.org/abs/2203.02385v1", "pdf_url": "http://arxiv.org/pdf/2203.02385v1", "repo_url": "https://github.com/zerohd4869/mm-dfn"}, "2203.02244": {"publish_time": "2022-03-04", "title": "IISERB Brains at SemEval 2022 Task 6: A Deep-learning Framework to Identify Intended Sarcasm in English", "author": "Tanuj Singh Shekhawat et.al.", "abstract": "This paper describes the system architectures and the models submitted by our team \"IISERBBrains\" to SemEval 2022 Task 6 competition. We contested for all three sub-tasks floated for the English dataset. On the leader-board, wegot19th rank out of43 teams for sub-taskA, the 8th rank out of22 teams for sub-task B,and13th rank out of 16 teams for sub-taskC. Apart from the submitted results and models, we also report the other models and results that we obtained through our experiments after organizers published the gold labels of their evaluation data", "paper_url": "http://arxiv.org/abs/2203.02244v1", "pdf_url": "http://arxiv.org/pdf/2203.02244v1", "repo_url": "https://github.com/manojmahan/isarcasmeval-intended-sarcasm-detection-in-english-main"}, "2203.03601": {"publish_time": "2022-03-07", "title": "Creating Speech-to-Speech Corpus from Dubbed Series", "author": "Massa Baali et.al.", "abstract": "Dubbed series are gaining a lot of popularity in recent years with strong support from major media service providers. Such popularity is fueled by studies that showed that dubbed versions of TV shows are more popular than their subtitled equivalents. We propose an unsupervised approach to construct speech-to-speech corpus, aligned on short segment levels, to produce a parallel speech corpus in the source- and target- languages. Our methodology exploits video frames, speech recognition, machine translation, and noisy frames removal algorithms to match segments in both languages. To verify the performance of the proposed method, we apply it on long and short dubbed clips. Out of 36 hours TR-AR dubbed series, our pipeline was able to generate 17 hours of paired segments, which is about 47% of the corpus. We applied our method on another language pair, EN-AR, to ensure it is robust enough and not tuned for a specific language or a specific corpus. Regardless of the language pairs, the accuracy of the paired segments was around 70% when evaluated using human subjective evaluation. The corpus will be freely available for the research community.", "paper_url": "http://arxiv.org/abs/2203.03601v1", "pdf_url": "http://arxiv.org/pdf/2203.03601v1", "repo_url": "https://github.com/massabaali7/speech_parallel_corpus"}, "2203.03598": {"publish_time": "2022-03-07", "title": "Audio-visual Generalised Zero-shot Learning with Cross-modal Attention and Language", "author": "Otniel-Bogdan Mercea et.al.", "abstract": "Learning to classify video data from classes not included in the training data, i.e. video-based zero-shot learning, is challenging. We conjecture that the natural alignment between the audio and visual modalities in video data provides a rich training signal for learning discriminative multi-modal representations. Focusing on the relatively underexplored task of audio-visual zero-shot learning, we propose to learn multi-modal representations from audio-visual data using cross-modal attention and exploit textual label embeddings for transferring knowledge from seen classes to unseen classes. Taking this one step further, in our generalised audio-visual zero-shot learning setting, we include all the training classes in the test-time search space which act as distractors and increase the difficulty while making the setting more realistic. Due to the lack of a unified benchmark in this domain, we introduce a (generalised) zero-shot learning benchmark on three audio-visual datasets of varying sizes and difficulty, VGGSound, UCF, and ActivityNet, ensuring that the unseen test classes do not appear in the dataset used for supervised training of the backbone deep models. Comparing multiple relevant and recent methods, we demonstrate that our proposed AVCA model achieves state-of-the-art performance on all three datasets. Code and data will be available at \\url{https://github.com/ExplainableML/AVCA-GZSL}.", "paper_url": "http://arxiv.org/abs/2203.03598v1", "pdf_url": "http://arxiv.org/pdf/2203.03598v1", "repo_url": "https://github.com/explainableml/avca-gzsl"}, "2203.03463": {"publish_time": "2022-03-07", "title": "Hierarchical Sketch Induction for Paraphrase Generation", "author": "Tom Hosking et.al.", "abstract": "We propose a generative model of paraphrase generation, that encourages syntactic diversity by conditioning on an explicit syntactic sketch. We introduce Hierarchical Refinement Quantized Variational Autoencoders (HRQ-VAE), a method for learning decompositions of dense encodings as a sequence of discrete latent variables that make iterative refinements of increasing granularity. This hierarchy of codes is learned through end-to-end training, and represents fine-to-coarse grained information about the input. We use HRQ-VAE to encode the syntactic form of an input sentence as a path through the hierarchy, allowing us to more easily predict syntactic sketches at test time. Extensive experiments, including a human evaluation, confirm that HRQ-VAE learns a hierarchical representation of the input space, and generates paraphrases of higher quality than previous systems.", "paper_url": "http://arxiv.org/abs/2203.03463v1", "pdf_url": "http://arxiv.org/pdf/2203.03463v1", "repo_url": "https://github.com/tomhosking/hrq-vae"}, "2203.03442": {"publish_time": "2022-03-07", "title": "Towards Automated Real-time Evaluation in Text-based Counseling", "author": "Anqi Li et.al.", "abstract": "Automated real-time evaluation of counselor-client interaction is important for ensuring quality counseling but the rules are difficult to articulate. Recent advancements in machine learning methods show the possibility of learning such rules automatically. However, these methods often demand large scale and high quality counseling data, which are difficult to collect. To address this issue, we build an online counseling platform, which allows professional psychotherapists to provide free counseling services to those are in need. In exchange, we collect the counseling transcripts. Within a year of its operation, we manage to get one of the largest set of (675) transcripts of counseling sessions. To further leverage the valuable data we have, we label our dataset using both coarse- and fine-grained labels and use a set of pretraining techniques. In the end, we are able to achieve practically useful accuracy in both labeling system.", "paper_url": "http://arxiv.org/abs/2203.03442v1", "pdf_url": "http://arxiv.org/pdf/2203.03442v1", "repo_url": null}, "2203.03441": {"publish_time": "2022-03-07", "title": "Multi-Modal Attribute Extraction for E-Commerce", "author": "Alo\u00efs De la Comble et.al.", "abstract": "To improve users' experience as they navigate the myriad of options offered by online marketplaces, it is essential to have well-organized product catalogs. One key ingredient to that is the availability of product attributes such as color or material. However, on some marketplaces such as Rakuten-Ichiba, which we focus on, attribute information is often incomplete or even missing. One promising solution to this problem is to rely on deep models pre-trained on large corpora to predict attributes from unstructured data, such as product descriptive texts and images (referred to as modalities in this paper). However, we find that achieving satisfactory performance with this approach is not straightforward but rather the result of several refinements, which we discuss in this paper. We provide a detailed description of our approach to attribute extraction, from investigating strong single-modality methods, to building a solid multimodal model combining textual and visual information. One key component of our multimodal architecture is a novel approach to seamlessly combine modalities, which is inspired by our single-modality investigations. In practice, we notice that this new modality-merging method may suffer from a modality collapse issue, i.e., it neglects one modality. Hence, we further propose a mitigation to this problem based on a principled regularization scheme. Experiments on Rakuten-Ichiba data provide empirical evidence for the benefits of our approach, which has been also successfully deployed to Rakuten-Ichiba. We also report results on publicly available datasets showing that our model is competitive compared to several recent multimodal and unimodal baselines.", "paper_url": "http://arxiv.org/abs/2203.03441v1", "pdf_url": "http://arxiv.org/pdf/2203.03441v1", "repo_url": null}, "2203.04218": {"publish_time": "2022-03-08", "title": "Learning Bidirectional Translation between Descriptions and Actions with Small Paired Data", "author": "Minori Toyoda et.al.", "abstract": "This study achieved bidirectional translation between descriptions and actions using small paired data. The ability to mutually generate descriptions and actions is essential for robots to collaborate with humans in their daily lives. The robot is required to associate real-world objects with linguistic expressions, and large-scale paired data are required for machine learning approaches. However, a paired dataset is expensive to construct and difficult to collect. This study proposes a two-stage training method for bidirectional translation. In the proposed method, we train recurrent autoencoders (RAEs) for descriptions and actions with a large amount of non-paired data. Then, we fine-tune the entire model to bind their intermediate representations using small paired data. Because the data used for pre-training do not require pairing, behavior-only data or a large language corpus can be used. We experimentally evaluated our method using a paired dataset consisting of motion-captured actions and descriptions. The results showed that our method performed well, even when the amount of paired data to train was small. The visualization of the intermediate representations of each RAE showed that similar actions were encoded in a clustered position and the corresponding feature vectors well aligned.", "paper_url": "http://arxiv.org/abs/2203.04218v1", "pdf_url": "http://arxiv.org/pdf/2203.04218v1", "repo_url": null}, "2203.04212": {"publish_time": "2022-03-08", "title": "Measuring the Mixing of Contextual Information in the Transformer", "author": "Javier Ferrando et.al.", "abstract": "The Transformer architecture aggregates input information through the self-attention mechanism, but there is no clear understanding of how this information is mixed across the entire model. Additionally, recent works have demonstrated that attention weights alone are not enough to describe the flow of information. In this paper, we consider the whole attention block --multi-head attention, residual connection, and layer normalization-- and define a metric to measure token-to-token interactions within each layer, considering the characteristics of the representation space. Then, we aggregate layer-wise interpretations to provide input attribution scores for model predictions. Experimentally, we show that our method, ALTI (Aggregation of Layer-wise Token-to-token Interactions), provides faithful explanations and outperforms similar aggregation methods.", "paper_url": "http://arxiv.org/abs/2203.04212v1", "pdf_url": "http://arxiv.org/pdf/2203.04212v1", "repo_url": "https://github.com/mt-upc/transformer-contributions"}, "2203.04111": {"publish_time": "2022-03-08", "title": "Plumeria at SemEval-2022 Task 6: Robust Approaches for Sarcasm Detection for English and Arabic Using Transformers and Data Augmentation", "author": "Shubham Kumar Nigam et.al.", "abstract": "This paper describes our submission to SemEval-2022 Task 6 on sarcasm detection and its five subtasks for English and Arabic. Sarcasm conveys a meaning which contradicts the literal meaning, and it is mainly found on social networks. It has a significant role in understanding the intention of the user. For detecting sarcasm, we used deep learning techniques based on transformers due to its success in the field of Natural Language Processing (NLP) without the need for feature engineering. The datasets were taken from tweets. We created new datasets by augmenting with external data or by using word embeddings and repetition of instances. Experiments were done on the datasets with different types of preprocessing because it is crucial in this task. The rank of our team was consistent across four subtasks (fourth rank in three subtasks and sixth rank in one subtask); whereas other teams might be in the top ranks for some subtasks but rank drastically less in other subtasks. This implies the robustness and stability of the models and the techniques we used.", "paper_url": "http://arxiv.org/abs/2203.04111v1", "pdf_url": "http://arxiv.org/pdf/2203.04111v1", "repo_url": null}, "2203.04076": {"publish_time": "2022-03-08", "title": "Semantic Distillation Guided Salient Object Detection", "author": "Bo Xu et.al.", "abstract": "Most existing CNN-based salient object detection methods can identify local segmentation details like hair and animal fur, but often misinterpret the real saliency due to the lack of global contextual information caused by the subjectiveness of the SOD task and the locality of convolution layers. Moreover, due to the unrealistically expensive labeling costs, the current existing SOD datasets are insufficient to cover the real data distribution. The limitation and bias of the training data add additional difficulty to fully exploring the semantic association between object-to-object and object-to-environment in a given image. In this paper, we propose a semantic distillation guided SOD (SDG-SOD) method that produces accurate results by fusing semantically distilled knowledge from generated image captioning into the Vision-Transformer-based SOD framework. SDG-SOD can better uncover inter-objects and object-to-environment saliency and cover the gap between the subjective nature of SOD and its expensive labeling. Comprehensive experiments on five benchmark datasets demonstrate that the SDG-SOD outperforms the state-of-the-art approaches on four evaluation metrics, and largely improves the model performance on DUTS, ECSSD, DUT, HKU-IS, and PASCAL-S datasets.", "paper_url": "http://arxiv.org/abs/2203.04076v1", "pdf_url": "http://arxiv.org/pdf/2203.04076v1", "repo_url": null}, "2203.04045": {"publish_time": "2022-03-08", "title": "Towards Generalized Models for Task-oriented Dialogue Modeling on Spoken Conversations", "author": "Ruijie Yan et.al.", "abstract": "Building robust and general dialogue models for spoken conversations is challenging due to the gap in distributions of spoken and written data. This paper presents our approach to build generalized models for the Knowledge-grounded Task-oriented Dialogue Modeling on Spoken Conversations Challenge of DSTC-10. In order to mitigate the discrepancies between spoken and written text, we mainly employ extensive data augmentation strategies on written data, including artificial error injection and round-trip text-speech transformation. To train robust models for spoken conversations, we improve pre-trained language models, and apply ensemble algorithms for each sub-task. Typically, for the detection task, we fine-tune \\roberta and ELECTRA, and run an error-fixing ensemble algorithm. For the selection task, we adopt a two-stage framework that consists of entity tracking and knowledge ranking, and propose a multi-task learning method to learn multi-level semantic information by domain classification and entity selection. For the generation task, we adopt a cross-validation data process to improve pre-trained generative language models, followed by a consensus decoding algorithm, which can add arbitrary features like relative \\rouge metric, and tune associated feature weights toward \\bleu directly. Our approach ranks third on the objective evaluation and second on the final official human evaluation.", "paper_url": "http://arxiv.org/abs/2203.04045v1", "pdf_url": "http://arxiv.org/pdf/2203.04045v1", "repo_url": null}, "2203.04911": {"publish_time": "2022-03-09", "title": "DUAL: Textless Spoken Question Answering with Speech Discrete Unit Adaptive Learning", "author": "Guan-Ting Lin et.al.", "abstract": "Spoken Question Answering (SQA) has gained research attention and made remarkable progress in recent years. However, existing SQA methods rely on Automatic Speech Recognition (ASR) transcripts, which are time and cost-prohibitive to collect. This work proposes an ASR transcript-free SQA framework named Discrete Unit Adaptive Learning (DUAL), which leverages unlabeled data for pre-training and is fine-tuned by the SQA downstream task. DAUL can directly predict the time interval of the spoken answer from the spoken document. We also release a new SQA benchmark corpus Natural Multi-speaker Spoken Question Answering (NMSQA) for testing SQA in realistic scenarios. The experimental results show that DUAL performs competitively with the cascade approach (ASR + text QA), and DUAL is robust to real-world speech. We will open-source our code and model to inspire more SQA innovations from the community", "paper_url": "http://arxiv.org/abs/2203.04911v1", "pdf_url": "http://arxiv.org/pdf/2203.04911v1", "repo_url": null}, "2203.04907": {"publish_time": "2022-03-09", "title": "Pose Guided Multi-person Image Generation From Text", "author": "Soon Yau Cheong et.al.", "abstract": "Transformers have recently been shown to generate high quality images from texts. However, existing methods struggle to create high fidelity full-body images, especially multiple people. A person's pose has a high degree of freedom that is difficult to describe using words only; this creates errors in the generated image, such as incorrect body proportions and pose. We propose a pose-guided text-to-image model, using pose as an additional input constraint. Using the proposed Keypoint Pose Encoding (KPE) to encode human pose into low dimensional representation, our model can generate novel multi-person images accurately representing the pose and text descriptions provided, with minimal errors. We demonstrate that KPE is invariant to changes in the target image domain and image resolution; we show results on the Deepfashion dataset and create a new multi-person Deepfashion dataset to demonstrate the multi-capabilities of our approach.", "paper_url": "http://arxiv.org/abs/2203.04907v1", "pdf_url": "http://arxiv.org/pdf/2203.04907v1", "repo_url": null}, "2203.04904": {"publish_time": "2022-03-09", "title": "Model-Agnostic Multitask Fine-tuning for Few-shot Vision-Language Transfer Learning", "author": "Zhenhailong Wang et.al.", "abstract": "Despite achieving state-of-the-art zero-shot performance, existing vision-language models, e.g., CLIP, still fall short of domain-specific classification tasks, e.g., Fungi Classification. In the context of few-shot transfer learning, traditional fine-tuning fails to prevent highly expressive model from exploiting spurious correlations in the training data. On the other hand, although model-agnostic meta-learning (MAML) presents as a natural alternative for transfer learning, the expensive computation due to implicit second-order optimization limits its use in large-scale models and datasets. In this work we aim to further improve the generalization of existing vision-language models on unseen tasks via a simple yet efficient fine-tuning strategy based on uniform task sampling. We term our method as Model-Agnostic Multitask Fine-tuning (MAMF). Compared with MAML, MAMF discards the bi-level optimization and uses only first-order gradients, which makes it easily scalable and computationally efficient. Due to the uniform task sampling procedure, MAMF consistently outperforms the classical fine-tuning method for few-shot transfer learning on five benchmark datasets. Empirically, we further discover that the effectiveness of first-order MAML is highly dependent on the zero-shot performance of the pretrained model, and our simple algorithm can outperform first-order MAML on more challenging datasets with low zero-shot performance.", "paper_url": "http://arxiv.org/abs/2203.04904v1", "pdf_url": "http://arxiv.org/pdf/2203.04904v1", "repo_url": null}, "2203.04863": {"publish_time": "2022-03-09", "title": "Unsupervised Alignment of Distributional Word Embeddings", "author": "Aissatou Diallo et.al.", "abstract": "Cross-domain alignment play a key roles in tasks ranging from machine translation to transfer learning. Recently, purely unsupervised methods operating on monolingual embeddings have successfully been used to infer a bilingual lexicon without relying on supervision. However, current state-of-the art methods only focus on point vectors although distributional embeddings have proven to embed richer semantic information when representing words. In this paper, we propose stochastic optimization approach for aligning probabilistic embeddings. Finally, we evaluate our method on the problem of unsupervised word translation, by aligning word embeddings trained on monolingual data. We show that the proposed approach achieves good performance on the bilingual lexicon induction task across several language pairs and performs better than the point-vector based approach.", "paper_url": "http://arxiv.org/abs/2203.04863v1", "pdf_url": "http://arxiv.org/pdf/2203.04863v1", "repo_url": null}, "2203.04860": {"publish_time": "2022-03-09", "title": "PET: A new Dataset for Process Extraction from Natural Language Text", "author": "Patrizio Bellan et.al.", "abstract": "Although there is a long tradition of work in NLP on extracting entities and relations from text, to date there exists little work on the acquisition of business processes from unstructured data such as textual corpora of process descriptions. With this work we aim at filling this gap and establishing the first steps towards bridging data-driven information extraction methodologies from Natural Language Processing and the model-based formalization that is aimed from Business Process Management. For this, we develop the first corpus of business process descriptions annotated with activities, gateways, actors and flow information. We present our new resource, including a detailed overview of the annotation schema and guidelines, as well as a variety of baselines to benchmark the difficulty and challenges of business process extraction from text.", "paper_url": "http://arxiv.org/abs/2203.04860v1", "pdf_url": "http://arxiv.org/pdf/2203.04860v1", "repo_url": null}, "2203.05557": {"publish_time": "2022-03-10", "title": "Conditional Prompt Learning for Vision-Language Models", "author": "Kaiyang Zhou et.al.", "abstract": "With the rise of powerful pre-trained vision-language models like CLIP, it becomes essential to investigate ways to adapt these models to downstream datasets. A recently proposed method named Context Optimization (CoOp) introduces the concept of prompt learning -- a recent trend in NLP -- to the vision domain for adapting pre-trained vision-language models. Specifically, CoOp turns context words in a prompt into a set of learnable vectors and, with only a few labeled images for learning, can achieve huge improvements over intensively-tuned manual prompts. In our study we identify a critical problem of CoOp: the learned context is not generalizable to wider unseen classes within the same dataset, suggesting that CoOp overfits base classes observed during training. To address the problem, we propose Conditional Context Optimization (CoCoOp), which extends CoOp by further learning a lightweight neural network to generate for each image an input-conditional token (vector). Compared to CoOp's static prompts, our dynamic prompts adapt to each instance and are thus less sensitive to class shift. Extensive experiments show that CoCoOp generalizes much better than CoOp to unseen classes, even showing promising transferability beyond a single dataset; and yields stronger domain generalization performance as well. Code is available at https://github.com/KaiyangZhou/CoOp.", "paper_url": "http://arxiv.org/abs/2203.05557v1", "pdf_url": "http://arxiv.org/pdf/2203.05557v1", "repo_url": "https://github.com/kaiyangzhou/coop"}, "2203.05482": {"publish_time": "2022-03-10", "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time", "author": "Mitchell Wortsman et.al.", "abstract": "The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results \"model soups.\" When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. As a highlight, the resulting ViT-G model attains 90.94% top-1 accuracy on ImageNet, a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically.", "paper_url": "http://arxiv.org/abs/2203.05482v1", "pdf_url": "http://arxiv.org/pdf/2203.05482v1", "repo_url": null}, "2203.05465": {"publish_time": "2022-03-10", "title": "LoopITR: Combining Dual and Cross Encoder Architectures for Image-Text Retrieval", "author": "Jie Lei et.al.", "abstract": "Dual encoders and cross encoders have been widely used for image-text retrieval. Between the two, the dual encoder encodes the image and text independently followed by a dot product, while the cross encoder jointly feeds image and text as the input and performs dense multi-modal fusion. These two architectures are typically modeled separately without interaction. In this work, we propose LoopITR, which combines them in the same network for joint learning. Specifically, we let the dual encoder provide hard negatives to the cross encoder, and use the more discriminative cross encoder to distill its predictions back to the dual encoder. Both steps are efficiently performed together in the same model. Our work centers on empirical analyses of this combined architecture, putting the main focus on the design of the distillation objective. Our experimental results highlight the benefits of training the two encoders in the same network, and demonstrate that distillation can be quite effective with just a few hard negative examples. Experiments on two standard datasets (Flickr30K and COCO) show our approach achieves state-of-the-art dual encoder performance when compared with approaches using a similar amount of data.", "paper_url": "http://arxiv.org/abs/2203.05465v1", "pdf_url": "http://arxiv.org/pdf/2203.05465v1", "repo_url": null}, "2203.05437": {"publish_time": "2022-03-10", "title": "IndicNLG Suite: Multilingual Datasets for Diverse NLG Tasks in Indic Languages", "author": "Aman Kumar et.al.", "abstract": "In this paper, we present the IndicNLG suite, a collection of datasets for benchmarking Natural Language Generation (NLG) for 11 Indic languages. We focus on five diverse tasks, namely, biography generation using Wikipedia infoboxes (WikiBio), news headline generation, sentence summarization, question generation and paraphrase generation. We describe the process of creating the datasets and present statistics of the dataset, following which we train and report a variety of strong monolingual and multilingual baselines that leverage pre-trained sequence-to-sequence models and analyze the results to understand the challenges involved in Indic language NLG. To the best of our knowledge, this is the first NLG dataset for Indic languages and also the largest multilingual NLG dataset. Our methods can also be easily applied to modest-resource languages with reasonable monolingual and parallel corpora, as well as corpora containing structured data like Wikipedia. We hope this dataset spurs research in NLG on diverse languages and tasks, particularly for Indic languages. The datasets and models are publicly available at https://indicnlp.ai4bharat.org/indicnlg-suite.", "paper_url": "http://arxiv.org/abs/2203.05437v1", "pdf_url": "http://arxiv.org/pdf/2203.05437v1", "repo_url": null}, "2203.05425": {"publish_time": "2022-03-10", "title": "Semantic Norm Recognition and its application to Portuguese Law", "author": "Maria Duarte et.al.", "abstract": "Being able to clearly interpret legal texts and fully understanding our rights, obligations and other legal norms has become progressively more important in the digital society. However, simply giving citizens access to the laws is not enough, as there is a need to provide meaningful information that cater to their specific queries and needs. For this, it is necessary to extract the relevant semantic information present in legal texts. Thus, we introduce the SNR (Semantic Norm Recognition) system, an automatic semantic information extraction system trained on a domain-specific (legal) text corpus taken from Portuguese Consumer Law. The SNR system uses the Portuguese Bert (BERTimbau) and was trained on a legislative Portuguese corpus. We demonstrate how our system achieved good results (81.44\\% F1-score) on this domain-specific corpus, despite existing noise, and how it can be used to improve downstream tasks such as information retrieval.", "paper_url": "http://arxiv.org/abs/2203.05425v1", "pdf_url": "http://arxiv.org/pdf/2203.05425v1", "repo_url": null}, "2203.06169": {"publish_time": "2022-03-11", "title": "LaPraDoR: Unsupervised Pretrained Dense Retriever for Zero-Shot Text Retrieval", "author": "Canwen Xu et.al.", "abstract": "In this paper, we propose LaPraDoR, a pretrained dual-tower dense retriever that does not require any supervised data for training. Specifically, we first present Iterative Contrastive Learning (ICoL) that iteratively trains the query and document encoders with a cache mechanism. ICoL not only enlarges the number of negative instances but also keeps representations of cached examples in the same hidden space. We then propose Lexicon-Enhanced Dense Retrieval (LEDR) as a simple yet effective way to enhance dense retrieval with lexical matching. We evaluate LaPraDoR on the recently proposed BEIR benchmark, including 18 datasets of 9 zero-shot text retrieval tasks. Experimental results show that LaPraDoR achieves state-of-the-art performance compared with supervised dense retrieval models, and further analysis reveals the effectiveness of our training strategy and objectives. Compared to re-ranking, our lexicon-enhanced approach can be run in milliseconds (22.5x faster) while achieving superior performance.", "paper_url": "http://arxiv.org/abs/2203.06169v1", "pdf_url": "http://arxiv.org/pdf/2203.06169v1", "repo_url": "https://github.com/jetrunner/laprador"}, "2203.06096": {"publish_time": "2022-03-11", "title": "WLASL-LEX: a Dataset for Recognising Phonological Properties in American Sign Language", "author": "Federico Tavella et.al.", "abstract": "Signed Language Processing (SLP) concerns the automated processing of signed languages, the main means of communication of Deaf and hearing impaired individuals. SLP features many different tasks, ranging from sign recognition to translation and production of signed speech, but has been overlooked by the NLP community thus far. In this paper, we bring to attention the task of modelling the phonology of sign languages. We leverage existing resources to construct a large-scale dataset of American Sign Language signs annotated with six different phonological properties. We then conduct an extensive empirical study to investigate whether data-driven end-to-end and feature-based approaches can be optimised to automatically recognise these properties. We find that, despite the inherent challenges of the task, graph-based neural networks that operate over skeleton features extracted from raw videos are able to succeed at the task to a varying degree. Most importantly, we show that this performance pertains even on signs unobserved during training.", "paper_url": "http://arxiv.org/abs/2203.06096v1", "pdf_url": "http://arxiv.org/pdf/2203.06096v1", "repo_url": null}, "2203.06063": {"publish_time": "2022-03-11", "title": "Active Evaluation: Efficient NLG Evaluation with Few Pairwise Comparisons", "author": "Akash Kumar Mohankumar et.al.", "abstract": "Recent studies have shown the advantages of evaluating NLG systems using pairwise comparisons as opposed to direct assessment. Given $k$ systems, a naive approach for identifying the top-ranked system would be to uniformly obtain pairwise comparisons from all ${k \\choose 2}$ pairs of systems. However, this can be very expensive as the number of human annotations required would grow quadratically with $k$. In this work, we introduce Active Evaluation, a framework to efficiently identify the top-ranked system by actively choosing system pairs for comparison using dueling bandit algorithms. We perform extensive experiments with 13 dueling bandits algorithms on 13 NLG evaluation datasets spanning 5 tasks and show that the number of human annotations can be reduced by 80%. To further reduce the number of human annotations, we propose model-based dueling bandit algorithms which combine automatic evaluation metrics with human evaluations. Specifically, we eliminate sub-optimal systems even before the human annotation process and perform human evaluations only on test examples where the automatic metric is highly uncertain. This reduces the number of human annotations required further by 89%. In effect, we show that identifying the top-ranked system requires only a few hundred human annotations, which grow linearly with $k$. Lastly, we provide practical recommendations and best practices to identify the top-ranked system efficiently. Our code has been made publicly available at https://github.com/akashkm99/duelnlg", "paper_url": "http://arxiv.org/abs/2203.06063v1", "pdf_url": "http://arxiv.org/pdf/2203.06063v1", "repo_url": "https://github.com/akashkm99/duelnlg"}, "2203.05948": {"publish_time": "2022-03-11", "title": "Block-Sparse Adversarial Attack to Fool Transformer-Based Text Classifiers", "author": "Sahar Sadrizadeh et.al.", "abstract": "Recently, it has been shown that, in spite of the significant performance of deep neural networks in different fields, those are vulnerable to adversarial examples. In this paper, we propose a gradient-based adversarial attack against transformer-based text classifiers. The adversarial perturbation in our method is imposed to be block-sparse so that the resultant adversarial example differs from the original sentence in only a few words. Due to the discrete nature of textual data, we perform gradient projection to find the minimizer of our proposed optimization problem. Experimental results demonstrate that, while our adversarial attack maintains the semantics of the sentence, it can reduce the accuracy of GPT-2 to less than 5% on different datasets (AG News, MNLI, and Yelp Reviews). Furthermore, the block-sparsity constraint of the proposed optimization problem results in small perturbations in the adversarial example.", "paper_url": "http://arxiv.org/abs/2203.05948v1", "pdf_url": "http://arxiv.org/pdf/2203.05948v1", "repo_url": null}, "2203.05936": {"publish_time": "2022-03-11", "title": "Are discrete units necessary for Spoken Language Modeling?", "author": "Tu Anh Nguyen et.al.", "abstract": "Recent work in spoken language modeling shows the possibility of learning a language unsupervisedly from raw audio without any text labels. The approach relies first on transforming the audio into a sequence of discrete units (or pseudo-text) and then training a language model directly on such pseudo-text. Is such a discrete bottleneck necessary, potentially introducing irreversible errors in the encoding of the speech signal, or could we learn a language model without discrete units at all? In this work, show that discretization is indeed essential for good results in spoken language modeling, but that can omit the discrete bottleneck if we use using discrete target features from a higher level than the input features. We also show that an end-to-end model trained with discrete target like HuBERT achieves similar results as the best language model trained on pseudo-text on a set of zero-shot spoken language modeling metrics from the Zero Resource Speech Challenge 2021.", "paper_url": "http://arxiv.org/abs/2203.05936v1", "pdf_url": "http://arxiv.org/pdf/2203.05936v1", "repo_url": null}, "2203.07362": {"publish_time": "2022-03-14", "title": "CoNTACT: A Dutch COVID-19 Adapted BERT for Vaccine Hesitancy and Argumentation Detection", "author": "Jens Lemmens et.al.", "abstract": "We present CoNTACT: a Dutch language model adapted to the domain of COVID-19 tweets. The model was developed by continuing the pre-training phase of RobBERT (Delobelle, 2020) by using 2.8M Dutch COVID-19 related tweets posted in 2021. In order to test the performance of the model and compare it to RobBERT, the two models were tested on two tasks: (1) binary vaccine hesitancy detection and (2) detection of arguments for vaccine hesitancy. For both tasks, not only Twitter but also Facebook data was used to show cross-genre performance. In our experiments, CoNTACT showed statistically significant gains over RobBERT in all experiments for task 1. For task 2, we observed substantial improvements in virtually all classes in all experiments. An error analysis indicated that the domain adaptation yielded better representations of domain-specific terminology, causing CoNTACT to make more accurate classification decisions.", "paper_url": "http://arxiv.org/abs/2203.07362v1", "pdf_url": "http://arxiv.org/pdf/2203.07362v1", "repo_url": null}, "2203.07285": {"publish_time": "2022-03-14", "title": "Diversifying Content Generation for Commonsense Reasoning with Mixture of Knowledge Graph Experts", "author": "Wenhao Yu et.al.", "abstract": "Generative commonsense reasoning (GCR) in natural language is to reason about the commonsense while generating coherent text. Recent years have seen a surge of interest in improving the generation quality of commonsense reasoning tasks. Nevertheless, these approaches have seldom investigated diversity in the GCR tasks, which aims to generate alternative explanations for a real-world situation or predict all possible outcomes. Diversifying GCR is challenging as it expects to generate multiple outputs that are not only semantically different but also grounded in commonsense knowledge. In this paper, we propose MoKGE, a novel method that diversifies the generative reasoning by a mixture of expert (MoE) strategy on commonsense knowledge graphs (KG). A set of knowledge experts seek diverse reasoning on KG to encourage various generation outputs. Empirical experiments demonstrated that MoKGE can significantly improve the diversity while achieving on par performance on accuracy on two GCR benchmarks, based on both automatic and human evaluations.", "paper_url": "http://arxiv.org/abs/2203.07285v1", "pdf_url": "http://arxiv.org/pdf/2203.07285v1", "repo_url": "https://github.com/DM2-ND/MoKGE"}, "2203.07281": {"publish_time": "2022-03-14", "title": "GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models", "author": "Archiki Prasad et.al.", "abstract": "Providing natural language instructions in prompts is a useful new paradigm for improving task performance of large language models in a zero-shot setting. Recent work has aimed to improve such prompts via manual rewriting or gradient-based tuning. However, manual rewriting is time-consuming and requires subjective interpretation, while gradient-based tuning can be extremely computationally demanding for large models and requires full access to model weights, which may not be available for API-based models. In this work, we introduce Gradient-free Instructional Prompt Search (GrIPS), a gradient-free, edit-based search approach for improving task instructions for large language models. GrIPS takes in instructions designed for humans and automatically returns an improved, edited prompt, while allowing for API-based tuning. The instructions in our search are iteratively edited using four operations (delete, add, swap, paraphrase) on text at the phrase-level. With InstructGPT models, GrIPS improves the average task performance by up to 4.30 percentage points on eight classification tasks from the Natural-Instructions dataset. We see improvements for both instruction-only prompts and for k-shot example+instruction prompts. Notably, GrIPS outperforms manual rewriting following the guidelines in Mishra et al. (2022) and also outperforms purely example-based prompts while controlling for the available compute and data budget. Lastly, we provide qualitative analysis of the edited instructions across several scales of GPT models. Our code is available at: https://github.com/archiki/GrIPS", "paper_url": "http://arxiv.org/abs/2203.07281v1", "pdf_url": "http://arxiv.org/pdf/2203.07281v1", "repo_url": "https://github.com/archiki/grips"}, "2203.07264": {"publish_time": "2022-03-14", "title": "Show Me More Details: Discovering Hierarchies of Procedures from Semi-structured Web Data", "author": "Shuyan Zhou et.al.", "abstract": "Procedures are inherently hierarchical. To \"make videos\", one may need to \"purchase a camera\", which in turn may require one to \"set a budget\". While such hierarchical knowledge is critical for reasoning about complex procedures, most existing work has treated procedures as shallow structures without modeling the parent-child relation.In this work, we attempt to construct an open-domain hierarchical knowledge-base (KB) of procedures based on wikiHow, a website containing more than 110k instructional articles, each documenting the steps to carry out a complex procedure. To this end, we develop a simple and efficient method that links steps (e.g., \"purchase a camera\") in an article to other articles with similar goals (e.g., \"how to choose a camera\"), recursively constructing the KB. Our method significantly outperforms several strong baselines according to automatic evaluation, human judgment, and application to downstream tasks such as instructional video retrieval.   A demo with partial data can be found at https://wikihow-hierarchy.github.io. The code and the data are at https://github.com/shuyanzhou/wikihow_hierarchy.", "paper_url": "http://arxiv.org/abs/2203.07264v1", "pdf_url": "http://arxiv.org/pdf/2203.07264v1", "repo_url": "https://github.com/shuyanzhou/wikihow_hierarchy"}, "2203.07259": {"publish_time": "2022-03-14", "title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models", "author": "Eldar Kurtic et.al.", "abstract": "Pre-trained Transformer-based language models have become a key building block for natural language processing (NLP) tasks. While these models are extremely accurate, they can be too large and computationally intensive to run on standard deployments. A variety of compression methods, including distillation, quantization, structured and unstructured pruning are known to be applicable to decrease model size and increase inference speed. In this context, this paper's contributions are two-fold. We begin with an in-depth study of the accuracy-compression trade-off for unstructured weight pruning in the context of BERT models, and introduce Optimal BERT Surgeon (O-BERT-S), an efficient and accurate weight pruning method based on approximate second-order information, which we show to yield state-of-the-art results in terms of the compression/accuracy trade-off. Specifically, Optimal BERT Surgeon extends existing work on second-order pruning by allowing for pruning blocks of weights, and by being applicable at BERT scale. Second, we investigate the impact of this pruning method when compounding compression approaches for Transformer-based models, which allows us to combine state-of-the-art structured and unstructured pruning together with quantization, in order to obtain highly compressed, but accurate models. The resulting compression framework is powerful, yet general and efficient: we apply it to both the fine-tuning and pre-training stages of language tasks, to obtain state-of-the-art results on the accuracy-compression trade-off with relatively simple compression recipes. For example, we obtain 10x model size compression with < 1% relative drop in accuracy to the dense BERT-base, 10x end-to-end CPU-inference speedup with < 2% relative drop in accuracy, and 29x inference speedups with < 7.5% relative accuracy drop.", "paper_url": "http://arxiv.org/abs/2203.07259v1", "pdf_url": "http://arxiv.org/pdf/2203.07259v1", "repo_url": null}, "2203.08118": {"publish_time": "2022-03-15", "title": "Representation Learning for Resource-Constrained Keyphrase Generation", "author": "Di Wu et.al.", "abstract": "State-of-the-art keyphrase generation methods generally depend on large annotated datasets, limiting their performance in domains with constrained resources. To overcome this challenge, we investigate strategies to learn an intermediate representation suitable for the keyphrase generation task. We introduce salient span recovery and salient span prediction as guided denoising language modeling objectives that condense the domain-specific knowledge essential for keyphrase generation. Through experiments on multiple scientific keyphrase generation benchmarks, we show the effectiveness of the proposed approach for facilitating low-resource and zero-shot keyphrase generation. Furthermore, we observe that our method especially benefits the generation of absent keyphrases, approaching the performance of SOTA methods trained with large training sets.", "paper_url": "http://arxiv.org/abs/2203.08118v1", "pdf_url": "http://arxiv.org/pdf/2203.08118v1", "repo_url": "https://github.com/xiaowu0162/low-resource-kpgen"}, "2203.08111": {"publish_time": "2022-03-15", "title": "Does Corpus Quality Really Matter for Low-Resource Languages?", "author": "Mikel Artetxe et.al.", "abstract": "The vast majority of non-English corpora are derived from automatically filtered versions of CommonCrawl. While prior work has identified major issues on the quality of these datasets (Kreutzer et al., 2021), it is not clear how this impacts downstream performance. Taking Basque as a case study, we explore tailored crawling (manually identifying and scraping websites with high-quality content) as an alternative to filtering CommonCrawl. Our new corpus, called EusCrawl, is similar in size to the Basque portion of popular multilingual corpora like CC100 and mC4, yet it has a much higher quality according to native annotators. For instance, 66% of documents are rated as high-quality for EusCrawl, in contrast with <33% for both mC4 and CC100. Nevertheless, we obtain similar results on downstream tasks regardless of the corpus used for pre-training. Our work suggests that NLU performance in low-resource languages is primarily constrained by the quantity rather than the quality of the data, prompting for methods to exploit more diverse data sources.", "paper_url": "http://arxiv.org/abs/2203.08111v1", "pdf_url": "http://arxiv.org/pdf/2203.08111v1", "repo_url": null}, "2203.08085": {"publish_time": "2022-03-15", "title": "Measuring the Impact of (Psycho-)Linguistic and Readability Features and Their Spill Over Effects on the Prediction of Eye Movement Patterns", "author": "Daniel Wiechmann et.al.", "abstract": "There is a growing interest in the combined use of NLP and machine learning methods to predict gaze patterns during naturalistic reading. While promising results have been obtained through the use of transformer-based language models, little work has been undertaken to relate the performance of such models to general text characteristics. In this paper we report on experiments with two eye-tracking corpora of naturalistic reading and two language models (BERT and GPT-2). In all experiments, we test effects of a broad spectrum of features for predicting human reading behavior that fall into five categories (syntactic complexity, lexical richness, register-based multiword combinations, readability and psycholinguistic word properties). Our experiments show that both the features included and the architecture of the transformer-based language models play a role in predicting multiple eye-tracking measures during naturalistic reading. We also report the results of experiments aimed at determining the relative importance of features from different groups using SP-LIME.", "paper_url": "http://arxiv.org/abs/2203.08085v1", "pdf_url": "http://arxiv.org/pdf/2203.08085v1", "repo_url": null}, "2203.08075": {"publish_time": "2022-03-15", "title": "Things not Written in Text: Exploring Spatial Commonsense from Visual Signals", "author": "Xiao Liu et.al.", "abstract": "Spatial commonsense, the knowledge about spatial position and relationship between objects (like the relative size of a lion and a girl, and the position of a boy relative to a bicycle when cycling), is an important part of commonsense knowledge. Although pretrained language models (PLMs) succeed in many NLP tasks, they are shown to be ineffective in spatial commonsense reasoning. Starting from the observation that images are more likely to exhibit spatial commonsense than texts, we explore whether models with visual signals learn more spatial commonsense than text-based PLMs. We propose a spatial commonsense benchmark that focuses on the relative scales of objects, and the positional relationship between people and objects under different actions. We probe PLMs and models with visual signals, including vision-language pretrained models and image synthesis models, on this benchmark, and find that image synthesis models are more capable of learning accurate and consistent spatial knowledge than other models. The spatial knowledge from image synthesis models also helps in natural language understanding tasks that require spatial commonsense.", "paper_url": "http://arxiv.org/abs/2203.08075v1", "pdf_url": "http://arxiv.org/pdf/2203.08075v1", "repo_url": "https://github.com/xxxiaol/spatial-commonsense"}, "2203.08055": {"publish_time": "2022-03-15", "title": "Modular and Parameter-Efficient Multimodal Fusion with Prompting", "author": "Sheng Liang et.al.", "abstract": "Recent research has made impressive progress in large-scale multimodal pre-training. In the context of the rapid growth of model size, it is necessary to seek efficient and flexible methods other than finetuning. In this paper, we propose to use prompt vectors to align the modalities. Our method achieves comparable performance to several other multimodal fusion methods in low-resource settings. We further show that our method is modular and parameter-efficient for processing tasks involving two or more data modalities.", "paper_url": "http://arxiv.org/abs/2203.08055v1", "pdf_url": "http://arxiv.org/pdf/2203.08055v1", "repo_url": null}, "2203.08788": {"publish_time": "2022-03-16", "title": "Are Shortest Rationales the Best Explanations for Human Understanding?", "author": "Hua Shen et.al.", "abstract": "Existing self-explaining models typically favor extracting the shortest possible rationales - snippets of an input text \"responsible for\" corresponding output - to explain the model prediction, with the assumption that shorter rationales are more intuitive to humans. However, this assumption has yet to be validated. Is the shortest rationale indeed the most human-understandable? To answer this question, we design a self-explaining model, LimitedInk, which allows users to extract rationales at any target length. Compared to existing baselines, LimitedInk achieves compatible end-task performance and human-annotated rationale agreement, making it a suitable representation of the recent class of self-explaining models. We use LimitedInk to conduct a user study on the impact of rationale length, where we ask human judges to predict the sentiment label of documents based only on LimitedInk-generated rationales with different lengths. We show rationales that are too short do not help humans predict labels better than randomly masked text, suggesting the need for more careful design of the best human rationales.", "paper_url": "http://arxiv.org/abs/2203.08788v1", "pdf_url": "http://arxiv.org/pdf/2203.08788v1", "repo_url": null}, "2203.08774": {"publish_time": "2022-03-16", "title": "CUE Vectors: Modular Training of Language Models Conditioned on Diverse Contextual Signals", "author": "Scott Novotney et.al.", "abstract": "We propose a framework to modularize the training of neural language models that use diverse forms of sentence-external context (including metadata) by eliminating the need to jointly train sentence-external and within-sentence encoders. Our approach, contextual universal embeddings (CUE), trains LMs on one set of context, such as date and author, and adapts to novel metadata types, such as article title, or previous sentence. The model consists of a pretrained neural sentence LM, a BERT-based context encoder, and a masked transformer decoder that estimates LM probabilities using sentence-internal and sentence-external information. When context or metadata are unavailable, our model learns to combine contextual and sentence-internal information using noisy oracle unigram embeddings as a proxy. Real contextual information can be introduced later and used to adapt a small number of parameters that map contextual data into the decoder's embedding space. We validate the CUE framework on a NYTimes text corpus with multiple metadata types, for which the LM perplexity can be lowered from 36.6 to 27.4 by conditioning on context. Bootstrapping a contextual LM with only a subset of the context/metadata during training retains 85\\% of the achievable gain. Training the model initially with proxy context retains 67% of the perplexity gain after adapting to real context. Furthermore, we can swap one type of pretrained sentence LM for another without retraining the context encoders, by only adapting the decoder model. Overall, we obtain a modular framework that allows incremental, scalable training of context-enhanced LMs.", "paper_url": "http://arxiv.org/abs/2203.08774v1", "pdf_url": "http://arxiv.org/pdf/2203.08774v1", "repo_url": null}, "2203.08773": {"publish_time": "2022-03-16", "title": "Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data", "author": "Shuohang Wang et.al.", "abstract": "Retrieval-based methods have been shown to be effective in NLP tasks via introducing external knowledge. However, the indexing and retrieving of large-scale corpora bring considerable computational cost. Surprisingly, we found that REtrieving from the traINing datA (REINA) only can lead to significant gains on multiple NLG and NLU tasks. We retrieve the labeled training instances most similar to the input text and then concatenate them with the input to feed into the model to generate the output. Experimental results show that this simple method can achieve significantly better performance on a variety of NLU and NLG tasks, including summarization, machine translation, language modeling, and question answering tasks. For instance, our proposed method achieved state-of-the-art results on XSum, BigPatent, and CommonsenseQA. Our code is released, https://github.com/microsoft/REINA .", "paper_url": "http://arxiv.org/abs/2203.08773v1", "pdf_url": "http://arxiv.org/pdf/2203.08773v1", "repo_url": "https://github.com/microsoft/reina"}, "2203.08757": {"publish_time": "2022-03-16", "title": "Sample, Translate, Recombine: Leveraging Audio Alignments for Data Augmentation in End-to-end Speech Translation", "author": "Tsz Kin Lam et.al.", "abstract": "End-to-end speech translation relies on data that pair source-language speech inputs with corresponding translations into a target language. Such data are notoriously scarce, making synthetic data augmentation by back-translation or knowledge distillation a necessary ingredient of end-to-end training. In this paper, we present a novel approach to data augmentation that leverages audio alignments, linguistic properties, and translation. First, we augment a transcription by sampling from a suffix memory that stores text and audio data. Second, we translate the augmented transcript. Finally, we recombine concatenated audio segments and the generated translation. Besides training an MT-system, we only use basic off-the-shelf components without fine-tuning. While having similar resource demands as knowledge distillation, adding our method delivers consistent improvements of up to 0.9 and 1.1 BLEU points on five language pairs on CoVoST 2 and on two language pairs on Europarl-ST, respectively.", "paper_url": "http://arxiv.org/abs/2203.08757v1", "pdf_url": "http://arxiv.org/pdf/2203.08757v1", "repo_url": null}, "2203.08745": {"publish_time": "2022-03-16", "title": "Multi-Stage Prompting for Knowledgeable Dialogue Generation", "author": "Zihan Liu et.al.", "abstract": "Existing knowledge-grounded dialogue systems typically use finetuned versions of a pretrained language model (LM) and large-scale knowledge bases. These models typically fail to generalize on topics outside of the knowledge base, and require maintaining separate potentially large checkpoints each time finetuning is needed. In this paper, we aim to address these limitations by leveraging the inherent knowledge stored in the pretrained LM as well as its powerful generation ability. We propose a multi-stage prompting approach to generate knowledgeable responses from a single pretrained LM. We first prompt the LM to generate knowledge based on the dialogue context. Then, we further prompt it to generate responses based on the dialogue context and the previously generated knowledge. Results show that our knowledge generator outperforms the state-of-the-art retrieval-based model by 5.8% when combining knowledge relevance and correctness. In addition, our multi-stage prompting outperforms the finetuning-based dialogue model in terms of response knowledgeability and engagement by up to 10% and 5%, respectively. Furthermore, we scale our model up to 530 billion parameters and show that larger LMs improve the generation correctness score by up to 10%, and response relevance, knowledgeability and engagement by up to 10%. Our code is available at: https://github.com/NVIDIA/Megatron-LM.", "paper_url": "http://arxiv.org/abs/2203.08745v1", "pdf_url": "http://arxiv.org/pdf/2203.08745v1", "repo_url": "https://github.com/NVIDIA/Megatron-LM"}, "2203.09509": {"publish_time": "2022-03-17", "title": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection", "author": "Thomas Hartvigsen et.al.", "abstract": "Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language. To help mitigate these issues, we create ToxiGen, a new large-scale and machine-generated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model. Controlling machine generation in this way allows ToxiGen to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human-written text. We conduct a human evaluation on a challenging subset of ToxiGen and find that annotators struggle to distinguish machine-generated text from human-written language. We also find that 94.5% of toxic examples are labeled as hate speech by human annotators. Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially. We also demonstrate that ToxiGen can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset.", "paper_url": "http://arxiv.org/abs/2203.09509v1", "pdf_url": "http://arxiv.org/pdf/2203.09509v1", "repo_url": null}, "2203.09498": {"publish_time": "2022-03-17", "title": "The Frost Hollow Experiments: Pavlovian Signalling as a Path to Coordination and Communication Between Agents", "author": "Patrick M. Pilarski et.al.", "abstract": "Learned communication between agents is a powerful tool when approaching decision-making problems that are hard to overcome by any single agent in isolation. However, continual coordination and communication learning between machine agents or human-machine partnerships remains a challenging open problem. As a stepping stone toward solving the continual communication learning problem, in this paper we contribute a multi-faceted study into what we term Pavlovian signalling -- a process by which learned, temporally extended predictions made by one agent inform decision-making by another agent with different perceptual access to their shared environment. We seek to establish how different temporal processes and representational choices impact Pavlovian signalling between learning agents. To do so, we introduce a partially observable decision-making domain we call the Frost Hollow. In this domain a prediction learning agent and a reinforcement learning agent are coupled into a two-part decision-making system that seeks to acquire sparse reward while avoiding time-conditional hazards. We evaluate two domain variations: 1) machine prediction and control learning in a linear walk, and 2) a prediction learning machine interacting with a human participant in a virtual reality environment. Our results showcase the speed of learning for Pavlovian signalling, the impact that different temporal representations do (and do not) have on agent-agent coordination, and how temporal aliasing impacts agent-agent and human-agent interactions differently. As a main contribution, we establish Pavlovian signalling as a natural bridge between fixed signalling paradigms and fully adaptive communication learning. Our results therefore point to an actionable, constructivist path towards continual communication learning between reinforcement learning agents, with potential impact in a range of real-world settings.", "paper_url": "http://arxiv.org/abs/2203.09498v1", "pdf_url": "http://arxiv.org/pdf/2203.09498v1", "repo_url": null}, "2203.09486": {"publish_time": "2022-03-17", "title": "An Imitation Learning Curriculum for Text Editing with Non-Autoregressive Models", "author": "Sweta Agrawal et.al.", "abstract": "We propose a framework for training non-autoregressive sequence-to-sequence models for editing tasks, where the original input sequence is iteratively edited to produce the output. We show that the imitation learning algorithms designed to train such models for machine translation introduces mismatches between training and inference that lead to undertraining and poor generalization in editing scenarios. We address this issue with two complementary strategies: 1) a roll-in policy that exposes the model to intermediate training sequences that it is more likely to encounter during inference, 2) a curriculum that presents easy-to-learn edit operations first, gradually increasing the difficulty of training samples as the model becomes competent. We show the efficacy of these strategies on two challenging English editing tasks: controllable text simplification and abstractive summarization. Our approach significantly improves output quality on both tasks and controls output complexity better on the simplification task.", "paper_url": "http://arxiv.org/abs/2203.09486v1", "pdf_url": "http://arxiv.org/pdf/2203.09486v1", "repo_url": null}, "2203.09435": {"publish_time": "2022-03-17", "title": "Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation", "author": "Xinyi Wang et.al.", "abstract": "The performance of multilingual pretrained models is highly dependent on the availability of monolingual or parallel text present in a target language. Thus, the majority of the world's languages cannot benefit from recent progress in NLP as they have no or limited textual data. To expand possibilities of using NLP technology in these under-represented languages, we systematically study strategies that relax the reliance on conventional language resources through the use of bilingual lexicons, an alternative resource with much better language coverage. We analyze different strategies to synthesize textual or labeled data using lexicons, and how this data can be combined with monolingual or parallel text when available. For 19 under-represented languages across 3 tasks, our methods lead to consistent improvements of up to 5 and 15 points with and without extra monolingual text respectively. Overall, our study highlights how NLP methods can be adapted to thousands more languages that are under-served by current technology", "paper_url": "http://arxiv.org/abs/2203.09435v1", "pdf_url": "http://arxiv.org/pdf/2203.09435v1", "repo_url": "https://github.com/cindyxinyiwang/expand-via-lexicon-based-adaptation"}, "2203.09424": {"publish_time": "2022-03-17", "title": "elBERto: Self-supervised Commonsense Learning for Question Answering", "author": "Xunlin Zhan et.al.", "abstract": "Commonsense question answering requires reasoning about everyday situations and causes and effects implicit in context. Typically, existing approaches first retrieve external evidence and then perform commonsense reasoning using these evidence. In this paper, we propose a Self-supervised Bidirectional Encoder Representation Learning of Commonsense (elBERto) framework, which is compatible with off-the-shelf QA model architectures. The framework comprises five self-supervised tasks to force the model to fully exploit the additional training signals from contexts containing rich commonsense. The tasks include a novel Contrastive Relation Learning task to encourage the model to distinguish between logically contrastive contexts, a new Jigsaw Puzzle task that requires the model to infer logical chains in long contexts, and three classic SSL tasks to maintain pre-trained models language encoding ability. On the representative WIQA, CosmosQA, and ReClor datasets, elBERto outperforms all other methods, including those utilizing explicit graph reasoning and external knowledge retrieval. Moreover, elBERto achieves substantial improvements on out-of-paragraph and no-effect questions where simple lexical similarity comparison does not help, indicating that it successfully learns commonsense and is able to leverage it when given dynamic context.", "paper_url": "http://arxiv.org/abs/2203.09424v1", "pdf_url": "http://arxiv.org/pdf/2203.09424v1", "repo_url": null}, "2203.10079": {"publish_time": "2022-03-18", "title": "Simulating Bandit Learning from User Feedback for Extractive Question Answering", "author": "Ge Gao et.al.", "abstract": "We study learning from user feedback for extractive question answering by simulating feedback using supervised data. We cast the problem as contextual bandit learning, and analyze the characteristics of several learning scenarios with focus on reducing data annotation. We show that systems initially trained on a small number of examples can dramatically improve given feedback from users on model-predicted answers, and that one can use existing datasets to deploy systems in new domains without any annotation, but instead improving the system on-the-fly via user feedback.", "paper_url": "http://arxiv.org/abs/2203.10079v1", "pdf_url": "http://arxiv.org/pdf/2203.10079v1", "repo_url": "https://github.com/lil-lab/bandit-qa"}, "2203.10053": {"publish_time": "2022-03-18", "title": "RELIC: Retrieving Evidence for Literary Claims", "author": "Katherine Thai et.al.", "abstract": "Humanities scholars commonly provide evidence for claims that they make about a work of literature (e.g., a novel) in the form of quotations from the work. We collect a large-scale dataset (RELiC) of 78K literary quotations and surrounding critical analysis and use it to formulate the novel task of literary evidence retrieval, in which models are given an excerpt of literary analysis surrounding a masked quotation and asked to retrieve the quoted passage from the set of all passages in the work. Solving this retrieval task requires a deep understanding of complex literary and linguistic phenomena, which proves challenging to methods that overwhelmingly rely on lexical and semantic similarity matching. We implement a RoBERTa-based dense passage retriever for this task that outperforms existing pretrained information retrieval baselines; however, experiments and analysis by human domain experts indicate that there is substantial room for improvement over our dense retriever.", "paper_url": "http://arxiv.org/abs/2203.10053v1", "pdf_url": "http://arxiv.org/pdf/2203.10053v1", "repo_url": null}, "2203.10024": {"publish_time": "2022-03-18", "title": "Offensive Language Detection in Under-resourced Algerian Dialectal Arabic Language", "author": "Oussama Boucherit et.al.", "abstract": "This paper addresses the problem of detecting the offensive and abusive content in Facebook comments, where we focus on the Algerian dialectal Arabic which is one of under-resourced languages. The latter has a variety of dialects mixed with different languages (i.e. Berber, French and English). In addition, we deal with texts written in both Arabic and Roman scripts (i.e. Arabizi). Due to the scarcity of works on the same language, we have built a new corpus regrouping more than 8.7k texts manually annotated as normal, abusive and offensive. We have conducted a series of experiments using the state-of-the-art classifiers of text categorisation, namely: BiLSTM, CNN, FastText, SVM and NB. The results showed acceptable performances, but the problem requires further investigation on linguistic features to increase the identification accuracy.", "paper_url": "http://arxiv.org/abs/2203.10024v1", "pdf_url": "http://arxiv.org/pdf/2203.10024v1", "repo_url": "https://github.com/xprogramer/dziriofn"}, "2203.10020": {"publish_time": "2022-03-18", "title": "Challenges and Strategies in Cross-Cultural NLP", "author": "Daniel Hershcovich et.al.", "abstract": "Various efforts in the Natural Language Processing (NLP) community have been made to accommodate linguistic diversity and serve speakers of many different languages. However, it is important to acknowledge that speakers and the content they produce and require, vary not just by language, but also by culture. Although language and culture are tightly linked, there are important differences. Analogous to cross-lingual and multilingual NLP, cross-cultural and multicultural NLP considers these differences in order to better serve users of NLP systems. We propose a principled framework to frame these efforts, and survey existing and potential strategies.", "paper_url": "http://arxiv.org/abs/2203.10020v1", "pdf_url": "http://arxiv.org/pdf/2203.10020v1", "repo_url": null}, "2203.10012": {"publish_time": "2022-03-18", "title": "Report from the NSF Future Directions Workshop on Automatic Evaluation of Dialog: Research Directions and Challenges", "author": "Shikib Mehri et.al.", "abstract": "This is a report on the NSF Future Directions Workshop on Automatic Evaluation of Dialog. The workshop explored the current state of the art along with its limitations and suggested promising directions for future work in this important and very rapidly changing area of research.", "paper_url": "http://arxiv.org/abs/2203.10012v1", "pdf_url": "http://arxiv.org/pdf/2203.10012v1", "repo_url": null}, "2203.11187": {"publish_time": "2022-03-21", "title": "Relevant CommonSense Subgraphs for \"What if...\" Procedural Reasoning", "author": "Chen Zheng et.al.", "abstract": "We study the challenge of learning causal reasoning over procedural text to answer \"What if...\" questions when external commonsense knowledge is required. We propose a novel multi-hop graph reasoning model to 1) efficiently extract a commonsense subgraph with the most relevant information from a large knowledge graph; 2) predict the causal answer by reasoning over the representations obtained from the commonsense subgraph and the contextual interactions between the questions and context. We evaluate our model on WIQA benchmark and achieve state-of-the-art performance compared to the recent models.", "paper_url": "http://arxiv.org/abs/2203.11187v1", "pdf_url": "http://arxiv.org/pdf/2203.11187v1", "repo_url": null}, "2203.11171": {"publish_time": "2022-03-21", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "author": "Xuezhi Wang et.al.", "abstract": "We explore a simple ensemble strategy, self-consistency, that significantly improves the reasoning accuracy of large language models. The idea is to sample a diverse set of outputs from a language model and return the most consistent answer in the set. Such ensembling method improves reasoning accuracy when combined with chain of thought prompting. For arithmetic and commonsense reasoning benchmarks we find that self-consistency yields significant accuracy improvements in a variety of datasets, such as GSM8K (+10%), SVAMP (+14%), MultiArith (+24%), CommonsenseQA (+5%) and ARC (easy +4%, challenge +5%).", "paper_url": "http://arxiv.org/abs/2203.11171v1", "pdf_url": "http://arxiv.org/pdf/2203.11171v1", "repo_url": null}, "2203.11147": {"publish_time": "2022-03-21", "title": "Teaching language models to support answers with verified quotes", "author": "Jacob Menick et.al.", "abstract": "Recent large language models often answer factual questions correctly. But users can't trust any given claim a model makes without fact-checking, because language models can hallucinate convincing nonsense. In this work we use reinforcement learning from human preferences (RLHP) to train \"open-book\" QA models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness. Supporting evidence is drawn from multiple documents found via a search engine, or from a single user-provided document. Our 280 billion parameter model, GopherCite, is able to produce answers with high quality supporting evidence and abstain from answering when unsure. We measure the performance of GopherCite by conducting human evaluation of answers to questions in a subset of the NaturalQuestions and ELI5 datasets. The model's response is found to be high-quality 80\\% of the time on this Natural Questions subset, and 67\\% of the time on the ELI5 subset. Abstaining from the third of questions for which it is most unsure improves performance to 90\\% and 80\\% respectively, approaching human baselines. However, analysis on the adversarial TruthfulQA dataset shows why citation is only one part of an overall strategy for safety and trustworthiness: not all claims supported by evidence are true.", "paper_url": "http://arxiv.org/abs/2203.11147v1", "pdf_url": "http://arxiv.org/pdf/2203.11147v1", "repo_url": null}, "2203.11131": {"publish_time": "2022-03-21", "title": "Towards Explainable Evaluation Metrics for Natural Language Generation", "author": "Christoph Leiter et.al.", "abstract": "Unlike classical lexical overlap metrics such as BLEU, most current evaluation metrics (such as BERTScore or MoverScore) are based on black-box language models such as BERT or XLM-R. They often achieve strong correlations with human judgments, but recent research indicates that the lower-quality classical metrics remain dominant, one of the potential reasons being that their decision processes are transparent. To foster more widespread acceptance of the novel high-quality metrics, explainability thus becomes crucial. In this concept paper, we identify key properties and propose key goals of explainable machine translation evaluation metrics. We also provide a synthesizing overview over recent approaches for explainable machine translation metrics and discuss how they relate to those goals and properties. Further, we conduct own novel experiments, which (among others) find that current adversarial NLP techniques are unsuitable for automatically identifying limitations of high-quality black-box evaluation metrics, as they are not meaning-preserving. Finally, we provide a vision of future approaches to explainable evaluation metrics and their evaluation. We hope that our work can help catalyze and guide future research on explainable evaluation metrics and, mediately, also contribute to better and more transparent text generation systems.", "paper_url": "http://arxiv.org/abs/2203.11131v1", "pdf_url": "http://arxiv.org/pdf/2203.11131v1", "repo_url": null}, "2203.11130": {"publish_time": "2022-03-21", "title": "PACS: A Dataset for Physical Audiovisual CommonSense Reasoning", "author": "Samuel Yu et.al.", "abstract": "In order for AI to be safely deployed in real-world scenarios such as hospitals, schools, and the workplace, they should be able to reason about the physical world by understanding the physical properties and affordances of available objects, how they can be manipulated, and how they interact with other physical objects. This research field of physical commonsense reasoning is fundamentally a multi-sensory task since physical properties are manifested through multiple modalities, two of them being vision and acoustics. Our paper takes a step towards real-world physical commonsense reasoning by contributing PACS: the first audiovisual benchmark annotated for physical commonsense attributes. PACS contains a total of 13,400 question-answer pairs, involving 1,377 unique physical commonsense questions and 1,526 videos. Our dataset provides new opportunities to advance the research field of physical reasoning by bringing audio as a core component of this multimodal problem. Using PACS, we evaluate multiple state-of-the-art models on this new challenging task. While some models show promising results (70% accuracy), they all fall short of human performance (95% accuracy). We conclude the paper by demonstrating the importance of multimodal reasoning and providing possible avenues for future research.", "paper_url": "http://arxiv.org/abs/2203.11130v1", "pdf_url": "http://arxiv.org/pdf/2203.11130v1", "repo_url": null}, "2203.11933": {"publish_time": "2022-03-22", "title": "A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning", "author": "Hugo Berg et.al.", "abstract": "Vision-language models can encode societal biases and stereotypes, but there are challenges to measuring and mitigating these harms. Prior proposed bias measurements lack robustness and feature degradation occurs when mitigating bias without access to pretraining data. We address both of these challenges in this paper: First, we evaluate different bias measures and propose the use of retrieval metrics to image-text representations via a bias measuring framework. Second, we investigate debiasing methods and show that optimizing for adversarial loss via learnable token embeddings minimizes various bias measures without substantially degrading feature representations.", "paper_url": "http://arxiv.org/abs/2203.11933v1", "pdf_url": "http://arxiv.org/pdf/2203.11933v1", "repo_url": "https://github.com/oxai/debias-vision-lang"}, "2203.11899": {"publish_time": "2022-03-22", "title": "Transformer based ensemble for emotion detection", "author": "Aditya Kane et.al.", "abstract": "Detecting emotions in languages is important to accomplish a complete interaction between humans and machines. This paper describes our contribution to the WASSA 2022 shared task which handles this crucial task of emotion detection. We have to identify the following emotions: sadness, surprise, neutral, anger, fear, disgust, joy based on a given essay text. We are using an ensemble of ELECTRA and BERT models to tackle this problem achieving an F1 score of 62.76%. Our codebase (https://bit.ly/WASSA_shared_task) and our WandB project (https://wandb.ai/acl_wassa_pictxmanipal/acl_wassa) is available.", "paper_url": "http://arxiv.org/abs/2203.11899v1", "pdf_url": "http://arxiv.org/pdf/2203.11899v1", "repo_url": null}, "2203.11856": {"publish_time": "2022-03-22", "title": "A Computational Approach to Understand Mental Health from Reddit: Knowledge-aware Multitask Learning Framework", "author": "Usha Lokala et.al.", "abstract": "Analyzing gender is critical to study mental health (MH) support in CVD (cardiovascular disease). The existing studies on using social media for extracting MH symptoms consider symptom detection and tend to ignore user context, disease, or gender. The current study aims to design and evaluate a system to capture how MH symptoms associated with CVD are expressed differently with the gender on social media. We observe that the reliable detection of MH symptoms expressed by persons with heart disease in user posts is challenging because of the co-existence of (dis)similar MH symptoms in one post and due to variation in the description of symptoms based on gender. We collect a corpus of $150k$ items (posts and comments) annotated using the subreddit labels and transfer learning approaches. We propose GeM, a novel task-adaptive multi-task learning approach to identify the MH symptoms in CVD patients based on gender. Specifically, we adapt a knowledge-assisted RoBERTa based bi-encoder model to capture CVD-related MH symptoms. Moreover, it enhances the reliability for differentiating the gender language in MH symptoms when compared to the state-of-art language models. Our model achieves high (statistically significant) performance and predicts four labels of MH issues and two gender labels, which outperforms RoBERTa, improving the recall by 2.14% on the symptom identification task and by 2.55% on the gender identification task.", "paper_url": "http://arxiv.org/abs/2203.11856v1", "pdf_url": "http://arxiv.org/pdf/2203.11856v1", "repo_url": null}, "2203.11849": {"publish_time": "2022-03-22", "title": "A Girl Has A Name, And It's ... Adversarial Authorship Attribution for Deobfuscation", "author": "Wanyue Zhai et.al.", "abstract": "Recent advances in natural language processing have enabled powerful privacy-invasive authorship attribution. To counter authorship attribution, researchers have proposed a variety of rule-based and learning-based text obfuscation approaches. However, existing authorship obfuscation approaches do not consider the adversarial threat model. Specifically, they are not evaluated against adversarially trained authorship attributors that are aware of potential obfuscation. To fill this gap, we investigate the problem of adversarial authorship attribution for deobfuscation. We show that adversarially trained authorship attributors are able to degrade the effectiveness of existing obfuscators from 20-30% to 5-10%. We also evaluate the effectiveness of adversarial training when the attributor makes incorrect assumptions about whether and which obfuscator was used. While there is a a clear degradation in attribution accuracy, it is noteworthy that this degradation is still at or above the attribution accuracy of the attributor that is not adversarially trained at all. Our results underline the need for stronger obfuscation approaches that are resistant to deobfuscation", "paper_url": "http://arxiv.org/abs/2203.11849v1", "pdf_url": "http://arxiv.org/pdf/2203.11849v1", "repo_url": null}, "2203.11841": {"publish_time": "2022-03-22", "title": "SU-NLP at SemEval-2022 Task 11: Complex Named Entity Recognition with Entity Linking", "author": "Buse \u00c7ar\u0131k et.al.", "abstract": "This paper describes the system proposed by Sabanc{\\i} University Natural Language Processing Group in the SemEval-2022 MultiCoNER task. We developed an unsupervised entity linking pipeline that detects potential entity mentions with the help of Wikipedia and also uses the corresponding Wikipedia context to help the classifier in finding the named entity type of that mention. Our results showed that our pipeline improved performance significantly, especially for complex entities in low-context settings.", "paper_url": "http://arxiv.org/abs/2203.11841v1", "pdf_url": "http://arxiv.org/pdf/2203.11841v1", "repo_url": null}, "2203.12574": {"publish_time": "2022-03-23", "title": "Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal", "author": "Umang Gupta et.al.", "abstract": "Language models excel at generating coherent text, and model compression techniques such as knowledge distillation have enabled their use in resource-constrained settings. However, these models can be biased in multiple ways, including the unfounded association of male and female genders with gender-neutral professions. Therefore, knowledge distillation without any fairness constraints may preserve or exaggerate the teacher model's biases onto the distilled model. To this end, we present a novel approach to mitigate gender disparity in text generation by learning a fair model during knowledge distillation. We propose two modifications to the base knowledge distillation based on counterfactual role reversal$\\unicode{x2014}$modifying teacher probabilities and augmenting the training set. We evaluate gender polarity across professions in open-ended text generated from the resulting distilled and finetuned GPT$\\unicode{x2012}$2 models and demonstrate a substantial reduction in gender disparity with only a minor compromise in utility. Finally, we observe that language models that reduce gender polarity in language generation do not improve embedding fairness or downstream classification fairness.", "paper_url": "http://arxiv.org/abs/2203.12574v1", "pdf_url": "http://arxiv.org/pdf/2203.12574v1", "repo_url": null}, "2203.12536": {"publish_time": "2022-03-23", "title": "Dynamically Refined Regularization for Improving Cross-corpora Hate Speech Detection", "author": "Tulika Bose et.al.", "abstract": "Hate speech classifiers exhibit substantial performance degradation when evaluated on datasets different from the source. This is due to learning spurious correlations between words that are not necessarily relevant to hateful language, and hate speech labels from the training corpus. Previous work has attempted to mitigate this problem by regularizing specific terms from pre-defined static dictionaries. While this has been demonstrated to improve the generalizability of classifiers, the coverage of such methods is limited and the dictionaries require regular manual updates from human experts. In this paper, we propose to automatically identify and reduce spurious correlations using attribution methods with dynamic refinement of the list of terms that need to be regularized during training. Our approach is flexible and improves the cross-corpora performance over previous work independently and in combination with pre-defined dictionaries.", "paper_url": "http://arxiv.org/abs/2203.12536v1", "pdf_url": "http://arxiv.org/pdf/2203.12536v1", "repo_url": "https://github.com/tbose20/d-ref"}, "2203.12524": {"publish_time": "2022-03-23", "title": "Computational historical linguistics and language diversity in South Asia", "author": "Aryaman Arora et.al.", "abstract": "South Asia is home to a plethora of languages, many of which severely lack access to new language technologies. This linguistic diversity also results in a research environment conducive to the study of comparative, contact, and historical linguistics -- fields which necessitate the gathering of extensive data from many languages. We claim that data scatteredness (rather than scarcity) is the primary obstacle in the development of South Asian language technology, and suggest that the study of language history is uniquely aligned with surmounting this obstacle. We review recent developments in and at the intersection of South Asian NLP and historical-comparative linguistics, describing our and others' current efforts in this area. We also offer new strategies towards breaking the data barrier.", "paper_url": "http://arxiv.org/abs/2203.12524v1", "pdf_url": "http://arxiv.org/pdf/2203.12524v1", "repo_url": null}, "2203.12515": {"publish_time": "2022-03-23", "title": "A Survey on Cross-Lingual Summarization", "author": "Jiaan Wang et.al.", "abstract": "Cross-lingual summarization is the task of generating a summary in one language (e.g., English) for the given document(s) in a different language (e.g., Chinese). Under the globalization background, this task has attracted increasing attention of the computational linguistics community. Nevertheless, there still remains a lack of comprehensive review for this task. Therefore, we present the first systematic critical review on the datasets, approaches and challenges in this field. Specifically, we carefully organize existing datasets and approaches according to different construction methods and solution paradigms, respectively. For each type of datasets or approaches, we thoroughly introduce and summarize previous efforts and further compare them with each other to provide deeper analyses. In the end, we also discuss promising directions and offer our thoughts to facilitate future research. This survey is for both beginners and experts in cross-lingual summarization, and we hope it will serve as a starting point as well as a source of new ideas for researchers and engineers interested in this area.", "paper_url": "http://arxiv.org/abs/2203.12515v1", "pdf_url": "http://arxiv.org/pdf/2203.12515v1", "repo_url": null}, "2203.12487": {"publish_time": "2022-03-23", "title": "A Context-Aware Feature Fusion Framework for Punctuation Restoration", "author": "Yangjun Wu et.al.", "abstract": "To accomplish the punctuation restoration task, most existing approaches focused on leveraging extra information (e.g., part-of-speech tags) or addressing the class imbalance problem. Recent works have widely applied the transformer-based language models and significantly improved their effectiveness. To the best of our knowledge, an inherent issue has remained neglected: the attention of individual heads in the transformer will be diluted or powerless while feeding the long non-punctuation utterances. Since those previous contexts, not the followings, are comparatively more valuable to the current position, it's hard to achieve a good balance by independent attention. In this paper, we propose a novel Feature Fusion framework based on two-type Attentions (FFA) to alleviate the shortage. It introduces a two-stream architecture. One module involves interaction between attention heads to encourage the communication, and another masked attention module captures the dependent feature representation. Then, it aggregates two feature embeddings to fuse information and enhances context-awareness. The experiments on the popular benchmark dataset IWSLT demonstrate that our approach is effective. Without additional data, it obtains comparable performance to the current state-of-the-art models.", "paper_url": "http://arxiv.org/abs/2203.12487v1", "pdf_url": "http://arxiv.org/pdf/2203.12487v1", "repo_url": "https://github.com/young1993/ffa"}, "2203.13240": {"publish_time": "2022-03-24", "title": "Token Dropping for Efficient BERT Pretraining", "author": "Le Hou et.al.", "abstract": "Transformer-based models generally allocate the same amount of computation for each token in a given sequence. We develop a simple but effective \"token dropping\" method to accelerate the pretraining of transformer models, such as BERT, without degrading its performance on downstream tasks. In short, we drop unimportant tokens starting from an intermediate layer in the model to make the model focus on important tokens; the dropped tokens are later picked up by the last layer of the model so that the model still produces full-length sequences. We leverage the already built-in masked language modeling (MLM) loss to identify unimportant tokens with practically no computational overhead. In our experiments, this simple approach reduces the pretraining cost of BERT by 25% while achieving similar overall fine-tuning performance on standard downstream tasks.", "paper_url": "http://arxiv.org/abs/2203.13240v1", "pdf_url": "http://arxiv.org/pdf/2203.13240v1", "repo_url": null}, "2203.13226": {"publish_time": "2022-03-24", "title": "SMARAGD: Synthesized sMatch for Accurate and Rapid AMR Graph Distance", "author": "Juri Opitz et.al.", "abstract": "The semantic similarity of graph-based meaning representations, such as Abstract Meaning Representation (AMR), is typically assessed using graph matching algorithms, such as SMATCH (Cai and Knight, 2013). However, SMATCH suffers from NP-completeness, making its large-scale application, e.g., for AMR clustering or semantic search, infeasible. To mitigate this issue, we propose SMARAGD (Synthesized sMatch for accurate and rapid AMR graph distance). We show the potential of neural networks to approximate the SMATCH scores and graph alignments, i) in linear time using a machine translation framework to predict the alignments, or ii) in constant time using a Siamese CNN to directly predict SMATCH scores. We show that the approximation error can be substantially reduced by applying data augmentation and AMR graph anonymization.", "paper_url": "http://arxiv.org/abs/2203.13226v1", "pdf_url": "http://arxiv.org/pdf/2203.13226v1", "repo_url": null}, "2203.13224": {"publish_time": "2022-03-24", "title": "Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion", "author": "Kurt Shuster et.al.", "abstract": "Language models (LMs) have recently been shown to generate more factual responses by employing modularity (Zhou et al., 2021) in combination with retrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et al. (2021) to include internet search as a module. Our SeeKeR (Search engine->Knowledge->Response) method thus applies a single LM to three modular tasks in succession: search, generating knowledge, and generating a final response. We show that, when using SeeKeR as a dialogue model, it outperforms the state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain knowledge-grounded conversations for the same number of parameters, in terms of consistency, knowledge and per-turn engagingness. SeeKeR applied to topical prompt completions as a standard language model outperforms GPT2 (Radford et al., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality, despite GPT3 being a vastly larger model. Our code and models are made publicly available.", "paper_url": "http://arxiv.org/abs/2203.13224v1", "pdf_url": "http://arxiv.org/pdf/2203.13224v1", "repo_url": null}, "2203.13209": {"publish_time": "2022-03-24", "title": "Direct parsing to sentiment graphs", "author": "David Samuel et.al.", "abstract": "This paper demonstrates how a graph-based semantic parser can be applied to the task of structured sentiment analysis, directly predicting sentiment graphs from text. We advance the state of the art on 4 out of 5 standard benchmark sets. We release the source code, models and predictions.", "paper_url": "http://arxiv.org/abs/2203.13209v1", "pdf_url": "http://arxiv.org/pdf/2203.13209v1", "repo_url": "https://github.com/jerbarnes/direct_parsing_to_sent_graph"}, "2203.13176": {"publish_time": "2022-03-24", "title": "Emergence of hierarchical reference systems in multi-agent communication", "author": "Xenia Ohmer et.al.", "abstract": "In natural language, referencing objects at different levels of specificity is a fundamental pragmatic mechanism for efficient communication in context. We develop a novel communication game, the hierarchical reference game, to study the emergence of such reference systems in artificial agents. We consider a simplified world, in which concepts are abstractions over a set of primitive attributes (e.g., color, style, shape). Depending on how many attributes are combined, concepts are more general (\"circle\") or more specific (\"red dotted circle\"). Based on the context, the agents have to communicate at different levels of this hierarchy. Our results show, that the agents learn to play the game successfully and can even generalize to novel concepts. To achieve abstraction, they use implicit (omitting irrelevant information) and explicit (indicating that attributes are irrelevant) strategies. In addition, the compositional structure underlying the concept hierarchy is reflected in the emergent protocols, indicating that the need to develop hierarchical reference systems supports the emergence of compositionality.", "paper_url": "http://arxiv.org/abs/2203.13176v1", "pdf_url": "http://arxiv.org/pdf/2203.13176v1", "repo_url": "https://github.com/xeniaohmer/hierarchical_reference_game"}, "2203.13778": {"publish_time": "2022-03-25", "title": "L3Cube-MahaHate: A Tweet-based Marathi Hate Speech Detection Dataset and BERT models", "author": "Abhishek Velankar et.al.", "abstract": "Social media platforms are used by a large number of people prominently to express their thoughts and opinions. However, these platforms have contributed to a substantial amount of hateful and abusive content as well. Therefore, it is important to curb the spread of hate speech on these platforms. In India, Marathi is one of the most popular languages used by a wide audience. In this work, we present L3Cube-MahaHate, the first major Hate Speech Dataset in Marathi. The dataset is curated from Twitter, annotated manually. Our dataset consists of over 25000 distinct tweets labeled into four major classes i.e hate, offensive, profane, and not. We present the approaches used for collecting and annotating the data and the challenges faced during the process. Finally, we present baseline classification results using deep learning models based on CNN, LSTM, and Transformers. We explore mono-lingual and multi-lingual variants of BERT like MahaBERT, IndicBERT, mBERT, and xlm-RoBERTa and show that mono-lingual models perform better than their multi-lingual counterparts. The MahaBERT model provides the best results on L3Cube-MahaHate Corpus. The data and models are available at https://github.com/l3cube-pune/MarathiNLP .", "paper_url": "http://arxiv.org/abs/2203.13778v1", "pdf_url": "http://arxiv.org/pdf/2203.13778v1", "repo_url": null}, "2203.13722": {"publish_time": "2022-03-25", "title": "Probing Pre-Trained Language Models for Cross-Cultural Differences in Values", "author": "Arnav Arora et.al.", "abstract": "Language embeds information about social, cultural, and political values people hold. Prior work has explored social and potentially harmful biases encoded in Pre-Trained Language models (PTLMs). However, there has been no systematic study investigating how values embedded in these models vary across cultures. In this paper, we introduce probes to study which values across cultures are embedded in these models, and whether they align with existing theories and cross-cultural value surveys. We find that PTLMs capture differences in values across cultures, but those only weakly align with established value surveys. We discuss implications of using mis-aligned models in cross-cultural settings, as well as ways of aligning PTLMs with value surveys.", "paper_url": "http://arxiv.org/abs/2203.13722v1", "pdf_url": "http://arxiv.org/pdf/2203.13722v1", "repo_url": null}, "2203.13696": {"publish_time": "2022-03-25", "title": "Speech-enhanced and Noise-aware Networks for Robust Speech Recognition", "author": "Hung-Shin Lee et.al.", "abstract": "Compensation for channel mismatch and noise interference is essential for robust automatic speech recognition. Enhanced speech has been introduced into the multi-condition training of acoustic models to improve their generalization ability. In this paper, a noise-aware training framework based on two cascaded neural structures is proposed to jointly optimize speech enhancement and speech recognition. The feature enhancement module is composed of a multi-task autoencoder, where noisy speech is decomposed into clean speech and noise. By concatenating its enhanced, noise-aware, and noisy features for each frame, the acoustic-modeling module maps each feature-augmented frame into a triphone state by optimizing the lattice-free maximum mutual information and cross entropy between the predicted and actual state sequences. On top of the factorized time delay neural network (TDNN-F) and its convolutional variant (CNN-TDNNF), both with SpecAug, the two proposed systems achieve word error rate (WER) of 3.90% and 3.55%, respectively, on the Aurora-4 task. Compared with the best existing systems that use bigram and trigram language models for decoding, the proposed CNN-TDNNF-based system achieves a relative WER reduction of 15.20% and 33.53%, respectively. In addition, the proposed CNN-TDNNF-based system also outperforms the baseline CNN-TDNNF system on the AMI task.", "paper_url": "http://arxiv.org/abs/2203.13696v1", "pdf_url": "http://arxiv.org/pdf/2203.13696v1", "repo_url": null}, "2203.13693": {"publish_time": "2022-03-25", "title": "UKP-SQUARE: An Online Platform for Question Answering Research", "author": "Tim Baumg\u00e4rtner et.al.", "abstract": "Recent advances in NLP and information retrieval have given rise to a diverse set of question answering tasks that are of different formats (e.g., extractive, abstractive), require different model architectures (e.g., generative, discriminative), and setups (e.g., with or without retrieval). Despite having a large number of powerful, specialized QA pipelines (which we refer to as Skills) that consider a single domain, model or setup, there exists no framework where users can easily explore and compare such pipelines and can extend them according to their needs. To address this issue, we present UKP-SQUARE, an extensible online QA platform for researchers which allows users to query and analyze a large collection of modern Skills via a user-friendly web interface and integrated behavioural tests. In addition, QA researchers can develop, manage, and share their custom Skills using our microservices that support a wide range of models (Transformers, Adapters, ONNX), datastores and retrieval techniques (e.g., sparse and dense). UKP-SQUARE is available on https://square.ukp-lab.de.", "paper_url": "http://arxiv.org/abs/2203.13693v1", "pdf_url": "http://arxiv.org/pdf/2203.13693v1", "repo_url": null}, "2203.13687": {"publish_time": "2022-03-25", "title": "Chain-based Discriminative Autoencoders for Speech Recognition", "author": "Hung-Shin Lee et.al.", "abstract": "In our previous work, we proposed a discriminative autoencoder (DcAE) for speech recognition. DcAE combines two training schemes into one. First, since DcAE aims to learn encoder-decoder mappings, the squared error between the reconstructed speech and the input speech is minimized. Second, in the code layer, frame-based phonetic embeddings are obtained by minimizing the categorical cross-entropy between ground truth labels and predicted triphone-state scores. DcAE is developed based on the Kaldi toolkit by treating various TDNN models as encoders. In this paper, we further propose three new versions of DcAE. First, a new objective function that considers both categorical cross-entropy and mutual information between ground truth and predicted triphone-state sequences is used. The resulting DcAE is called a chain-based DcAE (c-DcAE). For application to robust speech recognition, we further extend c-DcAE to hierarchical and parallel structures, resulting in hc-DcAE and pc-DcAE. In these two models, both the error between the reconstructed noisy speech and the input noisy speech and the error between the enhanced speech and the reference clean speech are taken into the objective function. Experimental results on the WSJ and Aurora-4 corpora show that our DcAE models outperform baseline systems.", "paper_url": "http://arxiv.org/abs/2203.13687v1", "pdf_url": "http://arxiv.org/pdf/2203.13687v1", "repo_url": null}, "2203.14936": {"publish_time": "2022-03-28", "title": "FedVLN: Privacy-preserving Federated Vision-and-Language Navigation", "author": "Kaiwen Zhou et.al.", "abstract": "Data privacy is a central problem for embodied agents that can perceive the environment, communicate with humans, and act in the real world. While helping humans complete tasks, the agent may observe and process sensitive information of users, such as house environments, human activities, etc. In this work, we introduce privacy-preserving embodied agent learning for the task of Vision-and-Language Navigation (VLN), where an embodied agent navigates house environments by following natural language instructions. We view each house environment as a local client, which shares nothing other than local updates with the cloud server and other clients, and propose a novel federated vision-and-language navigation (FedVLN) framework to protect data privacy during both training and pre-exploration. Particularly, we propose a decentralized training strategy to limit the data of each client to its local model training and a federated pre-exploration method to do partial model aggregation to improve model generalizability to unseen environments. Extensive results on R2R and RxR datasets show that under our FedVLN framework, decentralized VLN models achieve comparable results with centralized training while protecting seen environment privacy, and federated pre-exploration significantly outperforms centralized pre-exploration while preserving unseen environment privacy.", "paper_url": "http://arxiv.org/abs/2203.14936v1", "pdf_url": "http://arxiv.org/pdf/2203.14936v1", "repo_url": null}, "2203.14920": {"publish_time": "2022-03-28", "title": "UTSA NLP at SemEval-2022 Task 4: An Exploration of Simple Ensembles of Transformers, Convolutional, and Recurrent Neural Networks", "author": "Xingmeng Zhao et.al.", "abstract": "The act of appearing kind or helpful via the use of but having a feeling of superiority condescending and patronizing language can have have serious mental health implications to those that experience it. Thus, detecting this condescending and patronizing language online can be useful for online moderation systems. Thus, in this manuscript, we describe the system developed by Team UTSA SemEval-2022 Task 4, Detecting Patronizing and Condescending Language. Our approach explores the use of several deep learning architectures including RoBERTa, convolutions neural networks, and Bidirectional Long Short-Term Memory Networks. Furthermore, we explore simple and effective methods to create ensembles of neural network models. Overall, we experimented with several ensemble models and found that the a simple combination of five RoBERTa models achieved an F-score of .6441 on the development dataset and .5745 on the final test dataset. Finally, we also performed a comprehensive error analysis to better understand the limitations of the model and provide ideas for further research.", "paper_url": "http://arxiv.org/abs/2203.14920v1", "pdf_url": "http://arxiv.org/pdf/2203.14920v1", "repo_url": null}, "2203.14876": {"publish_time": "2022-03-28", "title": "Finnish Parliament ASR corpus - Analysis, benchmarks and statistics", "author": "Anja Virkkunen et.al.", "abstract": "Public sources like parliament meeting recordings and transcripts provide ever-growing material for the training and evaluation of automatic speech recognition (ASR) systems. In this paper, we publish and analyse the Finnish parliament ASR corpus, the largest publicly available collection of manually transcribed speech data for Finnish with over 3000 hours of speech and 449 speakers for which it provides rich demographic metadata. This corpus builds on earlier initial work, and as a result the corpus has a natural split into two training subsets from two periods of time. Similarly, there are two official, corrected test sets covering different times, setting an ASR task with longitudinal distribution-shift characteristics. An official development set is also provided. We develop a complete Kaldi-based data preparation pipeline, and hidden Markov model (HMM), hybrid deep neural network (HMM-DNN) and attention-based encoder-decoder (AED) ASR recipes. We set benchmarks on the official test sets, as well as multiple other recently used test sets. Both temporal corpus subsets are already large, and we observe that beyond their scale, ASR performance on the official test sets plateaus, whereas other domains benefit from added data. The HMM-DNN and AED approaches are compared in a carefully matched equal data setting, with the HMM-DNN system consistently performing better. Finally, the variation of the ASR accuracy is compared between the speaker categories available in the parliament metadata to detect potential biases based on factors such as gender, age, and education.", "paper_url": "http://arxiv.org/abs/2203.14876v1", "pdf_url": "http://arxiv.org/pdf/2203.14876v1", "repo_url": null}, "2203.14835": {"publish_time": "2022-03-28", "title": "Multilingual Simultaneous Speech Translation", "author": "Shashank Subramanya et.al.", "abstract": "Applications designed for simultaneous speech translation during events such as conferences or meetings need to balance quality and lag while displaying translated text to deliver a good user experience. One common approach to building online spoken language translation systems is by leveraging models built for offline speech translation. Based on a technique to adapt end-to-end monolingual models, we investigate multilingual models and different architectures (end-to-end and cascade) on the ability to perform online speech translation. On the multilingual TEDx corpus, we show that the approach generalizes to different architectures. We see similar gains in latency reduction (40% relative) across languages and architectures. However, the end-to-end architecture leads to smaller translation quality losses after adapting to the online model. Furthermore, the approach even scales to zero-shot directions.", "paper_url": "http://arxiv.org/abs/2203.14835v1", "pdf_url": "http://arxiv.org/pdf/2203.14835v1", "repo_url": null}, "2203.14757": {"publish_time": "2022-03-28", "title": "STUDIES: Corpus of Japanese Empathetic Dialogue Speech Towards Friendly Voice Agent", "author": "Yuki Saito et.al.", "abstract": "We present STUDIES, a new speech corpus for developing a voice agent that can speak in a friendly manner. Humans naturally control their speech prosody to empathize with each other. By incorporating this \"empathetic dialogue\" behavior into a spoken dialogue system, we can develop a voice agent that can respond to a user more naturally. We designed the STUDIES corpus to include a speaker who speaks with empathy for the interlocutor's emotion explicitly. We describe our methodology to construct an empathetic dialogue speech corpus and report the analysis results of the STUDIES corpus. We conducted a text-to-speech experiment to initially investigate how we can develop more natural voice agent that can tune its speaking style corresponding to the interlocutor's emotion. The results show that the use of interlocutor's emotion label and conversational context embedding can produce speech with the same degree of naturalness as that synthesized by using the agent's emotion label. Our project page of the STUDIES corpus is http://sython.org/Corpus/STUDIES.", "paper_url": "http://arxiv.org/abs/2203.14757v1", "pdf_url": "http://arxiv.org/pdf/2203.14757v1", "repo_url": null}, "2203.15773": {"publish_time": "2022-03-29", "title": "Streaming parallel transducer beam search with fast-slow cascaded encoders", "author": "Jay Mahadeokar et.al.", "abstract": "Streaming ASR with strict latency constraints is required in many speech recognition applications. In order to achieve the required latency, streaming ASR models sacrifice accuracy compared to non-streaming ASR models due to lack of future input context. Previous research has shown that streaming and non-streaming ASR for RNN Transducers can be unified by cascading causal and non-causal encoders. This work improves upon this cascaded encoders framework by leveraging two streaming non-causal encoders with variable input context sizes that can produce outputs at different audio intervals (e.g. fast and slow). We propose a novel parallel time-synchronous beam search algorithm for transducers that decodes from fast-slow encoders, where the slow encoder corrects the mistakes generated from the fast encoder. The proposed algorithm, achieves up to 20% WER reduction with a slight increase in token emission delays on the public Librispeech dataset and in-house datasets. We also explore techniques to reduce the computation by distributing processing between the fast and slow encoders. Lastly, we explore sharing the parameters in the fast encoder to reduce the memory footprint. This enables low latency processing on edge devices with low computation cost and a low memory footprint.", "paper_url": "http://arxiv.org/abs/2203.15773v1", "pdf_url": "http://arxiv.org/pdf/2203.15773v1", "repo_url": null}, "2203.15754": {"publish_time": "2022-03-29", "title": "Evaluating Prompts Across Multiple Choice Tasks In a Zero-Shot Setting", "author": "Gabriel Orlanski et.al.", "abstract": "Large language models have shown that impressive zero-shot performance can be achieved through natural language prompts (Radford et al., 2019; Brown et al., 2020; Sanh et al., 2021). Creating an effective prompt, however, requires significant trial and error. That \\textit{prompts} the question: how do the qualities of a prompt effects its performance? To this end, we collect and standardize prompts from a diverse range of tasks for use with tasks they were not designed for. We then evaluate these prompts across fixed multiple choice datasets for a quantitative analysis of how certain attributes of a prompt affect performance. We find that including the choices and using prompts not used during pre-training provide significant improvements. All experiments and code can be found https://github.com/gabeorlanski/zero-shot-cross-task.", "paper_url": "http://arxiv.org/abs/2203.15754v1", "pdf_url": "http://arxiv.org/pdf/2203.15754v1", "repo_url": "https://github.com/gabeorlanski/zero-shot-cross-task"}, "2203.15721": {"publish_time": "2022-03-29", "title": "On Decoding Strategies for Neural Text Generators", "author": "Gian Wiher et.al.", "abstract": "When generating text from probabilistic models, the chosen decoding strategy has a profound effect on the resulting text. Yet the properties elicited by various decoding strategies do not always transfer across natural language generation tasks. For example, while mode-seeking methods like beam search perform remarkably well for machine translation, they have been observed to lead to incoherent and repetitive text in story generation. Despite such observations, the effectiveness of decoding strategies is often assessed with respect to only a single task. This work -- in contrast -- provides a comprehensive analysis of the interaction between language generation tasks and decoding strategies. Specifically, we measure changes in attributes of generated text as a function of both decoding strategy and task using human and automatic evaluation. Our results reveal both previously-observed and surprising findings. For example, the nature of the diversity-quality trade-off in language generation is very task-specific; the length bias often attributed to beam search is not constant across tasks.", "paper_url": "http://arxiv.org/abs/2203.15721v1", "pdf_url": "http://arxiv.org/pdf/2203.15721v1", "repo_url": null}, "2203.15686": {"publish_time": "2022-03-29", "title": "Forecasting with Economic News", "author": "Luca Barbaglia et.al.", "abstract": "The goal of this paper is to evaluate the informational content of sentiment extracted from news articles about the state of the economy. We propose a fine-grained aspect-based sentiment analysis that has two main characteristics: 1) we consider only the text in the article that is semantically dependent on a term of interest (aspect-based) and, 2) assign a sentiment score to each word based on a dictionary that we develop for applications in economics and finance (fine-grained). Our data set includes six large US newspapers, for a total of over 6.6 million articles and 4.2 billion words. Our findings suggest that several measures of economic sentiment track closely business cycle fluctuations and that they are relevant predictors for four major macroeconomic variables. We find that there are significant improvements in forecasting when sentiment is considered along with macroeconomic factors. In addition, we also find that sentiment matters to explains the tails of the probability distribution across several macroeconomic variables.", "paper_url": "http://arxiv.org/abs/2203.15686v1", "pdf_url": "http://arxiv.org/pdf/2203.15686v1", "repo_url": null}, "2203.15685": {"publish_time": "2022-03-29", "title": "EnvEdit: Environment Editing for Vision-and-Language Navigation", "author": "Jialu Li et.al.", "abstract": "In Vision-and-Language Navigation (VLN), an agent needs to navigate through the environment based on natural language instructions. Due to limited available data for agent training and finite diversity in navigation environments, it is challenging for the agent to generalize to new, unseen environments. To address this problem, we propose EnvEdit, a data augmentation method that creates new environments by editing existing environments, which are used to train a more generalizable agent. Our augmented environments can differ from the seen environments in three diverse aspects: style, object appearance, and object classes. Training on these edit-augmented environments prevents the agent from overfitting to existing environments and helps generalize better to new, unseen environments. Empirically, on both the Room-to-Room and the multi-lingual Room-Across-Room datasets, we show that our proposed EnvEdit method gets significant improvements in all metrics on both pre-trained and non-pre-trained VLN agents, and achieves the new state-of-the-art on the test leaderboard. We further ensemble the VLN agents augmented on different edited environments and show that these edit methods are complementary. Code and data are available at https://github.com/jialuli-luka/EnvEdit", "paper_url": "http://arxiv.org/abs/2203.15685v1", "pdf_url": "http://arxiv.org/pdf/2203.15685v1", "repo_url": "https://github.com/jialuli-luka/envedit"}, "2203.16512": {"publish_time": "2022-03-30", "title": "Vakyansh: ASR Toolkit for Low Resource Indic languages", "author": "Harveen Singh Chadha et.al.", "abstract": "We present Vakyansh, an end to end toolkit for Speech Recognition in Indic languages. India is home to almost 121 languages and around 125 crore speakers. Yet most of the languages are low resource in terms of data and pretrained models. Through Vakyansh, we introduce automatic data pipelines for data creation, model training, model evaluation and deployment. We create 14,000 hours of speech data in 23 Indic languages and train wav2vec 2.0 based pretrained models. These pretrained models are then finetuned to create state of the art speech recognition models for 18 Indic languages which are followed by language models and punctuation restoration models. We open source all these resources with a mission that this will inspire the speech community to develop speech first applications using our ASR models in Indic languages.", "paper_url": "http://arxiv.org/abs/2203.16512v1", "pdf_url": "http://arxiv.org/pdf/2203.16512v1", "repo_url": null}, "2203.16502": {"publish_time": "2022-03-30", "title": "Generative Spoken Dialogue Language Modeling", "author": "Tu Anh Nguyen et.al.", "abstract": "We introduce dGSLM, the first \"textless\" model able to generate audio samples of naturalistic spoken dialogues. It uses recent work on unsupervised spoken unit discovery coupled with a dual-tower transformer architecture with cross-attention trained on 2000 hours of two-channel raw conversational audio (Fisher dataset) without any text or labels. It is able to generate speech, laughter and other paralinguistic signals in the two channels simultaneously and reproduces naturalistic turn taking. Generation samples can be found at: https://speechbot.github.io/dgslm.", "paper_url": "http://arxiv.org/abs/2203.16502v1", "pdf_url": "http://arxiv.org/pdf/2203.16502v1", "repo_url": null}, "2203.16487": {"publish_time": "2022-03-30", "title": "Lossless Speedup of Autoregressive Translation with Generalized Aggressive Decoding", "author": "Heming Xia et.al.", "abstract": "In this paper, we propose Generalized Aggressive Decoding (GAD) -- a novel approach to accelerating autoregressive translation with no quality loss, through the collaboration of autoregressive and non-autoregressive translation (NAT) of the Transformer. At each decoding iteration, GAD aggressively decodes a number of tokens in parallel as a draft through NAT and then verifies them in the autoregressive manner, where only the tokens that pass the verification are kept as decoded tokens. GAD can achieve the same performance as autoregressive translation but perform much more efficiently because both NAT drafting and autoregressive verification are fast due to parallel computing. We conduct experiments in the WMT14 English-German translation task and confirm that the vanilla GAD yields exactly the same results as greedy decoding with about 3x speedup, and that its variant (GAD++) with an advanced verification strategy not only outperforms the greedy translation and even achieves the comparable translation quality with the beam search result, but also further improves the decoding speed, resulting in an around 5x speedup over autoregressive translation.", "paper_url": "http://arxiv.org/abs/2203.16487v1", "pdf_url": "http://arxiv.org/pdf/2203.16487v1", "repo_url": null}, "2203.16474": {"publish_time": "2022-03-30", "title": "Zero Shot Crosslingual Eye-Tracking Data Prediction using Multilingual Transformer Models", "author": "Harshvardhan Srivastava et.al.", "abstract": "Eye tracking data during reading is a useful source of information to understand the cognitive processes that take place during language comprehension processes. Different languages account for different brain triggers , however there seems to be some uniform indicators. In this paper, we describe our submission to the CMCL 2022 shared task on predicting human reading patterns for multi-lingual dataset. Our model uses text representations from transformers and some hand engineered features with a regression layer on top to predict statistical measures of mean and standard deviation for 2 main eye-tracking features. We train an end to end model to extract meaningful information from different languages and test our model on two seperate datasets. We compare different transformer models and show ablation studies affecting model performance. Our final submission ranked 4th place for SubTask-1 and 1st place for SubTask-2 for the shared task.", "paper_url": "http://arxiv.org/abs/2203.16474v1", "pdf_url": "http://arxiv.org/pdf/2203.16474v1", "repo_url": null}, "2203.16434": {"publish_time": "2022-03-30", "title": "TubeDETR: Spatio-Temporal Video Grounding with Transformers", "author": "Antoine Yang et.al.", "abstract": "We consider the problem of localizing a spatio-temporal tube in a video corresponding to a given text query. This is a challenging task that requires the joint and efficient modeling of temporal, spatial and multi-modal interactions. To address this task, we propose TubeDETR, a transformer-based architecture inspired by the recent success of such models for text-conditioned object detection. Our model notably includes: (i) an efficient video and text encoder that models spatial multi-modal interactions over sparsely sampled frames and (ii) a space-time decoder that jointly performs spatio-temporal localization. We demonstrate the advantage of our proposed components through an extensive ablation study. We also evaluate our full approach on the spatio-temporal video grounding task and demonstrate improvements over the state of the art on the challenging VidSTG and HC-STVG benchmarks. Code and trained models are publicly available at https://antoyang.github.io/tubedetr.html.", "paper_url": "http://arxiv.org/abs/2203.16434v1", "pdf_url": "http://arxiv.org/pdf/2203.16434v1", "repo_url": "https://github.com/antoyang/TubeDETR"}, "2203.17225": {"publish_time": "2022-03-31", "title": "A Baseline Readability Model for Cebuano", "author": "Lloyd Lois Antonie Reyes et.al.", "abstract": "In this study, we developed the first baseline readability model for the Cebuano language. Cebuano is the second most-used native language in the Philippines with about 27.5 million speakers. As the baseline, we extracted traditional or surface-based features, syllable patterns based from Cebuano's documented orthography, and neural embeddings from the multilingual BERT model. Results show that the use of the first two handcrafted linguistic features obtained the best performance trained on an optimized Random Forest model with approximately 84\\% across all metrics. The feature sets and algorithm used also is similar to previous results in readability assessment for the Filipino language showing potential of crosslingual application. To encourage more work for readability assessment in Philippine languages such as Cebuano, we open-sourced both code and data.", "paper_url": "http://arxiv.org/abs/2203.17225v1", "pdf_url": "http://arxiv.org/pdf/2203.17225v1", "repo_url": "https://github.com/imperialite/cebuano-readability"}, "2203.17217": {"publish_time": "2022-03-31", "title": "On the probability-quality paradox in language generation", "author": "Clara Meister et.al.", "abstract": "When generating natural language from neural probabilistic models, high probability does not always coincide with high quality: It has often been observed that mode-seeking decoding methods, i.e., those that produce high-probability text under the model, lead to unnatural language. On the other hand, the lower-probability text generated by stochastic methods is perceived as more human-like. In this note, we offer an explanation for this phenomenon by analyzing language generation through an information-theoretic lens. Specifically, we posit that human-like language should contain an amount of information (quantified as negative log-probability) that is close to the entropy of the distribution over natural strings. Further, we posit that language with substantially more (or less) information is undesirable. We provide preliminary empirical evidence in favor of this hypothesis; quality ratings of both human and machine-generated text -- covering multiple tasks and common decoding strategies -- suggest high-quality text has an information content significantly closer to the entropy than we would expect by chance.", "paper_url": "http://arxiv.org/abs/2203.17217v1", "pdf_url": "http://arxiv.org/pdf/2203.17217v1", "repo_url": null}, "2203.17213": {"publish_time": "2022-03-31", "title": "Analyzing Wrap-Up Effects through an Information-Theoretic Lens", "author": "Clara Meister et.al.", "abstract": "Numerous analyses of reading time (RT) data have been implemented -- all in an effort to better understand the cognitive processes driving reading comprehension. However, data measured on words at the end of a sentence -- or even at the end of a clause -- is often omitted due to the confounding factors introduced by so-called \"wrap-up effects,\" which manifests as a skewed distribution of RTs for these words. Consequently, the understanding of the cognitive processes that might be involved in these wrap-up effects is limited. In this work, we attempt to learn more about these processes by examining the relationship between wrap-up effects and information-theoretic quantities, such as word and context surprisals. We find that the distribution of information in prior contexts is often predictive of sentence- and clause-final RTs (while not of sentence-medial RTs). This lends support to several prior hypotheses about the processes involved in wrap-up effects.", "paper_url": "http://arxiv.org/abs/2203.17213v1", "pdf_url": "http://arxiv.org/pdf/2203.17213v1", "repo_url": null}, "2203.17196": {"publish_time": "2022-03-31", "title": "CatIss: An Intelligent Tool for Categorizing Issues Reports using Transformers", "author": "Maliheh Izadi et.al.", "abstract": "Users use Issue Tracking Systems to keep track and manage issue reports in their repositories. An issue is a rich source of software information that contains different reports including a problem, a request for new features, or merely a question about the software product. As the number of these issues increases, it becomes harder to manage them manually. Thus, automatic approaches are proposed to help facilitate the management of issue reports.   This paper describes CatIss, an automatic CATegorizer of ISSue reports which is built upon the Transformer-based pre-trained RoBERTa model. CatIss classifies issue reports into three main categories of Bug reports, Enhancement/feature requests, and Questions. First, the datasets provided for the NLBSE tool competition are cleaned and preprocessed. Then, the pre-trained RoBERTa model is fine-tuned on the preprocessed dataset. Evaluating CatIss on about 80 thousand issue reports from GitHub, indicates that it performs very well surpassing the competition baseline, TicketTagger, and achieving 87.2% F1-score (micro average). Additionally, as CatIss is trained on a wide set of repositories, it is a generic prediction model, hence applicable for any unseen software project or projects with little historical data. Scripts for cleaning the datasets, training CatIss, and evaluating the model are publicly available.", "paper_url": "http://arxiv.org/abs/2203.17196v1", "pdf_url": "http://arxiv.org/pdf/2203.17196v1", "repo_url": "https://github.com/malihehizadi/catiss"}, "2203.17190": {"publish_time": "2022-03-31", "title": "Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech", "author": "Guangyan Zhang et.al.", "abstract": "Recently, leveraging BERT pre-training to improve the phoneme encoder in text to speech (TTS) has drawn increasing attention. However, the works apply pre-training with character-based units to enhance the TTS phoneme encoder, which is inconsistent with the TTS fine-tuning that takes phonemes as input. Pre-training only with phonemes as input can alleviate the input mismatch but lack the ability to model rich representations and semantic information due to limited phoneme vocabulary. In this paper, we propose MixedPhoneme BERT, a novel variant of the BERT model that uses mixed phoneme and sup-phoneme representations to enhance the learning capability. Specifically, we merge the adjacent phonemes into sup-phonemes and combine the phoneme sequence and the merged sup-phoneme sequence as the model input, which can enhance the model capacity to learn rich contextual representations. Experiment results demonstrate that our proposed Mixed-Phoneme BERT significantly improves the TTS performance with 0.30 CMOS gain compared with the FastSpeech 2 baseline. The Mixed-Phoneme BERT achieves 3x inference speedup and similar voice quality to the previous TTS pre-trained model PnG BERT", "paper_url": "http://arxiv.org/abs/2203.17190v1", "pdf_url": "http://arxiv.org/pdf/2203.17190v1", "repo_url": null}, "2204.00598": {"publish_time": "2022-04-01", "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language", "author": "Andy Zeng et.al.", "abstract": "Large foundation models can exhibit unique capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g. from spreadsheets, to SAT questions). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this model diversity is symbiotic, and can be leveraged to build AI systems with structured Socratic dialogue -- in which new multimodal tasks are formulated as a guided language-based exchange between different pre-existing foundation models, without additional finetuning. In the context of egocentric perception, we present a case study of Socratic Models (SMs) that can provide meaningful results for complex tasks such as generating free-form answers to contextual questions about egocentric video, by formulating video Q&A as short story Q&A, i.e. summarizing the video into a short story, then answering questions about it. Additionally, SMs can generate captions for Internet images, and are competitive with state-of-the-art on zero-shot video-to-text retrieval with 42.8 R@1 on MSR-VTT 1k-A. SMs demonstrate how to compose foundation models zero-shot to capture new multimodal functionalities, without domain-specific data collection. Prototypes are available at socraticmodels.github.io.", "paper_url": "http://arxiv.org/abs/2204.00598v1", "pdf_url": "http://arxiv.org/pdf/2204.00598v1", "repo_url": null}, "2204.00558": {"publish_time": "2022-04-01", "title": "Multi-task RNN-T with Semantic Decoder for Streamable Spoken Language Understanding", "author": "Xuandi Fu et.al.", "abstract": "End-to-end Spoken Language Understanding (E2E SLU) has attracted increasing interest due to its advantages of joint optimization and low latency when compared to traditionally cascaded pipelines. Existing E2E SLU models usually follow a two-stage configuration where an Automatic Speech Recognition (ASR) network first predicts a transcript which is then passed to a Natural Language Understanding (NLU) module through an interface to infer semantic labels, such as intent and slot tags. This design, however, does not consider the NLU posterior while making transcript predictions, nor correct the NLU prediction error immediately by considering the previously predicted word-pieces. In addition, the NLU model in the two-stage system is not streamable, as it must wait for the audio segments to complete processing, which ultimately impacts the latency of the SLU system. In this work, we propose a streamable multi-task semantic transducer model to address these considerations. Our proposed architecture predicts ASR and NLU labels auto-regressively and uses a semantic decoder to ingest both previously predicted word-pieces and slot tags while aggregating them through a fusion network. Using an industry scale SLU and a public FSC dataset, we show the proposed model outperforms the two-stage E2E SLU model for both ASR and NLU metrics.", "paper_url": "http://arxiv.org/abs/2204.00558v1", "pdf_url": "http://arxiv.org/pdf/2204.00558v1", "repo_url": null}, "2204.00556": {"publish_time": "2022-04-01", "title": "Nowruz at SemEval-2022 Task 7: Tackling Cloze Tests with Transformers and Ordinal Regression", "author": "Mohammadmahdi Nouriborji et.al.", "abstract": "This paper outlines the system using which team Nowruz participated in SemEval 2022 Task 7 Identifying Plausible Clarifications of Implicit and Underspecified Phrases for both subtasks A and B. Using a pre-trained transformer as a backbone, the model targeted the task of multi-task classification and ranking in the context of finding the best fillers for a cloze task related to instructional texts on the website Wikihow.   The system employed a combination of two ordinal regression components to tackle this task in a multi-task learning scenario. According to the official leaderboard of the shared task, this system was ranked 5th in the ranking and 7th in the classification subtasks out of 21 participating teams. With additional experiments, the models have since been further optimised.", "paper_url": "http://arxiv.org/abs/2204.00556v1", "pdf_url": "http://arxiv.org/pdf/2204.00556v1", "repo_url": null}, "2204.00548": {"publish_time": "2022-04-01", "title": "Unified and Effective Ensemble Knowledge Distillation", "author": "Chuhan Wu et.al.", "abstract": "Ensemble knowledge distillation can extract knowledge from multiple teacher models and encode it into a single student model. Many existing methods learn and distill the student model on labeled data only. However, the teacher models are usually learned on the same labeled data, and their predictions have high correlations with groudtruth labels. Thus, they cannot provide sufficient knowledge complementary to task labels for student teaching. Distilling on unseen unlabeled data has the potential to enhance the knowledge transfer from the teachers to the student. In this paper, we propose a unified and effective ensemble knowledge distillation method that distills a single student model from an ensemble of teacher models on both labeled and unlabeled data. Since different teachers may have diverse prediction correctness on the same sample, on labeled data we weight the predictions of different teachers according to their correctness. In addition, we weight the distillation loss based on the overall prediction correctness of the teacher ensemble to distill high-quality knowledge. On unlabeled data, there is no groundtruth to evaluate prediction correctness. Fortunately, the disagreement among teachers is an indication of sample hardness, and thereby we weight the distillation loss based on teachers' disagreement to emphasize knowledge distillation on important samples. Extensive experiments on four datasets show the effectiveness of our proposed ensemble distillation method.", "paper_url": "http://arxiv.org/abs/2204.00548v1", "pdf_url": "http://arxiv.org/pdf/2204.00548v1", "repo_url": null}, "2204.00545": {"publish_time": "2022-04-01", "title": "A Novel Multimodal Approach for Studying the Dynamics of Curiosity in Small Group Learning", "author": "Tanmay Sinha et.al.", "abstract": "Curiosity is a vital metacognitive skill in educational contexts, leading to creativity, and a love of learning. And while many school systems increasingly undercut curiosity by teaching to the test, teachers are increasingly interested in how to evoke curiosity in their students to prepare them for a world in which lifelong learning and reskilling will be more and more important. One aspect of curiosity that has received little attention, however, is the role of peers in eliciting curiosity. We present what we believe to be the first theoretical framework that articulates an integrated socio-cognitive account of curiosity that ties observable behaviors in peers to underlying curiosity states. We make a bipartite distinction between individual and interpersonal functions that contribute to curiosity, and multimodal behaviors that fulfill these functions. We validate the proposed framework by leveraging a longitudinal latent variable modeling approach. Findings confirm a positive predictive relationship between the latent variables of individual and interpersonal functions and curiosity, with the interpersonal functions exercising a comparatively stronger influence. Prominent behavioral realizations of these functions are also discovered in a data-driven manner. We instantiate the proposed theoretical framework in a set of strategies and tactics that can be incorporated into learning technologies to indicate, evoke, and scaffold curiosity. This work is a step towards designing learning technologies that can recognize and evoke moment-by-moment curiosity during learning in social contexts and towards a more complete multimodal learning analytics. The underlying rationale is applicable more generally for developing computer support for other metacognitive and socio-emotional skills.", "paper_url": "http://arxiv.org/abs/2204.00545v1", "pdf_url": "http://arxiv.org/pdf/2204.00545v1", "repo_url": null}, "2204.01691": {"publish_time": "2022-04-04", "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances", "author": "Michael Ahn et.al.", "abstract": "Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's \"hands and eyes,\" while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at https://say-can.github.io/", "paper_url": "http://arxiv.org/abs/2204.01691v1", "pdf_url": "http://arxiv.org/pdf/2204.01691v1", "repo_url": null}, "2204.01677": {"publish_time": "2022-04-04", "title": "Self-Supervised Speech Representations Preserve Speech Characteristics while Anonymizing Voices", "author": "Abner Hernandez et.al.", "abstract": "Collecting speech data is an important step in training speech recognition systems and other speech-based machine learning models. However, the issue of privacy protection is an increasing concern that must be addressed. The current study investigates the use of voice conversion as a method for anonymizing voices. In particular, we train several voice conversion models using self-supervised speech representations including Wav2Vec2.0, Hubert and UniSpeech. Converted voices retain a low word error rate within 1% of the original voice. Equal error rate increases from 1.52% to 46.24% on the LibriSpeech test set and from 3.75% to 45.84% on speakers from the VCTK corpus which signifies degraded performance on speaker verification. Lastly, we conduct experiments on dysarthric speech data to show that speech features relevant to articulation, prosody, phonation and phonology can be extracted from anonymized voices for discriminating between healthy and pathological speech.", "paper_url": "http://arxiv.org/abs/2204.01677v1", "pdf_url": "http://arxiv.org/pdf/2204.01677v1", "repo_url": null}, "2204.01670": {"publish_time": "2022-04-04", "title": "Cross-lingual Self-Supervised Speech Representations for Improved Dysarthric Speech Recognition", "author": "Abner Hernandez et.al.", "abstract": "State-of-the-art automatic speech recognition (ASR) systems perform well on healthy speech. However, the performance on impaired speech still remains an issue. The current study explores the usefulness of using Wav2Vec self-supervised speech representations as features for training an ASR system for dysarthric speech. Dysarthric speech recognition is particularly difficult as several aspects of speech such as articulation, prosody and phonation can be impaired. Specifically, we train an acoustic model with features extracted from Wav2Vec, Hubert, and the cross-lingual XLSR model. Results suggest that speech representations pretrained on large unlabelled data can improve word error rate (WER) performance. In particular, features from the multilingual model led to lower WERs than filterbanks (Fbank) or models trained on a single language. Improvements were observed in English speakers with cerebral palsy caused dysarthria (UASpeech corpus), Spanish speakers with Parkinsonian dysarthria (PC-GITA corpus) and Italian speakers with paralysis-based dysarthria (EasyCall corpus). Compared to using Fbank features, XLSR-based features reduced WERs by 6.8%, 22.0%, and 7.0% for the UASpeech, PC-GITA, and EasyCall corpus, respectively.", "paper_url": "http://arxiv.org/abs/2204.01670v1", "pdf_url": "http://arxiv.org/pdf/2204.01670v1", "repo_url": null}, "2204.01614": {"publish_time": "2022-04-04", "title": "MetaAID: A Flexible Framework for Developing Metaverse Applications via AI Technology and Human Editing", "author": "Hongyin Zhu et.al.", "abstract": "Achieving the expansion of domestic demand and the economic internal circulation requires balanced and coordinated support from multiple industries (domains) such as consumption, education, entertainment, engineering infrastructure, etc., which is indispensable for maintaining economic development. Metaverse applications may help with this task and can make many industries more interesting, more efficient, and provide a better user experience. The first challenge is that metaverse application development inevitably requires the support of various artificial intelligence (AI) technologies such as natural language processing (NLP), knowledge graph (KG), computer vision (CV), and machine learning (ML), etc. However, existing metaverse application development lacks a lightweight AI technology framework. This paper proposes a flexible metaverse AI technology framework metaAID that aims to support language and semantic technologies in the development of digital twins and virtual humans. The second challenge is that the development process of metaverse applications involves both technical development tasks and manual editing work, and often becomes a heavyweight multi-team collaboration project, not to mention the development of metaverse applications in multiple industries. Our framework summarizes common AI technologies and application development templates with common functional modules and interfaces. Based on this framework, we have designed 5 applications for 3 industries around the expansion of domestic demand and economic internal circulation. Experimental results show that our framework can support AI technologies when developing metaverse applications in different industries.", "paper_url": "http://arxiv.org/abs/2204.01614v1", "pdf_url": "http://arxiv.org/pdf/2204.01614v1", "repo_url": null}, "2204.01512": {"publish_time": "2022-04-04", "title": "LPAttack: A Feasible Annotation Scheme for Capturing Logic Pattern of Attacks in Arguments", "author": "Farjana Sultana Mim et.al.", "abstract": "In argumentative discourse, persuasion is often achieved by refuting or attacking others arguments. Attacking is not always straightforward and often comprise complex rhetorical moves such that arguers might agree with a logic of an argument while attacking another logic. Moreover, arguer might neither deny nor agree with any logics of an argument, instead ignore them and attack the main stance of the argument by providing new logics and presupposing that the new logics have more value or importance than the logics present in the attacked argument. However, no existing studies in the computational argumentation capture such complex rhetorical moves in attacks or the presuppositions or value judgements in them. In order to address this gap, we introduce LPAttack, a novel annotation scheme that captures the common modes and complex rhetorical moves in attacks along with the implicit presuppositions and value judgements in them. Our annotation study shows moderate inter-annotator agreement, indicating that human annotation for the proposed scheme is feasible. We publicly release our annotated corpus and the annotation guidelines.", "paper_url": "http://arxiv.org/abs/2204.01512v1", "pdf_url": "http://arxiv.org/pdf/2204.01512v1", "repo_url": null}, "2204.02380": {"publish_time": "2022-04-05", "title": "CLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations", "author": "Leonard Salewski et.al.", "abstract": "Providing explanations in the context of Visual Question Answering (VQA) presents a fundamental problem in machine learning. To obtain detailed insights into the process of generating natural language explanations for VQA, we introduce the large-scale CLEVR-X dataset that extends the CLEVR dataset with natural language explanations. For each image-question pair in the CLEVR dataset, CLEVR-X contains multiple structured textual explanations which are derived from the original scene graphs. By construction, the CLEVR-X explanations are correct and describe the reasoning and visual information that is necessary to answer a given question. We conducted a user study to confirm that the ground-truth explanations in our proposed dataset are indeed complete and relevant. We present baseline results for generating natural language explanations in the context of VQA using two state-of-the-art frameworks on the CLEVR-X dataset. Furthermore, we provide a detailed analysis of the explanation generation quality for different question and answer types. Additionally, we study the influence of using different numbers of ground-truth explanations on the convergence of natural language generation (NLG) metrics. The CLEVR-X dataset is publicly available at \\url{https://explainableml.github.io/CLEVR-X/}.", "paper_url": "http://arxiv.org/abs/2204.02380v1", "pdf_url": "http://arxiv.org/pdf/2204.02380v1", "repo_url": "https://github.com/explainableml/clevr-x"}, "2204.02363": {"publish_time": "2022-04-05", "title": "Towards Best Practices for Training Multilingual Dense Retrieval Models", "author": "Xinyu Zhang et.al.", "abstract": "Dense retrieval models using a transformer-based bi-encoder design have emerged as an active area of research. In this work, we focus on the task of monolingual retrieval in a variety of typologically diverse languages using one such design. Although recent work with multilingual transformers demonstrates that they exhibit strong cross-lingual generalization capabilities, there remain many open research questions, which we tackle here. Our study is organized as a \"best practices\" guide for training multilingual dense retrieval models, broken down into three main scenarios: where a multilingual transformer is available, but relevance judgments are not available in the language of interest; where both models and training data are available; and, where training data are available not but models. In considering these scenarios, we gain a better understanding of the role of multi-stage fine-tuning, the strength of cross-lingual transfer under various conditions, the usefulness of out-of-language data, and the advantages of multilingual vs. monolingual transformers. Our recommendations offer a guide for practitioners building search applications, particularly for low-resource languages, and while our work leaves open a number of research questions, we provide a solid foundation for future work.", "paper_url": "http://arxiv.org/abs/2204.02363v1", "pdf_url": "http://arxiv.org/pdf/2204.02363v1", "repo_url": null}, "2204.02329": {"publish_time": "2022-04-05", "title": "Can language models learn from explanations in context?", "author": "Andrew K. Lampinen et.al.", "abstract": "Large language models can perform new tasks by adapting to a few in-context examples. For humans, rapid learning from examples can benefit from explanations that connect examples to task principles. We therefore investigate whether explanations of few-shot examples can allow language models to adapt more effectively. We annotate a set of 40 challenging tasks from BIG-Bench with explanations of answers to a small subset of questions, as well as a variety of matched control explanations. We evaluate the effects of various zero-shot and few-shot prompts that include different types of explanations, instructions, and controls on the performance of a range of large language models. We analyze these results using statistical multilevel modeling techniques that account for the nested dependencies among conditions, tasks, prompts, and models. We find that explanations of examples can improve performance. Adding untuned explanations to a few-shot prompt offers a modest improvement in performance; about 1/3 the effect size of adding few-shot examples, but twice the effect size of task instructions. We then show that explanations tuned for performance on a small validation set offer substantially larger benefits; building a prompt by selecting examples and explanations together substantially improves performance over selecting examples alone. Hand-tuning explanations can substantially improve performance on challenging tasks. Furthermore, even untuned explanations outperform carefully matched controls, suggesting that the benefits are due to the link between an example and its explanation, rather than lower-level features of the language used. However, only large models can benefit from explanations. In summary, explanations can support the in-context learning abilities of large language models on", "paper_url": "http://arxiv.org/abs/2204.02329v1", "pdf_url": "http://arxiv.org/pdf/2204.02329v1", "repo_url": null}, "2204.02311": {"publish_time": "2022-04-05", "title": "PaLM: Scaling Language Modeling with Pathways", "author": "Aakanksha Chowdhery et.al.", "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.", "paper_url": "http://arxiv.org/abs/2204.02311v1", "pdf_url": "http://arxiv.org/pdf/2204.02311v1", "repo_url": null}, "2204.02292": {"publish_time": "2022-04-05", "title": "Parameter-Efficient Neural Reranking for Cross-Lingual and Multilingual Retrieval", "author": "Robert Litschko et.al.", "abstract": "State-of-the-art neural (re)rankers are notoriously data hungry which - given the lack of large-scale training data in languages other than English - makes them rarely used in multilingual and cross-lingual retrieval settings. Current approaches therefore typically transfer rankers trained on English data to other languages and cross-lingual setups by means of multilingual encoders: they fine-tune all the parameters of a pretrained massively multilingual Transformer (MMT, e.g., multilingual BERT) on English relevance judgments and then deploy it in the target language. In this work, we show that two parameter-efficient approaches to cross-lingual transfer, namely Sparse Fine-Tuning Masks (SFTMs) and Adapters, allow for a more lightweight and more effective zero-shot transfer to multilingual and cross-lingual retrieval tasks. We first train language adapters (or SFTMs) via Masked Language Modelling and then train retrieval (i.e., reranking) adapters (SFTMs) on top while keeping all other parameters fixed. At inference, this modular design allows us to compose the ranker by applying the task adapter (or SFTM) trained with source language data together with the language adapter (or SFTM) of a target language. Besides improved transfer performance, these two approaches offer faster ranker training, with only a fraction of parameters being updated compared to full MMT fine-tuning. We benchmark our models on the CLEF-2003 benchmark, showing that our parameter-efficient methods outperform standard zero-shot transfer with full MMT fine-tuning, while enabling modularity and reducing training times. Further, we show on the example of Swahili and Somali that, for low(er)-resource languages, our parameter-efficient neural re-rankers can improve the ranking of the competitive machine translation-based ranker.", "paper_url": "http://arxiv.org/abs/2204.02292v1", "pdf_url": "http://arxiv.org/pdf/2204.02292v1", "repo_url": null}, "2204.02967": {"publish_time": "2022-04-06", "title": "Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation", "author": "Sravya Popuri et.al.", "abstract": "Direct speech-to-speech translation (S2ST) models suffer from data scarcity issues as there exists little parallel S2ST data, compared to the amount of data available for conventional cascaded systems that consist of automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) synthesis. In this work, we explore self-supervised pre-training with unlabeled speech data and data augmentation to tackle this issue. We take advantage of a recently proposed speech-to-unit translation (S2UT) framework that encodes target speech into discrete representations, and transfer pre-training and efficient partial finetuning techniques that work well for speech-to-text translation (S2T) to the S2UT domain by studying both speech encoder and discrete unit decoder pre-training. Our experiments show that self-supervised pre-training consistently improves model performance compared with multitask learning with a BLEU gain of 4.3-12.0 under various data setups, and it can be further combined with data augmentation techniques that apply MT to create weakly supervised training data. Audio samples are available at: https://facebookresearch.github.io/speech_translation/enhanced_direct_s2st_units/index.html .", "paper_url": "http://arxiv.org/abs/2204.02967v1", "pdf_url": "http://arxiv.org/pdf/2204.02967v1", "repo_url": null}, "2204.02952": {"publish_time": "2022-04-06", "title": "Inducing Positive Perspectives with Text Reframing", "author": "Caleb Ziems et.al.", "abstract": "Sentiment transfer is one popular example of a text style transfer task, where the goal is to reverse the sentiment polarity of a text. With a sentiment reversal comes also a reversal in meaning. We introduce a different but related task called positive reframing in which we neutralize a negative point of view and generate a more positive perspective for the author without contradicting the original meaning. Our insistence on meaning preservation makes positive reframing a challenging and semantically rich task. To facilitate rapid progress, we introduce a large-scale benchmark, Positive Psychology Frames, with 8,349 sentence pairs and 12,755 structured annotations to explain positive reframing in terms of six theoretically-motivated reframing strategies. Then we evaluate a set of state-of-the-art text style transfer models, and conclude by discussing key challenges and directions for future work.", "paper_url": "http://arxiv.org/abs/2204.02952v1", "pdf_url": "http://arxiv.org/pdf/2204.02952v1", "repo_url": "https://github.com/gt-salt/positive-frames"}, "2204.02922": {"publish_time": "2022-04-06", "title": "Paying More Attention to Self-attention: Improving Pre-trained Language Models via Attention Guiding", "author": "Shanshan Wang et.al.", "abstract": "Pre-trained language models (PLM) have demonstrated their effectiveness for a broad range of information retrieval and natural language processing tasks. As the core part of PLM, multi-head self-attention is appealing for its ability to jointly attend to information from different positions. However, researchers have found that PLM always exhibits fixed attention patterns regardless of the input (e.g., excessively paying attention to [CLS] or [SEP]), which we argue might neglect important information in the other positions. In this work, we propose a simple yet effective attention guiding mechanism to improve the performance of PLM by encouraging attention towards the established goals. Specifically, we propose two kinds of attention guiding methods, i.e., map discrimination guiding (MDG) and attention pattern decorrelation guiding (PDG). The former definitely encourages the diversity among multiple self-attention heads to jointly attend to information from different representation subspaces, while the latter encourages self-attention to attend to as many different positions of the input as possible. We conduct experiments with multiple general pre-trained models (i.e., BERT, ALBERT, and Roberta) and domain-specific pre-trained models (i.e., BioBERT, ClinicalBERT, BlueBert, and SciBERT) on three benchmark datasets (i.e., MultiNLI, MedNLI, and Cross-genre-IR). Extensive experimental results demonstrate that our proposed MDG and PDG bring stable performance improvements on all datasets with high efficiency and low cost.", "paper_url": "http://arxiv.org/abs/2204.02922v1", "pdf_url": "http://arxiv.org/pdf/2204.02922v1", "repo_url": null}, "2204.02908": {"publish_time": "2022-04-06", "title": "Question Generation for Reading Comprehension Assessment by Modeling How and What to Ask", "author": "Bilal Ghanem et.al.", "abstract": "Reading is integral to everyday life, and yet learning to read is a struggle for many young learners. During lessons, teachers can use comprehension questions to increase engagement, test reading skills, and improve retention. Historically such questions were written by skilled teachers, but recently language models have been used to generate comprehension questions. However, many existing Question Generation (QG) systems focus on generating literal questions from the text, and have no way to control the type of the generated question. In this paper, we study QG for reading comprehension where inferential questions are critical and extractive techniques cannot be used. We propose a two-step model (HTA-WTA) that takes advantage of previous datasets, and can generate questions for a specific targeted comprehension skill. We propose a new reading comprehension dataset that contains questions annotated with story-based reading comprehension skills (SBRCS), allowing for a more complete reader assessment. Across several experiments, our results show that HTA-WTA outperforms multiple strong baselines on this new dataset. We show that the HTA-WTA model tests for strong SCRS by asking deep inferential questions.", "paper_url": "http://arxiv.org/abs/2204.02908v1", "pdf_url": "http://arxiv.org/pdf/2204.02908v1", "repo_url": null}, "2204.02906": {"publish_time": "2022-04-06", "title": "Knowledge Base Index Compression via Dimensionality and Precision Reduction", "author": "Vil\u00e9m Zouhar et.al.", "abstract": "Recently neural network based approaches to knowledge-intensive NLP tasks, such as question answering, started to rely heavily on the combination of neural retrievers and readers. Retrieval is typically performed over a large textual knowledge base (KB) which requires significant memory and compute resources, especially when scaled up. On HotpotQA we systematically investigate reducing the size of the KB index by means of dimensionality (sparse random projections, PCA, autoencoders) and numerical precision reduction.   Our results show that PCA is an easy solution that requires very little data and is only slightly worse than autoencoders, which are less stable. All methods are sensitive to pre- and post-processing and data should always be centered and normalized both before and after dimension reduction. Finally, we show that it is possible to combine PCA with using 1bit per dimension. Overall we achieve (1) 100$\\times$ compression with 75%, and (2) 24$\\times$ compression with 92% original retrieval performance.", "paper_url": "http://arxiv.org/abs/2204.02906v1", "pdf_url": "http://arxiv.org/pdf/2204.02906v1", "repo_url": null}, "2204.03637": {"publish_time": "2022-04-07", "title": "tmVar 3.0: an improved variant concept recognition and normalization tool", "author": "Chih-Hsuan Wei et.al.", "abstract": "Previous studies have shown that automated text-mining tools are becoming increasingly important for successfully unlocking variant information in scientific literature at large scale. Despite multiple attempts in the past, existing tools are still of limited recognition scope and precision. We propose tmVar 3.0: an improved variant recognition and normalization tool. Compared to its predecessors, tmVar 3.0 is able to recognize a wide spectrum of variant related entities (e.g., allele and copy number variants), and to group different variant mentions belonging to the same concept in an article for improved accuracy. Moreover, tmVar3 provides additional variant normalization options such as allele-specific identifiers from the ClinGen Allele Registry. tmVar3 exhibits a state-of-the-art performance with over 90% accuracy in F-measure in variant recognition and normalization, when evaluated on three independent benchmarking datasets. tmVar3 is freely available for download. We have also processed the entire PubMed and PMC with tmVar3 and released its annotations on our FTP. Availability: ftp://ftp.ncbi.nlm.nih.gov/pub/lu/tmVar3", "paper_url": "http://arxiv.org/abs/2204.03637v1", "pdf_url": "http://arxiv.org/pdf/2204.03637v1", "repo_url": null}, "2204.03619": {"publish_time": "2022-04-07", "title": "Modeling Label Correlations for Second-Order Semantic Dependency Parsing with Mean-Field Inference", "author": "Songlin Yang et.al.", "abstract": "Second-order semantic parsing with end-to-end mean-field inference has been shown good performance. In this work we aim to improve this method by modeling label correlations between adjacent arcs. However, direct modeling leads to memory explosion because second-order score tensors have sizes of $O(n^3L^2)$ ($n$ is the sentence length and $L$ is the number of labels), which is not affordable. To tackle this computational challenge, we leverage tensor decomposition techniques, and interestingly, we show that the large second-order score tensors have no need to be materialized during mean-field inference, thereby reducing the computational complexity from cubic to quadratic. We conduct experiments on SemEval 2015 Task 18 English datasets, showing the effectiveness of modeling label correlations. Our code is publicly available at https://github.com/sustcsonglin/mean-field-dep-parsing.", "paper_url": "http://arxiv.org/abs/2204.03619v1", "pdf_url": "http://arxiv.org/pdf/2204.03619v1", "repo_url": null}, "2204.03592": {"publish_time": "2022-04-07", "title": "Testing the limits of natural language models for predicting human language judgments", "author": "Tal Golan et.al.", "abstract": "Neural network language models can serve as computational hypotheses about how humans process language. We compared the model-human consistency of diverse language models using a novel experimental approach: controversial sentence pairs. For each controversial sentence pair, two language models disagree about which sentence is more likely to occur in natural text. Considering nine language models (including n-gram, recurrent neural networks, and transformer models), we created hundreds of such controversial sentence pairs by either selecting sentences from a corpus or synthetically optimizing sentence pairs to be highly controversial. Human subjects then provided judgments indicating for each pair which of the two sentences is more likely. Controversial sentence pairs proved highly effective at revealing model failures and identifying models that aligned most closely with human judgments. The most human-consistent model tested was GPT-2, although experiments also revealed significant shortcomings of its alignment with human perception.", "paper_url": "http://arxiv.org/abs/2204.03592v1", "pdf_url": "http://arxiv.org/pdf/2204.03592v1", "repo_url": "https://github.com/dpmlab/contstimlang"}, "2204.03574": {"publish_time": "2022-04-07", "title": "Learning to Compose Soft Prompts for Compositional Zero-Shot Learning", "author": "Nihal V. Nayak et.al.", "abstract": "We introduce compositional soft prompting (CSP), a parameter-efficient learning technique to improve the zero-shot compositionality of large-scale pretrained vision-language models (VLMs) without the overhead of fine-tuning the entire model. VLMs can represent arbitrary classes as natural language prompts in their flexible text encoders but they underperform state-of-the-art methods on compositional zero-shot benchmark tasks. To improve VLMs, we propose a novel form of soft prompting. We treat the attributes and objects that are composed to define classes as learnable tokens of vocabulary and tune them on multiple prompt compositions. During inference, we recompose the learned attribute-object vocabulary in new combinations and show that CSP outperforms the original VLM on benchmark datasets by an average of 14.7 percentage points of accuracy. CSP also achieves new state-of-the-art accuracies on two out of three benchmark datasets, while only fine-tuning a small number of parameters. Further, we show that CSP improves generalization to higher-order attribute-attribute-object compositions and combinations of pretrained attributes and fine-tuned objects.", "paper_url": "http://arxiv.org/abs/2204.03574v1", "pdf_url": "http://arxiv.org/pdf/2204.03574v1", "repo_url": "https://github.com/batsresearch/csp"}, "2204.03558": {"publish_time": "2022-04-07", "title": "Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic", "author": "Ant\u00f3nio C\u00e2mara et.al.", "abstract": "As natural language processing systems become more widespread, it is necessary to address fairness issues in their implementation and deployment to ensure that their negative impacts on society are understood and minimized. However, there is limited work that studies fairness using a multilingual and intersectional framework or on downstream tasks. In this paper, we introduce four multilingual Equity Evaluation Corpora, supplementary test sets designed to measure social biases, and a novel statistical framework for studying unisectional and intersectional social biases in natural language processing. We use these tools to measure gender, racial, ethnic, and intersectional social biases across five models trained on emotion regression tasks in English, Spanish, and Arabic. We find that many systems demonstrate statistically significant unisectional and intersectional social biases.", "paper_url": "http://arxiv.org/abs/2204.03558v1", "pdf_url": "http://arxiv.org/pdf/2204.03558v1", "repo_url": null}, "2204.04179": {"publish_time": "2022-04-08", "title": "GRAM: Fast Fine-tuning of Pre-trained Language Models for Content-based Collaborative Filtering", "author": "Yoonseok Yang et.al.", "abstract": "Content-based collaborative filtering (CCF) provides personalized item recommendations based on both users' interaction history and items' content information. Recently, pre-trained language models (PLM) have been used to extract high-quality item encodings for CCF. However, it is resource-intensive to finetune PLM in an end-to-end (E2E) manner in CCF due to its multi-modal nature: optimization involves redundant content encoding for interactions from users. For this, we propose GRAM (GRadient Accumulation for Multi-modality): (1) Single-step GRAM which aggregates gradients for each item while maintaining theoretical equivalence with E2E, and (2) Multi-step GRAM which further accumulates gradients across multiple training steps, with less than 40\\% GPU memory footprint of E2E. We empirically confirm that GRAM achieves a remarkable boost in training efficiency based on five datasets from two task domains of Knowledge Tracing and News Recommendation, where single-step and multi-step GRAM achieve 4x and 45x training speedup on average, respectively.", "paper_url": "http://arxiv.org/abs/2204.04179v1", "pdf_url": "http://arxiv.org/pdf/2204.04179v1", "repo_url": null}, "2204.04163": {"publish_time": "2022-04-08", "title": "Contextual Representation Learning beyond Masked Language Modeling", "author": "Zhiyi Fu et.al.", "abstract": "How do masked language models (MLMs) such as BERT learn contextual representations? In this work, we analyze the learning dynamics of MLMs. We find that MLMs adopt sampled embeddings as anchors to estimate and inject contextual semantics to representations, which limits the efficiency and effectiveness of MLMs. To address these issues, we propose TACO, a simple yet effective representation learning approach to directly model global semantics. TACO extracts and aligns contextual semantics hidden in contextualized representations to encourage models to attend global semantics when generating contextualized representations. Experiments on the GLUE benchmark show that TACO achieves up to 5x speedup and up to 1.2 points average improvement over existing MLMs. The code is available at https://github.com/FUZHIYI/TACO.", "paper_url": "http://arxiv.org/abs/2204.04163v1", "pdf_url": "http://arxiv.org/pdf/2204.04163v1", "repo_url": null}, "2204.04080": {"publish_time": "2022-04-08", "title": "Learning the Ordering of Coordinate Compounds and Elaborate Expressions in Hmong, Lahu, and Chinese", "author": "Chenxuan Cui et.al.", "abstract": "Coordinate compounds (CCs) and elaborate expressions (EEs) are coordinate constructions common in languages of East and Southeast Asia. Mortensen (2006) claims that (1) the linear ordering of EEs and CCs in Hmong, Lahu, and Chinese can be predicted via phonological hierarchies and (2) these phonological hierarchies lack a clear phonetic rationale. These claims are significant because morphosyntax has often been seen as in a feed-forward relationship with phonology, and phonological generalizations have often been assumed to be phonetically \"natural\". We investigate whether the ordering of CCs and EEs can be learned empirically and whether computational models (classifiers and sequence labeling models) learn unnatural hierarchies similar to those posited by Mortensen (2006). We find that decision trees and SVMs learn to predict the order of CCs/EEs on the basis of phonology, with DTs learning hierarchies strikingly similar to those proposed by Mortensen. However, we also find that a neural sequence labeling model is able to learn the ordering of elaborate expressions in Hmong very effectively without using any phonological information. We argue that EE ordering can be learned through two independent routes: phonology and lexical distribution, presenting a more nuanced picture than previous work. [ISO 639-3:hmn, lhu, cmn]", "paper_url": "http://arxiv.org/abs/2204.04080v1", "pdf_url": "http://arxiv.org/pdf/2204.04080v1", "repo_url": null}, "2204.04058": {"publish_time": "2022-04-08", "title": "Improving Tokenisation by Alternative Treatment of Spaces", "author": "Edward Gow-Smith et.al.", "abstract": "Tokenisation is the first step in almost all NLP tasks, and state-of-the-art transformer-based language models all use subword tokenisation algorithms to process input text. Existing algorithms have problems, often producing tokenisations of limited linguistic validity, and representing equivalent strings differently depending on their position within a word. We hypothesise that these problems hinder the ability of transformer-based models to handle complex words, and suggest that these problems are a result of allowing tokens to include spaces. We thus experiment with an alternative tokenisation approach where spaces are always treated as individual tokens. Specifically, we apply this modification to the BPE and Unigram algorithms. We find that our modified algorithms lead to improved performance on downstream NLP tasks that involve handling complex words, whilst having no detrimental effect on performance in general natural language understanding tasks. Intrinsically, we find our modified algorithms give more morphologically correct tokenisations, in particular when handling prefixes. Given the results of our experiments, we advocate for always treating spaces as individual tokens as an improved tokenisation method.", "paper_url": "http://arxiv.org/abs/2204.04058v1", "pdf_url": "http://arxiv.org/pdf/2204.04058v1", "repo_url": "https://github.com/edwardgowsmith/improved-tokenisation-methods"}, "2204.04043": {"publish_time": "2022-04-08", "title": "C-NMT: A Collaborative Inference Framework for Neural Machine Translation", "author": "Yukai Chen et.al.", "abstract": "Collaborative Inference (CI) optimizes the latency and energy consumption of deep learning inference through the inter-operation of edge and cloud devices. Albeit beneficial for other tasks, CI has never been applied to the sequence- to-sequence mapping problem at the heart of Neural Machine Translation (NMT). In this work, we address the specific issues of collaborative NMT, such as estimating the latency required to generate the (unknown) output sequence, and show how existing CI methods can be adapted to these applications. Our experiments show that CI can reduce the latency of NMT by up to 44% compared to a non-collaborative approach.", "paper_url": "http://arxiv.org/abs/2204.04043v1", "pdf_url": "http://arxiv.org/pdf/2204.04043v1", "repo_url": null}, "2204.05307": {"publish_time": "2022-04-11", "title": "Toward More Effective Human Evaluation for Machine Translation", "author": "Bel\u00e9n Sald\u00edas et.al.", "abstract": "Improvements in text generation technologies such as machine translation have necessitated more costly and time-consuming human evaluation procedures to ensure an accurate signal. We investigate a simple way to reduce cost by reducing the number of text segments that must be annotated in order to accurately predict a score for a complete test set. Using a sampling approach, we demonstrate that information from document membership and automatic metrics can help improve estimates compared to a pure random sampling baseline. We achieve gains of up to 20% in average absolute error by leveraging stratified sampling and control variates. Our techniques can improve estimates made from a fixed annotation budget, are easy to implement, and can be applied to any problem with structure similar to the one we study.", "paper_url": "http://arxiv.org/abs/2204.05307v1", "pdf_url": "http://arxiv.org/pdf/2204.05307v1", "repo_url": null}, "2204.05248": {"publish_time": "2022-04-11", "title": "Learning Downstream Task by Selectively Capturing Complementary Knowledge from Multiple Self-supervisedly Learning Pretexts", "author": "Quan Feng et.al.", "abstract": "Self-supervised learning (SSL), as a newly emerging unsupervised representation learning paradigm, generally follows a two-stage learning pipeline: 1) learning invariant and discriminative representations with auto-annotation pretext(s), then 2) transferring the representations to assist downstream task(s). Such two stages are usually implemented separately, making the learned representation learned agnostic to the downstream tasks. Currently, most works are devoted to exploring the first stage. Whereas, it is less studied on how to learn downstream tasks with limited labeled data using the already learned representations. Especially, it is crucial and challenging to selectively utilize the complementary representations from diverse pretexts for a downstream task. In this paper, we technically propose a novel solution by leveraging the attention mechanism to adaptively squeeze suitable representations for the tasks. Meanwhile, resorting to information theory, we theoretically prove that gathering representation from diverse pretexts is more effective than a single one. Extensive experiments validate that our scheme significantly exceeds current popular pretext-matching based methods in gathering knowledge and relieving negative transfer in downstream tasks.", "paper_url": "http://arxiv.org/abs/2204.05248v1", "pdf_url": "http://arxiv.org/pdf/2204.05248v1", "repo_url": null}, "2204.05239": {"publish_time": "2022-04-11", "title": "Exploring the Universal Vulnerability of Prompt-based Learning Paradigm", "author": "Lei Xu et.al.", "abstract": "Prompt-based learning paradigm bridges the gap between pre-training and fine-tuning, and works effectively under the few-shot setting. However, we find that this learning paradigm inherits the vulnerability from the pre-training stage, where model predictions can be misled by inserting certain triggers into the text. In this paper, we explore this universal vulnerability by either injecting backdoor triggers or searching for adversarial triggers on pre-trained language models using only plain text. In both scenarios, we demonstrate that our triggers can totally control or severely decrease the performance of prompt-based models fine-tuned on arbitrary downstream tasks, reflecting the universal vulnerability of the prompt-based learning paradigm. Further experiments show that adversarial triggers have good transferability among language models. We also find conventional fine-tuning models are not vulnerable to adversarial triggers constructed from pre-trained language models. We conclude by proposing a potential solution to mitigate our attack methods. Code and data are publicly available at https://github.com/leix28/prompt-universal-vulnerability", "paper_url": "http://arxiv.org/abs/2204.05239v1", "pdf_url": "http://arxiv.org/pdf/2204.05239v1", "repo_url": "https://github.com/leix28/prompt-universal-vulnerability"}, "2204.05232": {"publish_time": "2022-04-11", "title": "Survey of Aspect-based Sentiment Analysis Datasets", "author": "Siva Uday Sampreeth Chebolu et.al.", "abstract": "Aspect-based sentiment analysis (ABSA) is a natural language processing problem that requires analyzing user-generated reviews in order to determine: a) The target entity being reviewed, b) The high-level aspect to which it belongs, and c) The sentiment expressed toward the targets and the aspects. Numerous yet scattered corpora for ABSA make it difficult for researchers to quickly identify corpora best suited for a specific ABSA subtask. This study aims to present a database of corpora that can be used to train and assess autonomous ABSA systems. Additionally, we provide an overview of the major corpora concerning the various ABSA and its subtasks and highlight several corpus features that researchers should consider when selecting a corpus. We conclude that further large-scale ABSA corpora are required. Additionally, because each corpus is constructed differently, it is time-consuming for researchers to experiment with a novel ABSA algorithm on many corpora and often employ just one or a few corpora. The field would benefit from an agreement on a data standard for ABSA corpora. Finally, we discuss the advantages and disadvantages of current collection approaches and make recommendations for future ABSA dataset gathering.", "paper_url": "http://arxiv.org/abs/2204.05232v1", "pdf_url": "http://arxiv.org/pdf/2204.05232v1", "repo_url": null}, "2204.05231": {"publish_time": "2022-04-11", "title": "Towards Generalizeable Semantic Product Search by Text Similarity Pre-training on Search Click Logs", "author": "Zheng Liu et.al.", "abstract": "Recently, semantic search has been successfully applied to e-commerce product search and the learned semantic space(s) for query and product encoding are expected to generalize to unseen queries or products. Yet, whether generalization can conveniently emerge has not been thoroughly studied in the domain thus far. In this paper, we examine several general-domain and domain-specific pre-trained Roberta variants and discover that general-domain fine-tuning does not help generalization, which aligns with the discovery of prior art. Proper domain-specific fine-tuning with clickstream data can lead to better model generalization, based on a bucketed analysis of a publicly available manual annotated query-product pair data.", "paper_url": "http://arxiv.org/abs/2204.05231v1", "pdf_url": "http://arxiv.org/pdf/2204.05231v1", "repo_url": null}, "2204.05991": {"publish_time": "2022-04-12", "title": "ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension", "author": "Sanjay Subramanian et.al.", "abstract": "Training a referring expression comprehension (ReC) model for a new visual domain requires collecting referring expressions, and potentially corresponding bounding boxes, for images in the domain. While large-scale pre-trained models are useful for image classification across domains, it remains unclear if they can be applied in a zero-shot manner to more complex tasks like ReC. We present ReCLIP, a simple but strong zero-shot baseline that repurposes CLIP, a state-of-the-art large-scale model, for ReC. Motivated by the close connection between ReC and CLIP's contrastive pre-training objective, the first component of ReCLIP is a region-scoring method that isolates object proposals via cropping and blurring, and passes them to CLIP. However, through controlled experiments on a synthetic dataset, we find that CLIP is largely incapable of performing spatial reasoning off-the-shelf. Thus, the second component of ReCLIP is a spatial relation resolver that handles several types of spatial relations. We reduce the gap between zero-shot baselines from prior work and supervised models by as much as 29% on RefCOCOg, and on RefGTA (video game imagery), ReCLIP's relative improvement over supervised ReC models trained on real images is 8%.", "paper_url": "http://arxiv.org/abs/2204.05991v1", "pdf_url": "http://arxiv.org/pdf/2204.05991v1", "repo_url": "https://github.com/allenai/reclip"}, "2204.05990": {"publish_time": "2022-04-12", "title": "Detection, Disambiguation, Re-ranking: Autoregressive Entity Linking as a Multi-Task Problem", "author": "Khalil Mrini et.al.", "abstract": "We propose an autoregressive entity linking model, that is trained with two auxiliary tasks, and learns to re-rank generated samples at inference time. Our proposed novelties address two weaknesses in the literature. First, a recent method proposes to learn mention detection and then entity candidate selection, but relies on predefined sets of candidates. We use encoder-decoder autoregressive entity linking in order to bypass this need, and propose to train mention detection as an auxiliary task instead. Second, previous work suggests that re-ranking could help correct prediction errors. We add a new, auxiliary task, match prediction, to learn re-ranking. Without the use of a knowledge base or candidate sets, our model sets a new state of the art in two benchmark datasets of entity linking: COMETA in the biomedical domain, and AIDA-CoNLL in the news domain. We show through ablation studies that each of the two auxiliary tasks increases performance, and that re-ranking is an important factor to the increase. Finally, our low-resource experimental results suggest that performance on the main task benefits from the knowledge learned by the auxiliary tasks, and not just from the additional training data.", "paper_url": "http://arxiv.org/abs/2204.05990v1", "pdf_url": "http://arxiv.org/pdf/2204.05990v1", "repo_url": null}, "2204.05961": {"publish_time": "2022-04-12", "title": "Quantified Reproducibility Assessment of NLP Results", "author": "Anya Belz et.al.", "abstract": "This paper describes and tests a method for carrying out quantified reproducibility assessment (QRA) that is based on concepts and definitions from metrology. QRA produces a single score estimating the degree of reproducibility of a given system and evaluation measure, on the basis of the scores from, and differences between, different reproductions. We test QRA on 18 system and evaluation measure combinations (involving diverse NLP tasks and types of evaluation), for each of which we have the original results and one to seven reproduction results. The proposed QRA method produces degree-of-reproducibility scores that are comparable across multiple reproductions not only of the same, but of different original studies. We find that the proposed method facilitates insights into causes of variation between reproductions, and allows conclusions to be drawn about what changes to system and/or evaluation design might lead to improved reproducibility.", "paper_url": "http://arxiv.org/abs/2204.05961v1", "pdf_url": "http://arxiv.org/pdf/2204.05961v1", "repo_url": null}, "2204.05953": {"publish_time": "2022-04-12", "title": "Explore More Guidance: A Task-aware Instruction Network for Sign Language Translation Enhanced with Data Augmentation", "author": "Yong Cao et.al.", "abstract": "Sign language recognition and translation first uses a recognition module to generate glosses from sign language videos and then employs a translation module to translate glosses into spoken sentences. Most existing works focus on the recognition step, while paying less attention to sign language translation. In this work, we propose a task-aware instruction network, namely TIN-SLT, for sign language translation, by introducing the instruction module and the learning-based feature fuse strategy into a Transformer network. In this way, the pre-trained model's language ability can be well explored and utilized to further boost the translation performance. Moreover, by exploring the representation space of sign language glosses and target spoken language, we propose a multi-level data augmentation scheme to adjust the data distribution of the training set. We conduct extensive experiments on two challenging benchmark datasets, PHOENIX-2014-T and ASLG-PC12, on which our method outperforms former best solutions by 1.65 and 1.42 in terms of BLEU-4. Our code is published at https://github.com/yongcaoplus/TIN-SLT.", "paper_url": "http://arxiv.org/abs/2204.05953v1", "pdf_url": "http://arxiv.org/pdf/2204.05953v1", "repo_url": "https://github.com/yongcaoplus/tin-slt"}, "2204.05939": {"publish_time": "2022-04-12", "title": "Mining Logical Event Schemas From Pre-Trained Language Models", "author": "Lane Lawley et.al.", "abstract": "We present NESL (the Neuro-Episodic Schema Learner), an event schema learning system that combines large language models, FrameNet parsing, a powerful logical representation of language, and a set of simple behavioral schemas meant to bootstrap the learning process. In lieu of a pre-made corpus of stories, our dataset is a continuous feed of \"situation samples\" from a pre-trained language model, which are then parsed into FrameNet frames, mapped into simple behavioral schemas, and combined and generalized into complex, hierarchical schemas for a variety of everyday scenarios. We show that careful sampling from the language model can help emphasize stereotypical properties of situations and de-emphasize irrelevant details, and that the resulting schemas specify situations more comprehensively than those learned by other systems.", "paper_url": "http://arxiv.org/abs/2204.05939v1", "pdf_url": "http://arxiv.org/pdf/2204.05939v1", "repo_url": null}}}, "Reinforcement Learning": {"Reinforcement Learning": {"2202.12884": {"publish_time": "2022-02-25", "title": "Learning to Identify Perceptual Bugs in 3D Video Games", "author": "Benedict Wilkins et.al.", "abstract": "Automated Bug Detection (ABD) in video games is composed of two distinct but complementary problems: automated game exploration and bug identification. Automated game exploration has received much recent attention, spurred on by developments in fields such as reinforcement learning. The complementary problem of identifying the bugs present in a player's experience has for the most part relied on the manual specification of rules. Although it is widely recognised that many bugs of interest cannot be identified with such methods, little progress has been made in this direction. In this work we show that it is possible to identify a range of perceptual bugs using learning-based methods by making use of only the rendered game screen as seen by the player. To support our work, we have developed World of Bugs (WOB) an open platform for testing ABD methods in 3D game environments.", "paper_url": "http://arxiv.org/abs/2202.12884v1", "pdf_url": "http://arxiv.org/pdf/2202.12884v1", "repo_url": null}, "2202.12872": {"publish_time": "2022-02-25", "title": "AutoFR: Automated Filter Rule Generation for Adblocking", "author": "Hieu Le et.al.", "abstract": "Adblocking relies on filter lists, which are manually curated and maintained by a small community of filter list authors. This manual process is laborious and does not scale well to a large number of sites and over time. We introduce AutoFR, a reinforcement learning framework to fully automate the process of filter rule creation and evaluation. We design an algorithm based on multi-arm bandits to generate filter rules while controlling the trade-off between blocking ads and avoiding breakage. We test our implementation of AutoFR on thousands of sites in terms of efficiency and effectiveness. AutoFR is efficient: it takes only a few minutes to generate filter rules for a site. AutoFR is also effective: it generates filter rules that can block 86% of the ads, as compared to 87% by EasyList while achieving comparable visual breakage. The filter rules generated by AutoFR generalize well to new and unseen sites. We envision AutoFR to assist the adblocking community in automated filter rule generation at scale.", "paper_url": "http://arxiv.org/abs/2202.12872v1", "pdf_url": "http://arxiv.org/pdf/2202.12872v1", "repo_url": null}, "2202.12866": {"publish_time": "2022-02-25", "title": "Learning to Schedule Heuristics for the Simultaneous Stochastic Optimization of Mining Complexes", "author": "Yassine Yaakoubi et.al.", "abstract": "The simultaneous stochastic optimization of mining complexes (SSOMC) is a large-scale stochastic combinatorial optimization problem that simultaneously manages the extraction of materials from multiple mines and their processing using interconnected facilities to generate a set of final products, while taking into account material supply (geological) uncertainty to manage the associated risk. Although simulated annealing has been shown to outperform comparing methods for solving the SSOMC, early performance might dominate recent performance in that a combination of the heuristics' performance is used to determine which perturbations to apply. This work proposes a data-driven framework for heuristic scheduling in a fully self-managed hyper-heuristic to solve the SSOMC. The proposed learn-to-perturb (L2P) hyper-heuristic is a multi-neighborhood simulated annealing algorithm. The L2P selects the heuristic (perturbation) to be applied in a self-adaptive manner using reinforcement learning to efficiently explore which local search is best suited for a particular search point. Several state-of-the-art agents have been incorporated into L2P to better adapt the search and guide it towards better solutions. By learning from data describing the performance of the heuristics, a problem-specific ordering of heuristics that collectively finds better solutions faster is obtained. L2P is tested on several real-world mining complexes, with an emphasis on efficiency, robustness, and generalization capacity. Results show a reduction in the number of iterations by 30-50% and in the computational time by 30-45%.", "paper_url": "http://arxiv.org/abs/2202.12866v1", "pdf_url": "http://arxiv.org/pdf/2202.12866v1", "repo_url": null}, "2202.12861": {"publish_time": "2022-02-25", "title": "Hierarchical Control for Multi-Agent Autonomous Racing", "author": "Rishabh Saumil Thakkar et.al.", "abstract": "We develop a hierarchical controller for multi-agent autonomous racing. A high-level planner approximates the race as a discrete game with simplified dynamics that encodes the complex safety and fairness rules seen in real-life racing and calculates a series of target waypoints. The low-level controller takes the resulting waypoints as a reference trajectory and computes high-resolution control inputs by solving a simplified formulation of a multi-agent racing game. We consider two approaches for the low-level planner to construct two hierarchical controllers. One approach uses multi-agent reinforcement learning (MARL), and the other solves a linear-quadratic Nash game (LQNG) to produce control inputs. We test the controllers against three baselines: an end-to-end MARL controller, a MARL controller tracking a fixed racing line, and an LQNG controller tracking a fixed racing line. Quantitative results show that the proposed hierarchical methods outperform their respective baseline methods in terms of head-to-head race wins and abiding by the rules. The hierarchical controller using MARL for low-level control consistently outperformed all other methods by winning over 88\\% of head-to-head races and more consistently adhered to the complex racing rules. Qualitatively, we observe the proposed controllers mimicking actions performed by expert human drivers such as shielding/blocking, overtaking, and long-term planning for delayed advantages. We show that hierarchical planning for game-theoretic reasoning produces competitive behavior even when challenged with complex rules and constraints.", "paper_url": "http://arxiv.org/abs/2202.12861v1", "pdf_url": "http://arxiv.org/pdf/2202.12861v1", "repo_url": "https://github.com/ribsthakkar/HierarchicalKarting"}, "2202.12847": {"publish_time": "2022-02-25", "title": "Building a 3-Player Mahjong AI using Deep Reinforcement Learning", "author": "Xiangyu Zhao et.al.", "abstract": "Mahjong is a popular multi-player imperfect-information game developed in China in the late 19th-century, with some very challenging features for AI research. Sanma, being a 3-player variant of the Japanese Riichi Mahjong, possesses unique characteristics including fewer tiles and, consequently, a more aggressive playing style. It is thus challenging and of great research interest in its own right, but has not yet been explored. In this paper, we present Meowjong, an AI for Sanma using deep reinforcement learning. We define an informative and compact 2-dimensional data structure for encoding the observable information in a Sanma game. We pre-train 5 convolutional neural networks (CNNs) for Sanma's 5 actions -- discard, Pon, Kan, Kita and Riichi, and enhance the major action's model, namely the discard model, via self-play reinforcement learning using the Monte Carlo policy gradient method. Meowjong's models achieve test accuracies comparable with AIs for 4-player Mahjong through supervised learning, and gain a significant further enhancement from reinforcement learning. Being the first ever AI in Sanma, we claim that Meowjong stands as a state-of-the-art in this game.", "paper_url": "http://arxiv.org/abs/2202.12847v1", "pdf_url": "http://arxiv.org/pdf/2202.12847v1", "repo_url": null}, "2202.13914": {"publish_time": "2022-02-28", "title": "Combining Modular Skills in Multitask Learning", "author": "Edoardo M. Ponti et.al.", "abstract": "A modular design encourages neural models to disentangle and recombine different facets of knowledge to generalise more systematically to new tasks. In this work, we assume that each task is associated with a subset of latent discrete skills from a (potentially small) inventory. In turn, skills correspond to parameter-efficient (sparse / low-rank) model parameterisations. By jointly learning these and a task-skill allocation matrix, the network for each task is instantiated as the average of the parameters of active skills. To favour non-trivial soft partitions of skills across tasks, we experiment with a series of inductive biases, such as an Indian Buffet Process prior and a two-speed learning rate. We evaluate our latent-skill model on two main settings: 1) multitask reinforcement learning for grounded instruction following on 8 levels of the BabyAI platform; and 2) few-shot adaptation of pre-trained text-to-text generative models on CrossFit, a benchmark comprising 160 NLP tasks. We find that the modular design of a network significantly increases sample efficiency in reinforcement learning and few-shot generalisation in supervised learning, compared to baselines with fully shared, task-specific, or conditionally generated parameters where knowledge is entangled across tasks. In addition, we show how discrete skills help interpretability, as they yield an explicit hierarchy of tasks.", "paper_url": "http://arxiv.org/abs/2202.13914v1", "pdf_url": "http://arxiv.org/pdf/2202.13914v1", "repo_url": null}, "2202.13890": {"publish_time": "2022-02-28", "title": "Pessimistic Q-Learning for Offline Reinforcement Learning: Towards Optimal Sample Complexity", "author": "Laixi Shi et.al.", "abstract": "Offline or batch reinforcement learning seeks to learn a near-optimal policy using history data without active exploration of the environment. To counter the insufficient coverage and sample scarcity of many offline datasets, the principle of pessimism has been recently introduced to mitigate high bias of the estimated values. While pessimistic variants of model-based algorithms (e.g., value iteration with lower confidence bounds) have been theoretically investigated, their model-free counterparts -- which do not require explicit model estimation -- have not been adequately studied, especially in terms of sample efficiency. To address this inadequacy, we study a pessimistic variant of Q-learning in the context of finite-horizon Markov decision processes, and characterize its sample complexity under the single-policy concentrability assumption which does not require the full coverage of the state-action space. In addition, a variance-reduced pessimistic Q-learning algorithm is proposed to achieve near-optimal sample complexity. Altogether, this work highlights the efficiency of model-free algorithms in offline RL when used in conjunction with pessimism and variance reduction.", "paper_url": "http://arxiv.org/abs/2202.13890v1", "pdf_url": "http://arxiv.org/pdf/2202.13890v1", "repo_url": null}, "2202.13887": {"publish_time": "2022-02-28", "title": "Probing the Robustness of Trained Metrics for Conversational Dialogue Systems", "author": "Jan Deriu et.al.", "abstract": "This paper introduces an adversarial method to stress-test trained metrics to evaluate conversational dialogue systems. The method leverages Reinforcement Learning to find response strategies that elicit optimal scores from the trained metrics. We apply our method to test recently proposed trained metrics. We find that they all are susceptible to giving high scores to responses generated by relatively simple and obviously flawed strategies that our method converges on. For instance, simply copying parts of the conversation context to form a response yields competitive scores or even outperforms responses written by humans.", "paper_url": "http://arxiv.org/abs/2202.13887v1", "pdf_url": "http://arxiv.org/pdf/2202.13887v1", "repo_url": null}, "2202.13863": {"publish_time": "2022-02-28", "title": "Provably Efficient Convergence of Primal-Dual Actor-Critic with Nonlinear Function Approximation", "author": "Jing Dong et.al.", "abstract": "We study the convergence of the actor-critic algorithm with nonlinear function approximation under a nonconvex-nonconcave primal-dual formulation. Stochastic gradient descent ascent is applied with an adaptive proximal term for robust learning rates. We show the first efficient convergence result with primal-dual actor-critic with a convergence rate of $\\mathcal{O}\\left(\\sqrt{\\frac{\\ln \\left(N d G^2 \\right)}{N}}\\right)$ under Markovian sampling, where $G$ is the element-wise maximum of the gradient, $N$ is the number of iterations, and $d$ is the dimension of the gradient. Our result is presented with only the Polyak-\\L{}ojasiewicz condition for the dual variables, which is easy to verify and applicable to a wide range of reinforcement learning (RL) scenarios. The algorithm and analysis are general enough to be applied to other RL settings, like multi-agent RL. Empirical results on OpenAI Gym continuous control tasks corroborate our theoretical findings.", "paper_url": "http://arxiv.org/abs/2202.13863v1", "pdf_url": "http://arxiv.org/pdf/2202.13863v1", "repo_url": null}, "2202.13706": {"publish_time": "2022-02-28", "title": "Monkey Business: Reinforcement learning meets neighborhood search for Virtual Network Embedding", "author": "Maxime Elkael et.al.", "abstract": "In this article, we consider the Virtual Network Embedding (VNE) problem for 5G networks slicing. This problem requires to allocate multiple Virtual Networks (VN) on a substrate virtualized physical network while maximizing among others, resource utilization, maximum number of placed VNs and network operator's benefit. We solve the online version of the problem where slices arrive over time. Inspired by the Nested Rollout Policy Adaptation (NRPA) algorithm, a variant of the well known Monte Carlo Tree Search (MCTS) that learns how to perform good simulations over time, we propose a new algorithm that we call Neighborhood Enhanced Policy Adaptation (NEPA). The key feature of our algorithm is to observe NRPA cannot exploit knowledge acquired in one branch of the state tree for another one which starts differently. NEPA learns by combining NRPA with Neighbordhood Search in a frugal manner which improves only promising solutions while keeping the running time low. We call this technique a monkey business because it comes down to jumping from one interesting branch to the other, similar to how monkeys jump from tree to tree instead of going down everytime. NEPA achieves better results in terms of acceptance ratio and revenue-to-cost ratio compared to other state-of-the-art algorithms, both on real and synthetic topologies.", "paper_url": "http://arxiv.org/abs/2202.13706v1", "pdf_url": "http://arxiv.org/pdf/2202.13706v1", "repo_url": null}, "2203.00669": {"publish_time": "2022-03-01", "title": "AI Planning Annotation for Sample Efficient Reinforcement Learning", "author": "Junkyu Lee et.al.", "abstract": "AI planning and Reinforcement Learning (RL) both solve sequential decision-making problems under the different formulations. AI Planning requires operator models, but then allows efficient plan generation. RL requires no operator model, instead learns a policy to guide an agent to high reward states. Planning can be brittle in the face of noise whereas RL is more tolerant. However, RL requires a large number of training examples to learn the policy. In this work, we aim to bring AI planning and RL closer by showing that a suitably defined planning model can be used to improve the efficiency of RL. Specifically, we show that the options in the hierarchical RL can be derived from a planning task and integrate planning and RL algorithms for training option policy functions. Our experiments demonstrate an improved sample efficiency on a variety of RL environments over the previous state-of-the-art.", "paper_url": "http://arxiv.org/abs/2203.00669v1", "pdf_url": "http://arxiv.org/pdf/2203.00669v1", "repo_url": null}, "2203.00636": {"publish_time": "2022-03-01", "title": "Distributional Reinforcement Learning for Scheduling of (Bio)chemical Production Processes", "author": "Max Mowbray et.al.", "abstract": "Reinforcement Learning (RL) has recently received significant attention from the process systems engineering and control communities. Recent works have investigated the application of RL to identify optimal scheduling decision in the presence of uncertainty. In this work, we present a RL methodology to address precedence and disjunctive constraints as commonly imposed on production scheduling problems. This work naturally enables the optimization of risk-sensitive formulations such as the conditional value-at-risk (CVaR), which are essential in realistic scheduling processes. The proposed strategy is investigated thoroughly in a single-stage, parallel batch production environment, and benchmarked against mixed integer linear programming (MILP) strategies. We show that the policy identified by our approach is able to account for plant uncertainties in online decision-making, with expected performance comparable to existing MILP methods. Additionally, the framework gains the benefits of optimizing for risk-sensitive measures, and identifies decisions orders of magnitude faster than the most efficient optimization approaches. This promises to mitigate practical issues and ease in handling realizations of process uncertainty in the paradigm of online production scheduling.", "paper_url": "http://arxiv.org/abs/2203.00636v1", "pdf_url": "http://arxiv.org/pdf/2203.00636v1", "repo_url": null}, "2203.00543": {"publish_time": "2022-03-01", "title": "On the Generalization of Representations in Reinforcement Learning", "author": "Charline Le Lan et.al.", "abstract": "In reinforcement learning, state representations are used to tractably deal with large problem spaces. State representations serve both to approximate the value function with few parameters, but also to generalize to newly encountered states. Their features may be learned implicitly (as part of a neural network) or explicitly (for example, the successor representation of \\citet{dayan1993improving}). While the approximation properties of representations are reasonably well-understood, a precise characterization of how and when these representations generalize is lacking. In this work, we address this gap and provide an informative bound on the generalization error arising from a specific state representation. This bound is based on the notion of effective dimension which measures the degree to which knowing the value at one state informs the value at other states. Our bound applies to any state representation and quantifies the natural tension between representations that generalize well and those that approximate well. We complement our theoretical results with an empirical survey of classic representation learning methods from the literature and results on the Arcade Learning Environment, and find that the generalization behaviour of learned representations is well-explained by their effective dimension.", "paper_url": "http://arxiv.org/abs/2203.00543v1", "pdf_url": "http://arxiv.org/pdf/2203.00543v1", "repo_url": null}, "2203.00494": {"publish_time": "2022-03-01", "title": "DreamingV2: Reinforcement Learning with Discrete World Models without Reconstruction", "author": "Masashi Okada et.al.", "abstract": "The present paper proposes a novel reinforcement learning method with world models, DreamingV2, a collaborative extension of DreamerV2 and Dreaming. DreamerV2 is a cutting-edge model-based reinforcement learning from pixels that uses discrete world models to represent latent states with categorical variables. Dreaming is also a form of reinforcement learning from pixels that attempts to avoid the autoencoding process in general world model training by involving a reconstruction-free contrastive learning objective. The proposed DreamingV2 is a novel approach of adopting both the discrete representation of DreamingV2 and the reconstruction-free objective of Dreaming. Compared to DreamerV2 and other recent model-based methods without reconstruction, DreamingV2 achieves the best scores on five simulated challenging 3D robot arm tasks. We believe that DreamingV2 will be a reliable solution for robot learning since its discrete representation is suitable to describe discontinuous environments, and the reconstruction-free fashion well manages complex vision observations.", "paper_url": "http://arxiv.org/abs/2203.00494v1", "pdf_url": "http://arxiv.org/pdf/2203.00494v1", "repo_url": null}, "2203.00397": {"publish_time": "2022-03-01", "title": "A Theory of Abstraction in Reinforcement Learning", "author": "David Abel et.al.", "abstract": "Reinforcement learning defines the problem facing agents that learn to make good decisions through action and observation alone. To be effective problem solvers, such agents must efficiently explore vast worlds, assign credit from delayed feedback, and generalize to new experiences, all while making use of limited data, computational resources, and perceptual bandwidth. Abstraction is essential to all of these endeavors. Through abstraction, agents can form concise models of their environment that support the many practices required of a rational, adaptive decision maker. In this dissertation, I present a theory of abstraction in reinforcement learning. I first offer three desiderata for functions that carry out the process of abstraction: they should 1) preserve representation of near-optimal behavior, 2) be learned and constructed efficiently, and 3) lower planning or learning time. I then present a suite of new algorithms and analysis that clarify how agents can learn to abstract according to these desiderata. Collectively, these results provide a partial path toward the discovery and use of abstraction that minimizes the complexity of effective reinforcement learning.", "paper_url": "http://arxiv.org/abs/2203.00397v1", "pdf_url": "http://arxiv.org/pdf/2203.00397v1", "repo_url": null}, "2203.01302": {"publish_time": "2022-03-02", "title": "Evolving Curricula with Regret-Based Environment Design", "author": "Jack Parker-Holder et.al.", "abstract": "It remains a significant challenge to train generally capable agents with reinforcement learning (RL). A promising avenue for improving the robustness of RL agents is through the use of curricula. One such class of methods frames environment design as a game between a student and a teacher, using regret-based objectives to produce environment instantiations (or levels) at the frontier of the student agent's capabilities. These methods benefit from their generality, with theoretical guarantees at equilibrium, yet they often struggle to find effective levels in challenging design spaces. By contrast, evolutionary approaches seek to incrementally alter environment complexity, resulting in potentially open-ended learning, but often rely on domain-specific heuristics and vast amounts of computational resources. In this paper we propose to harness the power of evolution in a principled, regret-based curriculum. Our approach, which we call Adversarially Compounding Complexity by Editing Levels (ACCEL), seeks to constantly produce levels at the frontier of an agent's capabilities, resulting in curricula that start simple but become increasingly complex. ACCEL maintains the theoretical benefits of prior regret-based methods, while providing significant empirical gains in a diverse set of environments. An interactive version of the paper is available at accelagent.github.io.", "paper_url": "http://arxiv.org/abs/2203.01302v1", "pdf_url": "http://arxiv.org/pdf/2203.01302v1", "repo_url": null}, "2203.01298": {"publish_time": "2022-03-02", "title": "Pareto Frontier Approximation Network (PA-Net) to Solve Bi-objective TSP", "author": "Ishaan Mehta et.al.", "abstract": "Travelling salesperson problem (TSP) is a classic resource allocation problem used to find an optimal order of doing a set of tasks while minimizing (or maximizing) an associated objective function. It is widely used in robotics for applications such as planning, scheduling etc. In this work, we solve TSP for two objectives using reinforcement learning. Often in multi objective optimization problems, the associated objective functions can be conflicting in nature. In such cases, the optimality is defined in terms of Pareto optimality. A set of these Pareto Optimal solutions in the objective space form a Pareto front (or frontier). Each solution has its own trade off. } In this work, we present PA-Net, a network that generates good approximations of the Pareto front for the bi-objective travelling salesperson problem (BTSP). Firstly, BTSP is converted into a constrained optimization problem. We then train our network to solve this constrained problem using the Lagrangian relaxation and policy gradient. With PA-Net we are able to generate good quality Pareto fronts with fast inference times. Finally, we present the application of PA-Net to find optimal visiting order in a robotic navigation task/coverage planning.", "paper_url": "http://arxiv.org/abs/2203.01298v1", "pdf_url": "http://arxiv.org/pdf/2203.01298v1", "repo_url": null}, "2203.01292": {"publish_time": "2022-03-02", "title": "Andes_gym: A Versatile Environment for Deep Reinforcement Learning in Power Systems", "author": "Hantao Cui et.al.", "abstract": "This paper presents Andes_gym, a versatile and high-performance reinforcement learning environment for power system studies. The environment leverages the modeling and simulation capability of ANDES and the reinforcement learning (RL) environment OpenAI Gym to enable the prototyping and demonstration of RL algorithms for power systems. The architecture of the proposed software tool is elaborated to provide the observation and action interfaces for RL algorithms. An example is shown to rapidly prototype a load-frequency control algorithm based on RL trained by available algorithms. The proposed environment is highly generalized by supporting all the power system dynamic models available in ANDES and numerous RL algorithms available for OpenAI Gym.", "paper_url": "http://arxiv.org/abs/2203.01292v1", "pdf_url": "http://arxiv.org/pdf/2203.01292v1", "repo_url": null}, "2203.01190": {"publish_time": "2022-03-02", "title": "Model-free Neural Lyapunov Control for Safe Robot Navigation", "author": "Zikang Xiong et.al.", "abstract": "Model-free Deep Reinforcement Learning (DRL) controllers have demonstrated promising results on various challenging non-linear control tasks. While a model-free DRL algorithm can solve unknown dynamics and high-dimensional problems, it lacks safety assurance. Although safety constraints can be encoded as part of a reward function, there still exists a large gap between an RL controller trained with this modified reward and a safe controller. In contrast, instead of implicitly encoding safety constraints with rewards, we explicitly co-learn a Twin Neural Lyapunov Function (TNLF) with the control policy in the DRL training loop and use the learned TNLF to build a runtime monitor. Combined with the path generated from a planner, the monitor chooses appropriate waypoints that guide the learned controller to provide collision-free control trajectories. Our approach inherits the scalability advantages from DRL while enhancing safety guarantees. Our experimental evaluation demonstrates the effectiveness of our approach compared to DRL with augmented rewards and constrained DRL methods over a range of high-dimensional safety-sensitive navigation tasks.", "paper_url": "http://arxiv.org/abs/2203.01190v1", "pdf_url": "http://arxiv.org/pdf/2203.01190v1", "repo_url": null}, "2203.01148": {"publish_time": "2022-03-02", "title": "Reactive Stepping for Humanoid Robots using Reinforcement Learning: Application to Standing Push Recovery on the Exoskeleton Atalante", "author": "Alexis Duburcq et.al.", "abstract": "State-of-the-art reinforcement learning is now able to learn versatile locomotion, balancing and push-recovery capabilities for bipedal robots in simulation. Yet, the reality gap has mostly been overlooked and the simulated results hardly transfer to real hardware. Either it is unsuccessful in practice because the physics is over-simplified and hardware limitations are ignored, or regularity is not guaranteed and unexpected hazardous motions can occur. This paper presents a reinforcement learning framework capable of learning robust standing push recovery for bipedal robots with a smooth out-of-the-box transfer to reality, requiring only instantaneous proprioceptive observations. By combining original termination conditions and policy smoothness conditioning, we achieve stable learning, sim-to-real transfer and safety using a policy without memory nor observation history. Reward shaping is then used to give insights into how to keep balance. We demonstrate its performance in reality on the lower-limb medical exoskeleton Atalante.", "paper_url": "http://arxiv.org/abs/2203.01148v1", "pdf_url": "http://arxiv.org/pdf/2203.01148v1", "repo_url": null}, "2203.01889": {"publish_time": "2022-03-03", "title": "Quantum Reinforcement Learning via Policy Iteration", "author": "El Amine Cherrat et.al.", "abstract": "Quantum computing has shown the potential to substantially speed up machine learning applications, in particular for supervised and unsupervised learning. Reinforcement learning, on the other hand, has become essential for solving many decision making problems and policy iteration methods remain the foundation of such approaches. In this paper, we provide a general framework for performing quantum reinforcement learning via policy iteration. We validate our framework by designing and analyzing: \\emph{quantum policy evaluation} methods for infinite horizon discounted problems by building quantum states that approximately encode the value function of a policy $\\pi$; and \\emph{quantum policy improvement} methods by post-processing measurement outcomes on these quantum states. Last, we study the theoretical and experimental performance of our quantum algorithms on two environments from OpenAI's Gym.", "paper_url": "http://arxiv.org/abs/2203.01889v1", "pdf_url": "http://arxiv.org/pdf/2203.01889v1", "repo_url": null}, "2203.01855": {"publish_time": "2022-03-03", "title": "Reasoning about Counterfactuals to Improve Human Inverse Reinforcement Learning", "author": "Michael S. Lee et.al.", "abstract": "To collaborate well with robots, we must be able to understand their decision making. Humans naturally infer other agents' beliefs and desires by reasoning about their observable behavior in a way that resembles inverse reinforcement learning (IRL). Thus, robots can convey their beliefs and desires by providing demonstrations that are informative for a human's IRL. An informative demonstration is one that differs strongly from the learner's expectations of what the robot will do given their current understanding of the robot's decision making. However, standard IRL does not model the learner's existing expectations, and thus cannot do this counterfactual reasoning. We propose to incorporate the learner's current understanding of the robot's decision making into our model of human IRL, so that our robot can select demonstrations that maximize the human's understanding. We also propose a novel measure for estimating the difficulty for a human to predict instances of a robot's behavior in unseen environments. A user study finds that our test difficulty measure correlates well with human performance and confidence. Interestingly, considering human beliefs and counterfactuals when selecting demonstrations decreases human performance on easy tests, but increases performance on difficult tests, providing insight on how to best utilize such models.", "paper_url": "http://arxiv.org/abs/2203.01855v1", "pdf_url": "http://arxiv.org/pdf/2203.01855v1", "repo_url": null}, "2203.01821": {"publish_time": "2022-03-03", "title": "Socially Aware Robot Crowd Navigation with Interaction Graphs and Human Trajectory Prediction", "author": "Shuijing Liu et.al.", "abstract": "We study the problem of safe and socially aware robot navigation in dense and interactive human crowds. Previous works use simplified methods to model the personal spaces of pedestrians and ignore the social compliance of the robot behaviors. In this paper, we provide a more accurate representation of personal zones of walking pedestrians with their future trajectories. The predicted personal zones are incorporated into a reinforcement learning framework to prevent the robot from intruding into the personal zones. To learn socially aware navigation policies, we propose a novel recurrent graph neural network with attention mechanisms to capture the interactions among agents through space and time. We demonstrate that our method enables the robot to achieve good navigation performance and non-invasiveness in challenging crowd navigation scenarios. We successfully transfer the policy learned in the simulator to a real-world TurtleBot 2i.", "paper_url": "http://arxiv.org/abs/2203.01821v1", "pdf_url": "http://arxiv.org/pdf/2203.01821v1", "repo_url": null}, "2203.01758": {"publish_time": "2022-03-03", "title": "On Practical Reinforcement Learning: Provable Robustness, Scalability, and Statistical Efficiency", "author": "Thanh Nguyen-Tang et.al.", "abstract": "This thesis rigorously studies fundamental reinforcement learning (RL) methods in modern practical considerations, including robust RL, distributional RL, and offline RL with neural function approximation. The thesis first prepares the readers with an overall overview of RL and key technical background in statistics and optimization. In each of the settings, the thesis motivates the problems to be studied, reviews the current literature, provides computationally efficient algorithms with provable efficiency guarantees, and concludes with future research directions. The thesis makes fundamental contributions to the three settings above, both algorithmically, theoretically, and empirically, while staying relevant to practical considerations.", "paper_url": "http://arxiv.org/abs/2203.01758v1", "pdf_url": "http://arxiv.org/pdf/2203.01758v1", "repo_url": "https://github.com/thanhnguyentang/drbqo"}, "2203.01735": {"publish_time": "2022-03-03", "title": "Modality-Adaptive Mixup and Invariant Decomposition for RGB-Infrared Person Re-Identification", "author": "Zhipeng Huang et.al.", "abstract": "RGB-infrared person re-identification is an emerging cross-modality re-identification task, which is very challenging due to significant modality discrepancy between RGB and infrared images. In this work, we propose a novel modality-adaptive mixup and invariant decomposition (MID) approach for RGB-infrared person re-identification towards learning modality-invariant and discriminative representations. MID designs a modality-adaptive mixup scheme to generate suitable mixed modality images between RGB and infrared images for mitigating the inherent modality discrepancy at the pixel-level. It formulates modality mixup procedure as Markov decision process, where an actor-critic agent learns dynamical and local linear interpolation policy between different regions of cross-modality images under a deep reinforcement learning framework. Such policy guarantees modality-invariance in a more continuous latent space and avoids manifold intrusion by the corrupted mixed modality samples. Moreover, to further counter modality discrepancy and enforce invariant visual semantics at the feature-level, MID employs modality-adaptive convolution decomposition to disassemble a regular convolution layer into modality-specific basis layers and a modality-shared coefficient layer. Extensive experimental results on two challenging benchmarks demonstrate superior performance of MID over state-of-the-art methods.", "paper_url": "http://arxiv.org/abs/2203.01735v1", "pdf_url": "http://arxiv.org/pdf/2203.01735v1", "repo_url": null}, "2203.02481": {"publish_time": "2022-03-04", "title": "AutoDIME: Automatic Design of Interesting Multi-Agent Environments", "author": "Ingmar Kanitscheider et.al.", "abstract": "Designing a distribution of environments in which RL agents can learn interesting and useful skills is a challenging and poorly understood task, for multi-agent environments the difficulties are only exacerbated. One approach is to train a second RL agent, called a teacher, who samples environments that are conducive for the learning of student agents. However, most previous proposals for teacher rewards do not generalize straightforwardly to the multi-agent setting. We examine a set of intrinsic teacher rewards derived from prediction problems that can be applied in multi-agent settings and evaluate them in Mujoco tasks such as multi-agent Hide and Seek as well as a diagnostic single-agent maze task. Of the intrinsic rewards considered we found value disagreement to be most consistent across tasks, leading to faster and more reliable emergence of advanced skills in Hide and Seek and the maze task. Another candidate intrinsic reward considered, value prediction error, also worked well in Hide and Seek but was susceptible to noisy-TV style distractions in stochastic environments. Policy disagreement performed well in the maze task but did not speed up learning in Hide and Seek. Our results suggest that intrinsic teacher rewards, and in particular value disagreement, are a promising approach for automating both single and multi-agent environment design.", "paper_url": "http://arxiv.org/abs/2203.02481v1", "pdf_url": "http://arxiv.org/pdf/2203.02481v1", "repo_url": null}, "2203.02389": {"publish_time": "2022-03-04", "title": "Learning Goal-Oriented Non-Prehensile Pushing in Cluttered Scenes", "author": "Nils Dengler et.al.", "abstract": "Pushing objects through cluttered scenes is a challenging task, especially when the objects to be pushed have initially unknown dynamics and touching other entities has to be avoided to reduce the risk of damage. In this paper, we approach this problem by applying deep reinforcement learning to generate pushing actions for a robotic manipulator acting on a planar surface where objects have to be pushed to goal locations while avoiding other items in the same workspace. With the latent space learned from a depth image of the scene and other observations of the environment, such as contact information between the end effector and the object as well as distance to the goal, our framework is able to learn contact-rich pushing actions that avoid collisions with other objects. As the experimental results with a six degrees of freedom robotic arm show, our system is able to successfully push objects from start to end positions while avoiding nearby objects. Furthermore, we evaluate our learned policy in comparison to a state-of-the-art pushing controller for mobile robots and show that our agent performs better in terms of success rate, collisions with other objects, and continuous object contact in various scenarios.", "paper_url": "http://arxiv.org/abs/2203.02389v1", "pdf_url": "http://arxiv.org/pdf/2203.02389v1", "repo_url": null}, "2203.02381": {"publish_time": "2022-03-04", "title": "Where to Look Next: Learning Viewpoint Recommendations for Informative Trajectory Planning", "author": "Max Lodel et.al.", "abstract": "Search missions require motion planning and navigation methods for information gathering that continuously replan based on new observations of the robot's surroundings. Current methods for information gathering, such as Monte Carlo Tree Search, are capable of reasoning over long horizons, but they are computationally expensive. An alternative for fast online execution is to train, offline, an information gathering policy, which indirectly reasons about the information value of new observations. However, these policies lack safety guarantees and do not account for the robot dynamics. To overcome these limitations we train an information-aware policy via deep reinforcement learning, that guides a receding-horizon trajectory optimization planner. In particular, the policy continuously recommends a reference viewpoint to the local planner, such that the resulting dynamically feasible and collision-free trajectories lead to observations that maximize the information gain and reduce the uncertainty about the environment. In simulation tests in previously unseen environments, our method consistently outperforms greedy next-best-view policies and achieves competitive performance compared to Monte Carlo Tree Search, in terms of information gains and coverage time, with a reduction in execution time by three orders of magnitude.", "paper_url": "http://arxiv.org/abs/2203.02381v1", "pdf_url": "http://arxiv.org/pdf/2203.02381v1", "repo_url": null}, "2203.02230": {"publish_time": "2022-03-04", "title": "Cloud-Edge Training Architecture for Sim-to-Real Deep Reinforcement Learning", "author": "Hongpeng Cao et.al.", "abstract": "Deep reinforcement learning (DRL) is a promising approach to solve complex control tasks by learning policies through interactions with the environment. However, the training of DRL policies requires large amounts of training experiences, making it impractical to learn the policy directly on physical systems. Sim-to-real approaches leverage simulations to pretrain DRL policies and then deploy them in the real world. Unfortunately, the direct real-world deployment of pretrained policies usually suffers from performance deterioration due to the different dynamics, known as the reality gap. Recent sim-to-real methods, such as domain randomization and domain adaptation, focus on improving the robustness of the pretrained agents. Nevertheless, the simulation-trained policies often need to be tuned with real-world data to reach optimal performance, which is challenging due to the high cost of real-world samples.   This work proposes a distributed cloud-edge architecture to train DRL agents in the real world in real-time. In the architecture, the inference and training are assigned to the edge and cloud, separating the real-time control loop from the computationally expensive training loop. To overcome the reality gap, our architecture exploits sim-to-real transfer strategies to continue the training of simulation-pretrained agents on a physical system. We demonstrate its applicability on a physical inverted-pendulum control system, analyzing critical parameters. The real-world experiments show that our architecture can adapt the pretrained DRL agents to unseen dynamics consistently and efficiently.", "paper_url": "http://arxiv.org/abs/2203.02230v1", "pdf_url": "http://arxiv.org/pdf/2203.02230v1", "repo_url": null}, "2203.02201": {"publish_time": "2022-03-04", "title": "Neural Simulated Annealing", "author": "Alvaro H. C. Correia et.al.", "abstract": "Simulated annealing (SA) is a stochastic global optimisation technique applicable to a wide range of discrete and continuous variable problems. Despite its simplicity, the development of an effective SA optimiser for a given problem hinges on a handful of carefully handpicked components; namely, neighbour proposal distribution and temperature annealing schedule. In this work, we view SA from a reinforcement learning perspective and frame the proposal distribution as a policy, which can be optimised for higher solution quality given a fixed computational budget. We demonstrate that this Neural SA with such a learnt proposal distribution, parametrised by small equivariant neural networks, outperforms SA baselines on a number of problems: Rosenbrock's function, the Knapsack problem, the Bin Packing problem, and the Travelling Salesperson problem. We also show that Neural SA scales well to large problems - generalising to significantly larger problems than the ones seen during training - while achieving comparable performance to popular off-the-shelf solvers and other machine learning methods in terms of solution quality and wall-clock time.", "paper_url": "http://arxiv.org/abs/2203.02201v1", "pdf_url": "http://arxiv.org/pdf/2203.02201v1", "repo_url": null}, "2203.03535": {"publish_time": "2022-03-07", "title": "Influencing Long-Term Behavior in Multiagent Reinforcement Learning", "author": "Dong-Ki Kim et.al.", "abstract": "The main challenge of multiagent reinforcement learning is the difficulty of learning useful policies in the presence of other simultaneously learning agents whose changing behaviors jointly affect the environment's transition and reward dynamics. An effective approach that has recently emerged for addressing this non-stationarity is for each agent to anticipate the learning of other interacting agents and influence the evolution of their future policies towards desirable behavior for its own benefit. Unfortunately, all previous approaches for achieving this suffer from myopic evaluation, considering only a few or a finite number of updates to the policies of other agents. In this paper, we propose a principled framework for considering the limiting policies of other agents as the time approaches infinity. Specifically, we develop a new optimization objective that maximizes each agent's average reward by directly accounting for the impact of its behavior on the limiting set of policies that other agents will take on. Thanks to our farsighted evaluation, we demonstrate better long-term performance than state-of-the-art baselines in various domains, including the full spectrum of general-sum, competitive, and cooperative settings.", "paper_url": "http://arxiv.org/abs/2203.03535v1", "pdf_url": "http://arxiv.org/pdf/2203.03535v1", "repo_url": null}, "2203.03480": {"publish_time": "2022-03-07", "title": "Reinforcement Learning for Location-Aware Scheduling", "author": "Stelios Stavroulakis et.al.", "abstract": "Recent techniques in dynamical scheduling and resource management have found applications in warehouse environments due to their ability to organize and prioritize tasks in a higher temporal resolution. The rise of deep reinforcement learning, as a learning paradigm, has enabled decentralized agent populations to discover complex coordination strategies. However, training multiple agents simultaneously introduce many obstacles in training as observation and action spaces become exponentially large. In our work, we experimentally quantify how various aspects of the warehouse environment (e.g., floor plan complexity, information about agents' live location, level of task parallelizability) affect performance and execution priority. To achieve efficiency, we propose a compact representation of the state and action space for location-aware multi-agent systems, wherein each agent has knowledge of only self and task coordinates, hence only partial observability of the underlying Markov Decision Process. Finally, we show how agents trained in certain environments maintain performance in completely unseen settings and also correlate performance degradation with floor plan geometry.", "paper_url": "http://arxiv.org/abs/2203.03480v1", "pdf_url": "http://arxiv.org/pdf/2203.03480v1", "repo_url": null}, "2203.03457": {"publish_time": "2022-03-07", "title": "Graph Neural Networks for Image Classification and Reinforcement Learning using Graph representations", "author": "Naman Goyal et.al.", "abstract": "In this paper, we will evaluate the performance of graph neural networks in two distinct domains: computer vision and reinforcement learning. In the computer vision section, we seek to learn whether a novel non-redundant representation for images as graphs can improve performance over trivial pixel to node mapping on a graph-level prediction graph, specifically image classification. For the reinforcement learning section, we seek to learn if explicitly modeling solving a Rubik's cube as a graph problem can improve performance over a standard model-free technique with no inductive bias.", "paper_url": "http://arxiv.org/abs/2203.03457v1", "pdf_url": "http://arxiv.org/pdf/2203.03457v1", "repo_url": null}, "2203.03451": {"publish_time": "2022-03-07", "title": "Safety Verification of Autonomous Systems: A Multi-Fidelity Reinforcement Learning Approach", "author": "Jared J. Beard et.al.", "abstract": "As autonomous and semi-autonomous agents become more integrated with society, validation of their safety is increasingly important. The scenarios under which they are used, however, can be quite complicated; as such, formal verification may be impossible. To this end, simulation-based safety verification is being used more frequently to understand failure scenarios for the most complex problems. Recent approaches, such as adaptive stress testing (AST), use reinforcement learning, making them prone to excessive exploitation of known failures, limiting coverage of the space of failures. To overcome this, the work below defines a class of Markov decision processes, the knowledge MDP, which captures information about the learned model to reason over. More specifically, by leveraging, the \"knows what it knows\" (KWIK) framework, the learner estimates its knowledge (model estimates and confidence, as well as assumptions) about the underlying system. This formulation is vetted through MF-KWIK-AST which extends bidirectional learning in multiple fidelities (MF) of simulators to the safety verification problem. The knowledge MDP formulation is applied to detect convergence of the model, penalizing this behavior to encourage further exploration. Results are evaluated in a grid world, training an adversary to intercept a system under test. Monte Carlo trials compare the relative sample efficiency of MF-KWIK-AST to learning with a single-fidelity simulator, as well as demonstrate the utility of incorporating knowledge about learned models into the decision making process.", "paper_url": "http://arxiv.org/abs/2203.03451v1", "pdf_url": "http://arxiv.org/pdf/2203.03451v1", "repo_url": null}, "2203.03417": {"publish_time": "2022-03-07", "title": "Scalable multi-agent reinforcement learning for distributed control of residential energy flexibility", "author": "Flora Charbonnier et.al.", "abstract": "This paper proposes a novel scalable type of multi-agent reinforcement learning-based coordination for distributed residential energy. Cooperating agents learn to control the flexibility offered by electric vehicles, space heating and flexible loads in a partially observable stochastic environment. In the standard independent Q-learning approach, the coordination performance of agents under partial observability drops at scale in stochastic environments. Here, the novel combination of learning from off-line convex optimisations on historical data and isolating marginal contributions to total rewards in reward signals increases stability and performance at scale. Using fixed-size Q-tables, prosumers are able to assess their marginal impact on total system objectives without sharing personal data either with each other or with a central coordinator. Case studies are used to assess the fitness of different combinations of exploration sources, reward definitions, and multi-agent learning frameworks. It is demonstrated that the proposed strategies create value at individual and system levels thanks to reductions in the costs of energy imports, losses, distribution network congestion, battery depreciation and greenhouse gas emissions.", "paper_url": "http://arxiv.org/abs/2203.03417v1", "pdf_url": "http://arxiv.org/pdf/2203.03417v1", "repo_url": null}, "2203.04272": {"publish_time": "2022-03-08", "title": "Policy-Based Bayesian Experimental Design for Non-Differentiable Implicit Models", "author": "Vincent Lim et.al.", "abstract": "For applications in healthcare, physics, energy, robotics, and many other fields, designing maximally informative experiments is valuable, particularly when experiments are expensive, time-consuming, or pose safety hazards. While existing approaches can sequentially design experiments based on prior observation history, many of these methods do not extend to implicit models, where simulation is possible but computing the likelihood is intractable. Furthermore, they often require either significant online computation during deployment or a differentiable simulation system. We introduce Reinforcement Learning for Deep Adaptive Design (RL-DAD), a method for simulation-based optimal experimental design for non-differentiable implicit models. RL-DAD extends prior work in policy-based Bayesian Optimal Experimental Design (BOED) by reformulating it as a Markov Decision Process with a reward function based on likelihood-free information lower bounds, which is used to learn a policy via deep reinforcement learning. The learned design policy maps prior histories to experiment designs offline and can be quickly deployed during online execution. We evaluate RL-DAD and find that it performs competitively with baselines on three benchmarks.", "paper_url": "http://arxiv.org/abs/2203.04272v1", "pdf_url": "http://arxiv.org/pdf/2203.04272v1", "repo_url": null}, "2203.04271": {"publish_time": "2022-03-08", "title": "Gradient Ascent Pulse Engineering with Feedback", "author": "Riccardo Porotti et.al.", "abstract": "Efficient approaches to quantum control and feedback are essential for quantum technologies, from sensing to quantum computation. Pure control tasks have been successfully solved using optimization techniques, including methods like gradient-ascent pulse engineering (GRAPE) , relying on a differentiable model of the quantum dynamics. For feedback tasks, such methods are not directly applicable, since the aim is to discover strategies conditioned on measurement outcomes. There, model-free reinforcement learning (RL) has recently proven a powerful new ansatz. What is missing is a way to combine the best of both approaches for scenarios that go beyond weak measurements. In this work, we introduce feedback-GRAPE, which borrows concepts from model-free RL to incorporate the response to strong stochastic (discrete or continuous) measurements, while still performing direct gradient ascent through the quantum dynamics. We illustrate its power on a Jaynes-Cummings model with feedback, where it yields interpretable feedback strategies for state preparation and stabilization in the presence of noise. This approach could be employed for discovering strategies in a wide range of feedback tasks, from calibration of multi-qubit devices to linear-optics quantum computation strategies, quantum-enhanced sensing with adaptive measurements, and quantum error correction.", "paper_url": "http://arxiv.org/abs/2203.04271v1", "pdf_url": "http://arxiv.org/pdf/2203.04271v1", "repo_url": null}, "2203.04236": {"publish_time": "2022-03-08", "title": "A Sharp Characterization of Linear Estimators for Offline Policy Evaluation", "author": "Juan C. Perdomo et.al.", "abstract": "Offline policy evaluation is a fundamental statistical problem in reinforcement learning that involves estimating the value function of some decision-making policy given data collected by a potentially different policy. In order to tackle problems with complex, high-dimensional observations, there has been significant interest from theoreticians and practitioners alike in understanding the possibility of function approximation in reinforcement learning. Despite significant study, a sharp characterization of when we might expect offline policy evaluation to be tractable, even in the simplest setting of linear function approximation, has so far remained elusive, with a surprising number of strong negative results recently appearing in the literature.   In this work, we identify simple control-theoretic and linear-algebraic conditions that are necessary and sufficient for classical methods, in particular Fitted Q-iteration (FQI) and least squares temporal difference learning (LSTD), to succeed at offline policy evaluation. Using this characterization, we establish a precise hierarchy of regimes under which these estimators succeed. We prove that LSTD works under strictly weaker conditions than FQI. Furthermore, we establish that if a problem is not solvable via LSTD, then it cannot be solved by a broad class of linear estimators, even in the limit of infinite data. Taken together, our results provide a complete picture of the behavior of linear estimators for offline policy evaluation (OPE), unify previously disparate analyses of canonical algorithms, and provide significantly sharper notions of the underlying statistical complexity of OPE.", "paper_url": "http://arxiv.org/abs/2203.04236v1", "pdf_url": "http://arxiv.org/pdf/2203.04236v1", "repo_url": null}, "2203.04227": {"publish_time": "2022-03-08", "title": "Learning based Age of Information Minimization in UAV-relayed IoT Networks", "author": "Biplav Choudhury et.al.", "abstract": "Unmanned Aerial Vehicles (UAVs) are used as aerial base-stations to relay time-sensitive packets from IoT devices to the nearby terrestrial base-station (TBS). Scheduling of packets in such UAV-relayed IoT-networks to ensure fresh (or up-to-date) IoT devices' packets at the TBS is a challenging problem as it involves two simultaneous steps of (i) sampling of packets generated at IoT devices by the UAVs [hop-1] and (ii) updating of sampled packets from UAVs to the TBS [hop-2]. To address this, we propose Age-of-Information (AoI) scheduling algorithms for two-hop UAV-relayed IoT-networks. First, we propose a low-complexity AoI scheduler, termed, MAF-MAD that employs Maximum AoI First (MAF) policy for sampling of IoT devices at UAV (hop-1) and Maximum AoI Difference (MAD) policy for updating sampled packets from UAV to the TBS (hop-2). We prove that MAF-MAD is the optimal AoI scheduler under ideal conditions (lossless wireless channels and generate-at-will traffic-generation at IoT devices). On the contrary, for general conditions (lossy channel conditions and varying periodic traffic-generation at IoT devices), a deep reinforcement learning algorithm, namely, Proximal Policy Optimization (PPO)-based scheduler is proposed. Simulation results show that the proposed PPO-based scheduler outperforms other schedulers like MAF-MAD, MAF, and round-robin in all considered general scenarios.", "paper_url": "http://arxiv.org/abs/2203.04227v1", "pdf_url": "http://arxiv.org/pdf/2203.04227v1", "repo_url": null}, "2203.04172": {"publish_time": "2022-03-08", "title": "Distributed Control using Reinforcement Learning with Temporal-Logic-Based Reward Shaping", "author": "Ningyuan Zhang et.al.", "abstract": "We present a computational framework for synthesis of distributed control strategies for a heterogeneous team of robots in a partially observable environment. The goal is to cooperatively satisfy specifications given as Truncated Linear Temporal Logic (TLTL) formulas. Our approach formulates the synthesis problem as a stochastic game and employs a policy graph method to find a control strategy with memory for each agent. We construct the stochastic game on the product between the team transition system and a finite state automaton (FSA) that tracks the satisfaction of the TLTL formula. We use the quantitative semantics of TLTL as the reward of the game, and further reshape it using the FSA to guide and accelerate the learning process. Simulation results demonstrate the efficacy of the proposed solution under demanding task specifications and the effectiveness of reward shaping in significantly accelerating the speed of learning.", "paper_url": "http://arxiv.org/abs/2203.04172v1", "pdf_url": "http://arxiv.org/pdf/2203.04172v1", "repo_url": null}, "2203.04927": {"publish_time": "2022-03-09", "title": "Investigation of Factorized Optical Flows as Mid-Level Representations", "author": "Hsuan-Kung Yang et.al.", "abstract": "In this paper, we introduce a new concept of incorporating factorized flow maps as mid-level representations, for bridging the perception and the control modules in modular learning based robotic frameworks. To investigate the advantages of factorized flow maps and examine their interplay with the other types of mid-level representations, we further develop a configurable framework, along with four different environments that contain both static and dynamic objects, for analyzing the impacts of factorized optical flow maps on the performance of deep reinforcement learning agents. Based on this framework, we report our experimental results on various scenarios, and offer a set of analyses to justify our hypothesis. Finally, we validate flow factorization in real world scenarios.", "paper_url": "http://arxiv.org/abs/2203.04927v1", "pdf_url": "http://arxiv.org/pdf/2203.04927v1", "repo_url": null}, "2203.04923": {"publish_time": "2022-03-09", "title": "On-Robot Policy Learning with $\\mathrm{O}(2)$-Equivariant SAC", "author": "Dian Wang et.al.", "abstract": "Recently, equivariant neural network models have been shown to be useful in improving sample efficiency for tasks in computer vision and reinforcement learning. This paper explores this idea in the context of on-robot policy learning where a policy must be learned entirely on a physical robotic system without reference to a model, a simulator, or an offline dataset. We focus on applications of $\\mathrm{SO}(2)$-Equivariant SAC to robotic manipulation and explore a number of variations of the algorithm. Ultimately, we demonstrate the ability to learn several non-trivial manipulation tasks completely through on-robot experiences in less than an hour or two of wall clock time.", "paper_url": "http://arxiv.org/abs/2203.04923v1", "pdf_url": "http://arxiv.org/pdf/2203.04923v1", "repo_url": null}, "2203.04857": {"publish_time": "2022-03-09", "title": "Neuro-symbolic Natural Logic with Introspective Revision for Natural Language Inference", "author": "Yufei Feng et.al.", "abstract": "We introduce a neuro-symbolic natural logic framework based on reinforcement learning with introspective revision. The model samples and rewards specific reasoning paths through policy gradient, in which the introspective revision algorithm modifies intermediate symbolic reasoning steps to discover reward-earning operations as well as leverages external knowledge to alleviate spurious reasoning and training inefficiency. The framework is supported by properly designed local relation models to avoid input entangling, which helps ensure the interpretability of the proof paths. The proposed model has built-in interpretability and shows superior capability in monotonicity inference, systematic generalization, and interpretability, compared to previous models on the existing datasets.", "paper_url": "http://arxiv.org/abs/2203.04857v1", "pdf_url": "http://arxiv.org/pdf/2203.04857v1", "repo_url": null}, "2203.04791": {"publish_time": "2022-03-09", "title": "Dimensionality Reduction and Prioritized Exploration for Policy Search", "author": "Marius Memmel et.al.", "abstract": "Black-box policy optimization is a class of reinforcement learning algorithms that explores and updates the policies at the parameter level. This class of algorithms is widely applied in robotics with movement primitives or non-differentiable policies. Furthermore, these approaches are particularly relevant where exploration at the action level could cause actuator damage or other safety issues. However, Black-box optimization does not scale well with the increasing dimensionality of the policy, leading to high demand for samples, which are expensive to obtain in real-world systems. In many practical applications, policy parameters do not contribute equally to the return. Identifying the most relevant parameters allows to narrow down the exploration and speed up the learning. Furthermore, updating only the effective parameters requires fewer samples, improving the scalability of the method. We present a novel method to prioritize the exploration of effective parameters and cope with full covariance matrix updates. Our algorithm learns faster than recent approaches and requires fewer samples to achieve state-of-the-art results. To select the effective parameters, we consider both the Pearson correlation coefficient and the Mutual Information. We showcase the capabilities of our approach on the Relative Entropy Policy Search algorithm in several simulated environments, including robotics simulations. Code is available at https://git.ias.informatik.tu-darmstadt.de/ias\\_code/aistats2022/dr-creps}{git.ias.informatik.tu-darmstadt.de/ias\\_code/aistats2022/dr-creps.", "paper_url": "http://arxiv.org/abs/2203.04791v1", "pdf_url": "http://arxiv.org/pdf/2203.04791v1", "repo_url": null}, "2203.04700": {"publish_time": "2022-03-09", "title": "Multi-robot Cooperative Pursuit via Potential Field-Enhanced Reinforcement Learning", "author": "Zheng Zhang et.al.", "abstract": "It is of great challenge, though promising, to coordinate collective robots for hunting an evader in a decentralized manner purely in light of local observations. In this paper, this challenge is addressed by a novel hybrid cooperative pursuit algorithm that combines reinforcement learning with the artificial potential field method. In the proposed algorithm, decentralized deep reinforcement learning is employed to learn cooperative pursuit policies that are adaptive to dynamic environments. The artificial potential field method is integrated into the learning process as predefined rules to improve the data efficiency and generalization ability. It is shown by numerical simulations that the proposed hybrid design outperforms the pursuit policies either learned from vanilla reinforcement learning or designed by the potential field method. Furthermore, experiments are conducted by transferring the learned pursuit policies into real-world mobile robots. Experimental results demonstrate the feasibility and potential of the proposed algorithm in learning multiple cooperative pursuit strategies.", "paper_url": "http://arxiv.org/abs/2203.04700v1", "pdf_url": "http://arxiv.org/pdf/2203.04700v1", "repo_url": null}, "2203.05449": {"publish_time": "2022-03-10", "title": "Artificial Intelligence in Vehicular Wireless Networks: A Case Study Using ns-3", "author": "Matteo Drago et.al.", "abstract": "Artificial intelligence (AI) techniques have emerged as a powerful approach to make wireless networks more efficient and adaptable. In this paper we present an ns-3 simulation framework, able to implement AI algorithms for the optimization of wireless networks. Our pipeline consists of: (i) a new geometry-based mobility-dependent channel model for V2X; (ii) all the layers of a 5G-NR-compliant protocol stack, based on the ns3-mmwave module; (iii) a new application to simulate V2X data transmission, and (iv) a new intelligent entity for the control of the network via AI. Thanks to its flexible and modular design, researchers can use this tool to implement, train, and evaluate their own algorithms in a realistic and controlled environment. We test the behavior of our framework in a Predictive Quality of Service (PQoS) scenario, where AI functionalities are implemented using Reinforcement Learning (RL), and demonstrate that it promotes better network optimization compared to baseline solutions that do not implement AI.", "paper_url": "http://arxiv.org/abs/2203.05449v1", "pdf_url": "http://arxiv.org/pdf/2203.05449v1", "repo_url": null}, "2203.05434": {"publish_time": "2022-03-10", "title": "Near-optimal Deep Reinforcement Learning Policies from Data for Zone Temperature Control", "author": "Loris Di Natale et.al.", "abstract": "Replacing poorly performing existing controllers with smarter solutions will decrease the energy intensity of the building sector. Recently, controllers based on Deep Reinforcement Learning (DRL) have been shown to be more effective than conventional baselines. However, since the optimal solution is usually unknown, it is still unclear if DRL agents are attaining near-optimal performance in general or if there is still a large gap to bridge.   In this paper, we investigate the performance of DRL agents compared to the theoretically optimal solution. To that end, we leverage Physically Consistent Neural Networks (PCNNs) as simulation environments, for which optimal control inputs are easy to compute. Furthermore, PCNNs solely rely on data to be trained, avoiding the difficult physics-based modeling phase, while retaining physical consistency. Our results hint that DRL agents not only clearly outperform conventional rule-based controllers, they furthermore attain near-optimal performance.", "paper_url": "http://arxiv.org/abs/2203.05434v1", "pdf_url": "http://arxiv.org/pdf/2203.05434v1", "repo_url": null}, "2203.05360": {"publish_time": "2022-03-10", "title": "Deep Residual Reinforcement Learning based Autonomous Blimp Control", "author": "Yu Tang Liu et.al.", "abstract": "Blimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/ robot-perception-group/AutonomousBlimpDRL.", "paper_url": "http://arxiv.org/abs/2203.05360v1", "pdf_url": "http://arxiv.org/pdf/2203.05360v1", "repo_url": "https://github.com/robot-perception-group/autonomousblimpdrl"}, "2203.05285": {"publish_time": "2022-03-10", "title": "API: Boosting Multi-Agent Reinforcement Learning via Agent-Permutation-Invariant Networks", "author": "Xiaotian Hao et.al.", "abstract": "Multi-agent reinforcement learning suffers from poor sample efficiency due to the exponential growth of the state-action space. Considering a homogeneous multiagent system, a global state consisting of $m$ homogeneous components has $m!$ differently ordered representations, thus designing functions satisfying permutation invariant (PI) can reduce the state space by a factor of $\\frac{1}{m!}$. However, mainstream MARL algorithms ignore this property and learn over the original state space. To achieve PI, previous works including data augmentation based methods and embedding-sharing architecture based methods, suffer from training instability and limited model capacity. In this work, we propose two novel designs to achieve PI, while avoiding the above limitations. The first design permutes the same but differently ordered inputs back to the same order and the downstream networks only need to learn function mapping over fixed-ordering inputs instead of all permutations, which is much easier to train. The second design applies a hypernetwork to generate customized embedding for each component, which has higher representational capacity than the previous embedding-sharing method. Empirical results on the SMAC benchmark show that the proposed method achieves 100% win-rates in almost all hard and super-hard scenarios (never achieved before), and superior sample-efficiency than the state-of-the-art baselines by up to 400%.", "paper_url": "http://arxiv.org/abs/2203.05285v1", "pdf_url": "http://arxiv.org/pdf/2203.05285v1", "repo_url": "https://github.com/tju-drl-lab/api-marl"}, "2203.05194": {"publish_time": "2022-03-10", "title": "Learning Torque Control for Quadrupedal Locomotion", "author": "Shuxiao Chen et.al.", "abstract": "Reinforcement learning (RL) is a promising tool for developing controllers for quadrupedal locomotion. The design of most learning-based locomotion controllers adopts the joint position-based paradigm, wherein a low-frequency RL policy outputs target joint positions that are then tracked by a high-frequency proportional-derivative (PD) controller that outputs joint torques. However, the low frequency of such a policy hinders the advancement of highly dynamic locomotion behaviors. Moreover, determining the PD gains for optimal tracking performance is laborious and dependent on the task at hand. In this paper, we introduce a learning torque control framework for quadrupedal locomotion, which trains an RL policy that directly predicts joint torques at a high frequency, thus circumventing the use of PD controllers. We validate the proposed framework with extensive experiments where the robot is able to both traverse various terrains and resist external pushes, given user-specified commands. To our knowledge, this is the first attempt of learning torque control for quadrupedal locomotion with an end-to-end single neural network that has led to successful real-world experiments among recent research on learning-based quadrupedal locomotion which is mostly position-based.", "paper_url": "http://arxiv.org/abs/2203.05194v1", "pdf_url": "http://arxiv.org/pdf/2203.05194v1", "repo_url": null}, "2203.06173": {"publish_time": "2022-03-11", "title": "Masked Visual Pre-training for Motor Control", "author": "Tete Xiao et.al.", "abstract": "This paper shows that self-supervised visual pre-training from real-world images is effective for learning motor control tasks from pixels. We first train the visual representations by masked modeling of natural images. We then freeze the visual encoder and train neural network controllers on top with reinforcement learning. We do not perform any task-specific fine-tuning of the encoder; the same visual representations are used for all motor control tasks. To the best of our knowledge, this is the first self-supervised model to exploit real-world images at scale for motor control. To accelerate progress in learning from pixels, we contribute a benchmark suite of hand-designed tasks varying in movements, scenes, and robots. Without relying on labels, state-estimation, or expert demonstrations, we consistently outperform supervised encoders by up to 80% absolute success rate, sometimes even matching the oracle state performance. We also find that in-the-wild images, e.g., from YouTube or Egocentric videos, lead to better visual representations for various manipulation tasks than ImageNet images.", "paper_url": "http://arxiv.org/abs/2203.06173v1", "pdf_url": "http://arxiv.org/pdf/2203.06173v1", "repo_url": null}, "2203.06053": {"publish_time": "2022-03-11", "title": "A Machine Learning Approach for Prosumer Management in Intraday Electricity Markets", "author": "Saeed Mohammadi et.al.", "abstract": "Prosumer operators are dealing with extensive challenges to participate in short-term electricity markets while taking uncertainties into account. Challenges such as variation in demand, solar energy, wind power, and electricity prices as well as faster response time in intraday electricity markets. Machine learning approaches could resolve these challenges due to their ability to continuous learning of complex relations and providing a real-time response. Such approaches are applicable with presence of the high performance computing and big data. To tackle these challenges, a Markov decision process is proposed and solved with a reinforcement learning algorithm with proper observations and actions employing tabular Q-learning. Trained agent converges to a policy which is similar to the global optimal solution. It increases the prosumer's profit by 13.39% compared to the well-known stochastic optimization approach.", "paper_url": "http://arxiv.org/abs/2203.06053v1", "pdf_url": "http://arxiv.org/pdf/2203.06053v1", "repo_url": null}, "2203.05985": {"publish_time": "2022-03-11", "title": "Graph Neural Networks for Relational Inductive Bias in Vision-based Deep Reinforcement Learning of Robot Control", "author": "Marco Oliva et.al.", "abstract": "State-of-the-art reinforcement learning algorithms predominantly learn a policy from either a numerical state vector or images. Both approaches generally do not take structural knowledge of the task into account, which is especially prevalent in robotic applications and can benefit learning if exploited. This work introduces a neural network architecture that combines relational inductive bias and visual feedback to learn an efficient position control policy for robotic manipulation. We derive a graph representation that models the physical structure of the manipulator and combines the robot's internal state with a low-dimensional description of the visual scene generated by an image encoding network. On this basis, a graph neural network trained with reinforcement learning predicts joint velocities to control the robot. We further introduce an asymmetric approach of training the image encoder separately from the policy using supervised learning. Experimental results demonstrate that, for a 2-DoF planar robot in a geometrically simplistic 2D environment, a learned representation of the visual scene can replace access to the explicit coordinates of the reaching target without compromising on the quality and sample efficiency of the policy. We further show the ability of the model to improve sample efficiency for a 6-DoF robot arm in a visually realistic 3D environment.", "paper_url": "http://arxiv.org/abs/2203.05985v1", "pdf_url": "http://arxiv.org/pdf/2203.05985v1", "repo_url": null}, "2203.05804": {"publish_time": "2022-03-11", "title": "Near-optimal Offline Reinforcement Learning with Linear Representation: Leveraging Variance Information with Pessimism", "author": "Ming Yin et.al.", "abstract": "Offline reinforcement learning, which seeks to utilize offline/historical data to optimize sequential decision-making strategies, has gained surging prominence in recent studies. Due to the advantage that appropriate function approximators can help mitigate the sample complexity burden in modern reinforcement learning problems, existing endeavors usually enforce powerful function representation models (e.g. neural networks) to learn the optimal policies. However, a precise understanding of the statistical limits with function representations, remains elusive, even when such a representation is linear.   Towards this goal, we study the statistical limits of offline reinforcement learning with linear model representations. To derive the tight offline learning bound, we design the variance-aware pessimistic value iteration (VAPVI), which adopts the conditional variance information of the value function for time-inhomogeneous episodic linear Markov decision processes (MDPs). VAPVI leverages estimated variances of the value functions to reweight the Bellman residuals in the least-square pessimistic value iteration and provides improved offline learning bounds over the best-known existing results (whereas the Bellman residuals are equally weighted by design). More importantly, our learning bounds are expressed in terms of system quantities, which provide natural instance-dependent characterizations that previous results are short of. We hope our results draw a clearer picture of what offline learning should look like when linear representations are provided.", "paper_url": "http://arxiv.org/abs/2203.05804v1", "pdf_url": "http://arxiv.org/pdf/2203.05804v1", "repo_url": null}, "2203.05775": {"publish_time": "2022-03-11", "title": "Physics-informed Reinforcement Learning for Perception and Reasoning about Fluids", "author": "Beatriz Moya et.al.", "abstract": "Learning and reasoning about physical phenomena is still a challenge in robotics development, and computational sciences play a capital role in the search for accurate methods able to provide explanations for past events and rigorous forecasts of future situations. We propose a physics-informed reinforcement learning strategy for fluid perception and reasoning from observations. As a model problem, we take the sloshing phenomena of different fluids contained in a glass. Starting from full-field and high-resolution synthetic data for a particular fluid, we develop a method for the tracking (perception) and analysis (reasoning) of any previously unseen liquid whose free surface is observed with a commodity camera. This approach demonstrates the importance of physics and knowledge not only in data-driven (grey box) modeling but also in the correction for real physics adaptation in low data regimes and partial observations of the dynamics. The method here presented is extensible to other domains such as the development of cognitive digital twins, able to learn from observation of phenomena for which they have not been trained explicitly.", "paper_url": "http://arxiv.org/abs/2203.05775v1", "pdf_url": "http://arxiv.org/pdf/2203.05775v1", "repo_url": "https://github.com/beatrizmoya/rlfluidperception"}, "2203.07368": {"publish_time": "2022-03-14", "title": "The Efficacy of Pessimism in Asynchronous Q-Learning", "author": "Yuling Yan et.al.", "abstract": "This paper is concerned with the asynchronous form of Q-learning, which applies a stochastic approximation scheme to Markovian data samples. Motivated by the recent advances in offline reinforcement learning, we develop an algorithmic framework that incorporates the principle of pessimism into asynchronous Q-learning, which penalizes infrequently-visited state-action pairs based on suitable lower confidence bounds (LCBs). This framework leads to, among other things, improved sample efficiency and enhanced adaptivity in the presence of near-expert data. Our approach permits the observed data in some important scenarios to cover only partial state-action space, which is in stark contrast to prior theory that requires uniform coverage of all state-action pairs. When coupled with the idea of variance reduction, asynchronous Q-learning with LCB penalization achieves near-optimal sample complexity, provided that the target accuracy level is small enough. In comparison, prior works were suboptimal in terms of the dependency on the effective horizon even when i.i.d. sampling is permitted. Our results deliver the first theoretical support for the use of pessimism principle in the presence of Markovian non-i.i.d. data.", "paper_url": "http://arxiv.org/abs/2203.07368v1", "pdf_url": "http://arxiv.org/pdf/2203.07368v1", "repo_url": null}, "2203.07322": {"publish_time": "2022-03-14", "title": "Efficient Model-based Multi-agent Reinforcement Learning via Optimistic Equilibrium Computation", "author": "Pier Giuseppe Sessa et.al.", "abstract": "We consider model-based multi-agent reinforcement learning, where the environment transition model is unknown and can only be learned via expensive interactions with the environment. We propose H-MARL (Hallucinated Multi-Agent Reinforcement Learning), a novel sample-efficient algorithm that can efficiently balance exploration, i.e., learning about the environment, and exploitation, i.e., achieve good equilibrium performance in the underlying general-sum Markov game. H-MARL builds high-probability confidence intervals around the unknown transition model and sequentially updates them based on newly observed data. Using these, it constructs an optimistic hallucinated game for the agents for which equilibrium policies are computed at each round. We consider general statistical models (e.g., Gaussian processes, deep ensembles, etc.) and policy classes (e.g., deep neural networks), and theoretically analyze our approach by bounding the agents' dynamic regret. Moreover, we provide a convergence rate to the equilibria of the underlying Markov game. We demonstrate our approach experimentally on an autonomous driving simulation benchmark. H-MARL learns successful equilibrium policies after a few interactions with the environment and can significantly improve the performance compared to non-exploratory methods.", "paper_url": "http://arxiv.org/abs/2203.07322v1", "pdf_url": "http://arxiv.org/pdf/2203.07322v1", "repo_url": null}, "2203.07276": {"publish_time": "2022-03-14", "title": "FRL-FI: Transient Fault Analysis for Federated Reinforcement Learning-Based Navigation Systems", "author": "Zishen Wan et.al.", "abstract": "Swarm intelligence is being increasingly deployed in autonomous systems, such as drones and unmanned vehicles. Federated reinforcement learning (FRL), a key swarm intelligence paradigm where agents interact with their own environments and cooperatively learn a consensus policy while preserving privacy, has recently shown potential advantages and gained popularity. However, transient faults are increasing in the hardware system with continuous technology node scaling and can pose threats to FRL systems. Meanwhile, conventional redundancy-based protection methods are challenging to deploy on resource-constrained edge applications. In this paper, we experimentally evaluate the fault tolerance of FRL navigation systems at various scales with respect to fault models, fault locations, learning algorithms, layer types, communication intervals, and data types at both training and inference stages. We further propose two cost-effective fault detection and recovery techniques that can achieve up to 3.3x improvement in resilience with <2.7% overhead in FRL systems.", "paper_url": "http://arxiv.org/abs/2203.07276v1", "pdf_url": "http://arxiv.org/pdf/2203.07276v1", "repo_url": null}, "2203.07263": {"publish_time": "2022-03-14", "title": "Logical shadow tomography: Efficient estimation of error-mitigated observables", "author": "Hong-Ye Hu et.al.", "abstract": "We introduce a technique to estimate error-mitigated expectation values on noisy quantum computers. Our technique performs shadow tomography on a logical state to produce a memory-efficient classical reconstruction of the noisy density matrix. Using efficient classical post-processing, one can mitigate errors by projecting a general nonlinear function of the noisy density matrix into the codespace. The subspace expansion and virtual distillation can be viewed as special cases of the new framekwork. We show our method is favorable in the quantum and classical resources overhead. Relative to subspace expansion which requires $O\\left(2^{N} \\right)$ samples to estimate a logical Pauli observable with $[[N, k]]$ error correction code, our technique requires only $O\\left(4^{k} \\right)$ samples. Relative to virtual distillation, our technique can compute powers of the density matrix without additional copies of quantum states or quantum memory. We present numerical evidence using logical states encoded with up to sixty physical qubits and show fast convergence to error-free expectation values with only $10^5$ samples under 1% depolarizing noise.", "paper_url": "http://arxiv.org/abs/2203.07263v1", "pdf_url": "http://arxiv.org/pdf/2203.07263v1", "repo_url": null}, "2203.07196": {"publish_time": "2022-03-14", "title": "Optimising low-Reynolds-number predation via optimal control and reinforcement learning", "author": "Guangpu Zhu et.al.", "abstract": "We seek the best stroke sequences of a finite-size swimming predator chasing a non-motile point or finite--size prey at low Reynolds number. We use optimal control to seek the globally-optimal solutions for the former and RL for general situations. The predator is represented by a squirmer model that can translate forward and laterally, rotate and generate a stresslet flow. We identify the predator's best squirming sequences to achieve the time-optimal (TO) and efficiency-optimal (EO) predation. For a point prey, the TO squirmer executing translational motions favours a two-fold L-shaped trajectory that enables it to exploit the disturbance flow for accelerated predation; using a stresslet mode significantly expedites the EO predation, allowing the predator to catch the prey faster yet with lower energy consumption and higher predatory efficiency; the predator can harness its stresslet disturbance flow to suck the prey towards itself; compared to a translating predator, its compeer combining translation and rotation is less time--efficient, and the latter occasionally achieves the TO predation via retreating in order to advance. We also adopt RL to reproduce the globally-optimal predatory strategy of chasing a point prey, qualitatively capturing the crucial two--fold attribute of TO path. Using a numerically emulated RL environment, we explore the dependence of the optimal predatory path on the size of prey. Our results might provide useful information that help design synthetic microswimmers such as \\textit{in vivo} medical micro-robots capable of capturing and approaching objects in viscous flows.", "paper_url": "http://arxiv.org/abs/2203.07196v1", "pdf_url": "http://arxiv.org/pdf/2203.07196v1", "repo_url": null}, "2203.08098": {"publish_time": "2022-03-15", "title": "RB2: Robotic Manipulation Benchmarking with a Twist", "author": "Sudeep Dasari et.al.", "abstract": "Benchmarks offer a scientific way to compare algorithms using objective performance metrics. Good benchmarks have two features: (a) they should be widely useful for many research groups; (b) and they should produce reproducible findings. In robotic manipulation research, there is a trade-off between reproducibility and broad accessibility. If the benchmark is kept restrictive (fixed hardware, objects), the numbers are reproducible but the setup becomes less general. On the other hand, a benchmark could be a loose set of protocols (e.g. object sets) but the underlying variation in setups make the results non-reproducible. In this paper, we re-imagine benchmarking for robotic manipulation as state-of-the-art algorithmic implementations, alongside the usual set of tasks and experimental protocols. The added baseline implementations will provide a way to easily recreate SOTA numbers in a new local robotic setup, thus providing credible relative rankings between existing approaches and new work. However, these local rankings could vary between different setups. To resolve this issue, we build a mechanism for pooling experimental data between labs, and thus we establish a single global ranking for existing (and proposed) SOTA algorithms. Our benchmark, called Ranking-Based Robotics Benchmark (RB2), is evaluated on tasks that are inspired from clinically validated Southampton Hand Assessment Procedures. Our benchmark was run across two different labs and reveals several surprising findings. For example, extremely simple baselines like open-loop behavior cloning, outperform more complicated models (e.g. closed loop, RNN, Offline-RL, etc.) that are preferred by the field. We hope our fellow researchers will use RB2 to improve their research's quality and rigor.", "paper_url": "http://arxiv.org/abs/2203.08098v1", "pdf_url": "http://arxiv.org/pdf/2203.08098v1", "repo_url": null}, "2203.07889": {"publish_time": "2022-03-15", "title": "Comparing two samples through stochastic dominance: a graphical approach", "author": "Etor Arza et.al.", "abstract": "Non-deterministic measurements are common in real-world scenarios: the performance of a stochastic optimization algorithm or the total reward of a reinforcement learning agent in a chaotic environment are just two examples in which unpredictable outcomes are common. These measures can be modeled as random variables and compared among each other via their expected values or more sophisticated tools such as null hypothesis statistical tests. In this paper, we propose an alternative framework to visually compare two samples according to their estimated cumulative distribution functions. First, we introduce a dominance measure for two random variables that quantifies the proportion in which the cumulative distribution function of one of the random variables scholastically dominates the other one. Then, we present a graphical method that decomposes in quantiles i) the proposed dominance measure and ii) the probability that one of the random variables takes lower values than the other. With illustrative purposes, we re-evaluate the experimentation of an already published work with the proposed methodology and we show that additional conclusions (missed by the rest of the methods) can be inferred. Additionally, the software package RVCompare was created as a convenient way of applying and experimenting with the proposed framework.", "paper_url": "http://arxiv.org/abs/2203.07889v1", "pdf_url": "http://arxiv.org/pdf/2203.07889v1", "repo_url": "https://github.com/etorarza/supplementarypaperrvcompare"}, "2203.07709": {"publish_time": "2022-03-15", "title": "Adaptive Environment Modeling Based Reinforcement Learning for Collision Avoidance in Complex Scenes", "author": "Shuaijun Wang et.al.", "abstract": "The major challenges of collision avoidance for robot navigation in crowded scenes lie in accurate environment modeling, fast perceptions, and trustworthy motion planning policies. This paper presents a novel adaptive environment model based collision avoidance reinforcement learning (i.e., AEMCARL) framework for an unmanned robot to achieve collision-free motions in challenging navigation scenarios. The novelty of this work is threefold: (1) developing a hierarchical network of gated-recurrent-unit (GRU) for environment modeling; (2) developing an adaptive perception mechanism with an attention module; (3) developing an adaptive reward function for the reinforcement learning (RL) framework to jointly train the environment model, perception function and motion planning policy. The proposed method is tested with the Gym-Gazebo simulator and a group of robots (Husky and Turtlebot) under various crowded scenes. Both simulation and experimental results have demonstrated the superior performance of the proposed method over baseline methods.", "paper_url": "http://arxiv.org/abs/2203.07709v1", "pdf_url": "http://arxiv.org/pdf/2203.07709v1", "repo_url": "https://github.com/sjwang2015/aemcarl"}, "2203.07676": {"publish_time": "2022-03-15", "title": "An Introduction to Multi-Agent Reinforcement Learning and Review of its Application to Autonomous Mobility", "author": "Lukas M. Schmidt et.al.", "abstract": "Many scenarios in mobility and traffic involve multiple different agents that need to cooperate to find a joint solution. Recent advances in behavioral planning use Reinforcement Learning to find effective and performant behavior strategies. However, as autonomous vehicles and vehicle-to-X communications become more mature, solutions that only utilize single, independent agents leave potential performance gains on the road. Multi-Agent Reinforcement Learning (MARL) is a research field that aims to find optimal solutions for multiple agents that interact with each other. This work aims to give an overview of the field to researchers in autonomous mobility. We first explain MARL and introduce important concepts. Then, we discuss the central paradigms that underlie MARL algorithms, and give an overview of state-of-the-art methods and ideas in each paradigm. With this background, we survey applications of MARL in autonomous mobility scenarios and give an overview of existing scenarios and implementations.", "paper_url": "http://arxiv.org/abs/2203.07676v1", "pdf_url": "http://arxiv.org/pdf/2203.07676v1", "repo_url": null}, "2203.07632": {"publish_time": "2022-03-15", "title": "Graph Representation Learning for Popularity Prediction Problem: A Survey", "author": "Tiantian Chen et.al.", "abstract": "The online social platforms, like Twitter, Facebook, LinkedIn and WeChat, have grown really fast in last decade and have been one of the most effective platforms for people to communicate and share information with each other. Due to the \"word of mouth\" effects, information usually can spread rapidly on these social media platforms. Therefore, it is important to study the mechanisms driving the information diffusion and quantify the consequence of information spread. A lot of efforts have been focused on this problem to help us better understand and achieve higher performance in viral marketing and advertising. On the other hand, the development of neural networks has blossomed in the last few years, leading to a large number of graph representation learning (GRL) models. Compared to traditional models, GRL methods are often shown to be more effective. In this paper, we present a comprehensive review for existing works using GRL methods for popularity prediction problem, and categorize related literatures into two big classes, according to their mainly used model and techniques: embedding-based methods and deep learning methods. Deep learning method is further classified into six small classes: convolutional neural networks, graph convolutional networks, graph attention networks, graph neural networks, recurrent neural networks, and reinforcement learning. We compare the performance of these different models and discuss their strengths and limitations. Finally, we outline the challenges and future chances for popularity prediction problem.", "paper_url": "http://arxiv.org/abs/2203.07632v1", "pdf_url": "http://arxiv.org/pdf/2203.07632v1", "repo_url": null}, "2203.08553": {"publish_time": "2022-03-16", "title": "PMIC: Improving Multi-Agent Reinforcement Learning with Progressive Mutual Information Collaboration", "author": "Pengyi Li et.al.", "abstract": "Learning to collaborate is critical in multi-agent reinforcement learning (MARL). A number of previous works promote collaboration by maximizing the correlation of agents' behaviors, which is typically characterised by mutual information (MI) in different forms. However, in this paper, we reveal that strong correlation can emerge from sub-optimal collaborative behaviors, and simply maximizing the MI can, surprisingly, hinder the learning towards better collaboration. To address this issue, we propose a novel MARL framework, called Progressive Mutual Information Collaboration (PMIC), for more effective MI-driven collaboration. In PMIC, we use a new collaboration criterion measured by the MI between global states and joint actions. Based on the criterion, the key idea of PMIC is maximizing the MI associated with superior collaborative behaviors and minimizing the MI associated with inferior ones. The two MI objectives play complementary roles by facilitating learning towards better collaborations while avoiding falling into sub-optimal ones. Specifically, PMIC stores and progressively maintains sets of superior and inferior interaction experiences, from which dual MI neural estimators are established. Experiments on a wide range of MARL benchmarks show the superior performance of PMIC compared with other algorithms.", "paper_url": "http://arxiv.org/abs/2203.08553v1", "pdf_url": "http://arxiv.org/pdf/2203.08553v1", "repo_url": null}, "2203.08542": {"publish_time": "2022-03-16", "title": "Lazy-MDPs: Towards Interpretable Reinforcement Learning by Learning When to Act", "author": "Alexis Jacq et.al.", "abstract": "Traditionally, Reinforcement Learning (RL) aims at deciding how to act optimally for an artificial agent. We argue that deciding when to act is equally important. As humans, we drift from default, instinctive or memorized behaviors to focused, thought-out behaviors when required by the situation. To enhance RL agents with this aptitude, we propose to augment the standard Markov Decision Process and make a new mode of action available: being lazy, which defers decision-making to a default policy. In addition, we penalize non-lazy actions in order to encourage minimal effort and have agents focus on critical decisions only. We name the resulting formalism lazy-MDPs. We study the theoretical properties of lazy-MDPs, expressing value functions and characterizing optimal solutions. Then we empirically demonstrate that policies learned in lazy-MDPs generally come with a form of interpretability: by construction, they show us the states where the agent takes control over the default policy. We deem those states and corresponding actions important since they explain the difference in performance between the default and the new, lazy policy. With suboptimal policies as default (pretrained or random), we observe that agents are able to get competitive performance in Atari games while only taking control in a limited subset of states.", "paper_url": "http://arxiv.org/abs/2203.08542v1", "pdf_url": "http://arxiv.org/pdf/2203.08542v1", "repo_url": null}, "2203.08454": {"publish_time": "2022-03-16", "title": "Coach-assisted Multi-Agent Reinforcement Learning Framework for Unexpected Crashed Agents", "author": "Jian Zhao et.al.", "abstract": "Multi-agent reinforcement learning is difficult to be applied in practice, which is partially due to the gap between the simulated and real-world scenarios. One reason for the gap is that the simulated systems always assume that the agents can work normally all the time, while in practice, one or more agents may unexpectedly \"crash\" during the coordination process due to inevitable hardware or software failures. Such crashes will destroy the cooperation among agents, leading to performance degradation. In this work, we present a formal formulation of a cooperative multi-agent reinforcement learning system with unexpected crashes. To enhance the robustness of the system to crashes, we propose a coach-assisted multi-agent reinforcement learning framework, which introduces a virtual coach agent to adjust the crash rate during training. We design three coaching strategies and the re-sampling strategy for our coach agent. To the best of our knowledge, this work is the first to study the unexpected crashes in the multi-agent system. Extensive experiments on grid-world and StarCraft II micromanagement tasks demonstrate the efficacy of adaptive strategy compared with the fixed crash rate strategy and curriculum learning strategy. The ablation study further illustrates the effectiveness of our re-sampling strategy.", "paper_url": "http://arxiv.org/abs/2203.08454v1", "pdf_url": "http://arxiv.org/pdf/2203.08454v1", "repo_url": null}, "2203.08412": {"publish_time": "2022-03-16", "title": "CTDS: Centralized Teacher with Decentralized Student for Multi-Agent Reinforcement Learning", "author": "Jian Zhao et.al.", "abstract": "Due to the partial observability and communication constraints in many multi-agent reinforcement learning (MARL) tasks, centralized training with decentralized execution (CTDE) has become one of the most widely used MARL paradigms. In CTDE, centralized information is dedicated to learning the allocation of the team reward with a mixing network, while the learning of individual Q-values is usually based on local observations. The insufficient utility of global observation will degrade performance in challenging environments. To this end, this work proposes a novel Centralized Teacher with Decentralized Student (CTDS) framework, which consists of a teacher model and a student model. Specifically, the teacher model allocates the team reward by learning individual Q-values conditioned on global observation, while the student model utilizes the partial observations to approximate the Q-values estimated by the teacher model. In this way, CTDS balances the full utilization of global observation during training and the feasibility of decentralized execution for online inference. Our CTDS framework is generic which is ready to be applied upon existing CTDE methods to boost their performance. We conduct experiments on a challenging set of StarCraft II micromanagement tasks to test the effectiveness of our method and the results show that CTDS outperforms the existing value-based MARL methods.", "paper_url": "http://arxiv.org/abs/2203.08412v1", "pdf_url": "http://arxiv.org/pdf/2203.08412v1", "repo_url": null}, "2203.08409": {"publish_time": "2022-03-16", "title": "How to Learn from Risk: Explicit Risk-Utility Reinforcement Learning for Efficient and Safe Driving Strategies", "author": "Lukas M. Schmidt et.al.", "abstract": "Autonomous driving has the potential to revolutionize mobility and is hence an active area of research. In practice, the behavior of autonomous vehicles must be acceptable, i.e., efficient, safe, and interpretable. While vanilla reinforcement learning (RL) finds performant behavioral strategies, they are often unsafe and uninterpretable. Safety is introduced through Safe RL approaches, but they still mostly remain uninterpretable as the learned behaviour is jointly optimized for safety and performance without modeling them separately. Interpretable machine learning is rarely applied to RL. This paper proposes SafeDQN, which allows to make the behavior of autonomous vehicles safe and interpretable while still being efficient. SafeDQN offers an understandable, semantic trade-off between the expected risk and the utility of actions while being algorithmically transparent. We show that SafeDQN finds interpretable and safe driving policies for a variety of scenarios and demonstrate how state-of-the-art saliency techniques can help to assess both risk and utility.", "paper_url": "http://arxiv.org/abs/2203.08409v1", "pdf_url": "http://arxiv.org/pdf/2203.08409v1", "repo_url": null}, "2203.09498": {"publish_time": "2022-03-17", "title": "The Frost Hollow Experiments: Pavlovian Signalling as a Path to Coordination and Communication Between Agents", "author": "Patrick M. Pilarski et.al.", "abstract": "Learned communication between agents is a powerful tool when approaching decision-making problems that are hard to overcome by any single agent in isolation. However, continual coordination and communication learning between machine agents or human-machine partnerships remains a challenging open problem. As a stepping stone toward solving the continual communication learning problem, in this paper we contribute a multi-faceted study into what we term Pavlovian signalling -- a process by which learned, temporally extended predictions made by one agent inform decision-making by another agent with different perceptual access to their shared environment. We seek to establish how different temporal processes and representational choices impact Pavlovian signalling between learning agents. To do so, we introduce a partially observable decision-making domain we call the Frost Hollow. In this domain a prediction learning agent and a reinforcement learning agent are coupled into a two-part decision-making system that seeks to acquire sparse reward while avoiding time-conditional hazards. We evaluate two domain variations: 1) machine prediction and control learning in a linear walk, and 2) a prediction learning machine interacting with a human participant in a virtual reality environment. Our results showcase the speed of learning for Pavlovian signalling, the impact that different temporal representations do (and do not) have on agent-agent coordination, and how temporal aliasing impacts agent-agent and human-agent interactions differently. As a main contribution, we establish Pavlovian signalling as a natural bridge between fixed signalling paradigms and fully adaptive communication learning. Our results therefore point to an actionable, constructivist path towards continual communication learning between reinforcement learning agents, with potential impact in a range of real-world settings.", "paper_url": "http://arxiv.org/abs/2203.09498v1", "pdf_url": "http://arxiv.org/pdf/2203.09498v1", "repo_url": null}, "2203.09365": {"publish_time": "2022-03-17", "title": "Semi-Markov Offline Reinforcement Learning for Healthcare", "author": "Mehdi Fatemi et.al.", "abstract": "Reinforcement learning (RL) tasks are typically framed as Markov Decision Processes (MDPs), assuming that decisions are made at fixed time intervals. However, many applications of great importance, including healthcare, do not satisfy this assumption, yet they are commonly modelled as MDPs after an artificial reshaping of the data. In addition, most healthcare (and similar) problems are offline by nature, allowing for only retrospective studies. To address both challenges, we begin by discussing the Semi-MDP (SMDP) framework, which formally handles actions of variable timings. We next present a formal way to apply SMDP modifications to nearly any given value-based offline RL method. We use this theory to introduce three SMDP-based offline RL algorithms, namely, SDQN, SDDQN, and SBCQ. We then experimentally demonstrate that these SMDP-based algorithms learn the optimal policy in these variable-time environments, whereas un-directed modifications of MDP modelling lead to sub-optimal policies. Finally, we apply our new algorithms to a real-world offline dataset pertaining to warfarin dosing for stroke prevention and demonstrate similar results.", "paper_url": "http://arxiv.org/abs/2203.09365v1", "pdf_url": "http://arxiv.org/pdf/2203.09365v1", "repo_url": null}, "2203.09251": {"publish_time": "2022-03-17", "title": "Near Instance-Optimal PAC Reinforcement Learning for Deterministic MDPs", "author": "Andrea Tirinzoni et.al.", "abstract": "In probably approximately correct (PAC) reinforcement learning (RL), an agent is required to identify an $\\epsilon$-optimal policy with probability $1-\\delta$. While minimax optimal algorithms exist for this problem, its instance-dependent complexity remains elusive in episodic Markov decision processes (MDPs). In this paper, we propose the first (nearly) matching upper and lower bounds on the sample complexity of PAC RL in deterministic episodic MDPs with finite state and action spaces. In particular, our bounds feature a new notion of sub-optimality gap for state-action pairs that we call the deterministic return gap. While our instance-dependent lower bound is written as a linear program, our algorithms are very simple and do not require solving such an optimization problem during learning. Their design and analyses employ novel ideas, including graph-theoretical concepts such as minimum flows and maximum cuts, which we believe to shed new light on this problem.", "paper_url": "http://arxiv.org/abs/2203.09251v1", "pdf_url": "http://arxiv.org/pdf/2203.09251v1", "repo_url": null}, "2203.08975": {"publish_time": "2022-03-16", "title": "A Survey of Multi-Agent Reinforcement Learning with Communication", "author": "Changxi Zhu et.al.", "abstract": "Communication is an effective mechanism for coordinating the behavior of multiple agents. In the field of multi-agent reinforcement learning, agents can improve the overall learning performance and achieve their objectives by communication. Moreover, agents can communicate various types of messages, either to all agents or to specific agent groups, and through specific channels. With the growing body of research work in MARL with communication (Comm-MARL), there is lack of a systematic and structural approach to distinguish and classify existing Comm-MARL systems. In this paper, we survey recent works in the Comm-MARL field and consider various aspects of communication that can play a role in the design and development of multi-agent reinforcement learning systems. With these aspects in mind, we propose several dimensions along which Comm-MARL systems can be analyzed, developed, and compared.", "paper_url": "http://arxiv.org/abs/2203.08975v1", "pdf_url": "http://arxiv.org/pdf/2203.08975v1", "repo_url": null}, "2203.08949": {"publish_time": "2022-03-16", "title": "Latent-Variable Advantage-Weighted Policy Optimization for Offline RL", "author": "Xi Chen et.al.", "abstract": "Offline reinforcement learning methods hold the promise of learning policies from pre-collected datasets without the need to query the environment for new transitions. This setting is particularly well-suited for continuous control robotic applications for which online data collection based on trial-and-error is costly and potentially unsafe. In practice, offline datasets are often heterogeneous, i.e., collected in a variety of scenarios, such as data from several human demonstrators or from policies that act with different purposes. Unfortunately, such datasets can exacerbate the distribution shift between the behavior policy underlying the data and the optimal policy to be learned, leading to poor performance. To address this challenge, we propose to leverage latent-variable policies that can represent a broader class of policy distributions, leading to better adherence to the training data distribution while maximizing reward via a policy over the latent variable. As we empirically show on a range of simulated locomotion, navigation, and manipulation tasks, our method referred to as latent-variable advantage-weighted policy optimization (LAPO), improves the average performance of the next best-performing offline reinforcement learning methods by 49% on heterogeneous datasets, and by 8% on datasets with narrow and biased distributions.", "paper_url": "http://arxiv.org/abs/2203.08949v1", "pdf_url": "http://arxiv.org/pdf/2203.08949v1", "repo_url": null}, "2203.10050": {"publish_time": "2022-03-18", "title": "SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning", "author": "Jongjin Park et.al.", "abstract": "Preference-based reinforcement learning (RL) has shown potential for teaching agents to perform the target tasks without a costly, pre-defined reward function by learning the reward with a supervisor's preference between the two agent behaviors. However, preference-based learning often requires a large amount of human feedback, making it difficult to apply this approach to various applications. This data-efficiency problem, on the other hand, has been typically addressed by using unlabeled samples or data augmentation techniques in the context of supervised learning. Motivated by the recent success of these approaches, we present SURF, a semi-supervised reward learning framework that utilizes a large amount of unlabeled samples with data augmentation. In order to leverage unlabeled samples for reward learning, we infer pseudo-labels of the unlabeled samples based on the confidence of the preference predictor. To further improve the label-efficiency of reward learning, we introduce a new data augmentation that temporally crops consecutive subsequences from the original behaviors. Our experiments demonstrate that our approach significantly improves the feedback-efficiency of the state-of-the-art preference-based method on a variety of locomotion and robotic manipulation tasks.", "paper_url": "http://arxiv.org/abs/2203.10050v1", "pdf_url": "http://arxiv.org/pdf/2203.10050v1", "repo_url": null}, "2203.10045": {"publish_time": "2022-03-18", "title": "Risk-Sensitive Bayesian Games for Multi-Agent Reinforcement Learning under Policy Uncertainty", "author": "Hannes Eriksson et.al.", "abstract": "In stochastic games with incomplete information, the uncertainty is evoked by the lack of knowledge about a player's own and the other players' types, i.e. the utility function and the policy space, and also the inherent stochasticity of different players' interactions. In existing literature, the risk in stochastic games has been studied in terms of the inherent uncertainty evoked by the variability of transitions and actions. In this work, we instead focus on the risk associated with the \\textit{uncertainty over types}. We contrast this with the multi-agent reinforcement learning framework where the other agents have fixed stationary policies and investigate risk-sensitiveness due to the uncertainty about the other agents' adaptive policies. We propose risk-sensitive versions of existing algorithms proposed for risk-neutral stochastic games, such as Iterated Best Response (IBR), Fictitious Play (FP) and a general multi-objective gradient approach using dual ascent (DAPG). Our experimental analysis shows that risk-sensitive DAPG performs better than competing algorithms for both social welfare and general-sum stochastic games.", "paper_url": "http://arxiv.org/abs/2203.10045v1", "pdf_url": "http://arxiv.org/pdf/2203.10045v1", "repo_url": null}, "2203.10033": {"publish_time": "2022-03-18", "title": "Skill-based Multi-objective Reinforcement Learning of Industrial Robot Tasks with Planning and Knowledge Integration", "author": "Matthias Mayr et.al.", "abstract": "In modern industrial settings with small batch sizes it should be easy to set up a robot system for a new task. Strategies exist, e.g. the use of skills, but when it comes to handling forces and torques, these systems often fall short. We introduce an approach that provides a combination of task-level planning with targeted learning of scenario-specific parameters for skill-based systems. We propose the following pipeline: (1) the user provides a task goal in the planning language PDDL, (2) a plan (i.e., a sequence of skills) is generated and the learnable parameters of the skills are automatically identified. An operator then chooses (3) reward functions and hyperparameters for the learning process. Two aspects of our methodology are critical: (a) learning is tightly integrated with a knowledge framework to support symbolic planning and to provide priors for learning, (b) using multi-objective optimization. This can help to balance key performance indicators (KPIs) such as safety and task performance since they can often affect each other. We adopt a multi-objective Bayesian optimization approach and learn entirely in simulation. We demonstrate the efficacy and versatility of our approach by learning skill parameters for two different contact-rich tasks. We show their successful execution on a real 7-DOF KUKA-iiwa manipulator and outperform the manual parameterization by human robot operators.", "paper_url": "http://arxiv.org/abs/2203.10033v1", "pdf_url": "http://arxiv.org/pdf/2203.10033v1", "repo_url": null}, "2203.09844": {"publish_time": "2022-03-18", "title": "Reinforcement Learning Approach to Clear Paths of Robots in Elevator Environment", "author": "Wanli Ma et.al.", "abstract": "Efficiently using the space of an elevator for a service robot is very necessary, due to the need for reducing the amount of time caused by waiting for the next elevator. To solve this, we propose a hybrid approach that combines reinforcement learning (RL) with voice interaction for robot navigation in the scene of entering the elevator. RL provides robots with a high exploration ability to find a new clear path to enter the elevator compared to the traditional navigation methods such as Optimal Reciprocal Collision Avoidance (ORCA). The proposed method allows the robot to take an active clear path action towards the elevator whilst a crowd of people stands at the entrance of the elevator wherein there are still lots of space. This is done by embedding a clear path action (beep) into the RL framework, and the proposed navigation policy leads the robot to finish tasks efficiently and safely. Our model approach provides a great improvement in the success rate and reward of entering the elevator compared to state-of-the-art ORCA and RL navigation policy without beep.", "paper_url": "http://arxiv.org/abs/2203.09844v1", "pdf_url": "http://arxiv.org/pdf/2203.09844v1", "repo_url": null}, "2203.09809": {"publish_time": "2022-03-18", "title": "Proximal Policy Optimization with Adaptive Threshold for Symmetric Relative Density Ratio", "author": "Taisuke Kobayashi et.al.", "abstract": "Deep reinforcement learning (DRL) is one of the promising approaches for introducing robots into complicated environments. The recent remarkable progress of DRL stands on regularization of policy, which allows the policy to improve stably and efficiently. A popular method, so-called proximal policy optimization (PPO), and its variants constrain density ratio of the latest and baseline policies when the density ratio exceeds a given threshold. This threshold can be designed relatively intuitively, and in fact its recommended value range has been suggested. However, the density ratio is asymmetric for its center, and the possible error scale from its center, which should be close to the threshold, would depend on how the baseline policy is given. In order to maximize the values of regularization of policy, this paper proposes a new PPO derived using relative Pearson (RPE) divergence, therefore so-called PPO-RPE, to design the threshold adaptively. In PPO-RPE, the relative density ratio, which can be formed with symmetry, replaces the raw density ratio. Thanks to this symmetry, its error scale from center can easily be estimated, hence, the threshold can be adapted for the estimated error scale. From three simple benchmark simulations, the importance of algorithm-dependent threshold design is revealed. By simulating additional four locomotion tasks, it is verified that the proposed method statistically contributes to task accomplishment by appropriately restricting the policy updates.", "paper_url": "http://arxiv.org/abs/2203.09809v1", "pdf_url": "http://arxiv.org/pdf/2203.09809v1", "repo_url": null}, "2203.11176": {"publish_time": "2022-03-21", "title": "One After Another: Learning Incremental Skills for a Changing World", "author": "Nur Muhammad Shafiullah et.al.", "abstract": "Reward-free, unsupervised discovery of skills is an attractive alternative to the bottleneck of hand-designing rewards in environments where task supervision is scarce or expensive. However, current skill pre-training methods, like many RL techniques, make a fundamental assumption - stationary environments during training. Traditional methods learn all their skills simultaneously, which makes it difficult for them to both quickly adapt to changes in the environment, and to not forget earlier skills after such adaptation. On the other hand, in an evolving or expanding environment, skill learning must be able to adapt fast to new environment situations while not forgetting previously learned skills. These two conditions make it difficult for classic skill discovery to do well in an evolving environment. In this work, we propose a new framework for skill discovery, where skills are learned one after another in an incremental fashion. This framework allows newly learned skills to adapt to new environment or agent dynamics, while the fixed old skills ensure the agent doesn't forget a learned skill. We demonstrate experimentally that in both evolving and static environments, incremental skills significantly outperform current state-of-the-art skill discovery methods on both skill quality and the ability to solve downstream tasks. Videos for learned skills and code are made public on https://notmahi.github.io/disk", "paper_url": "http://arxiv.org/abs/2203.11176v1", "pdf_url": "http://arxiv.org/pdf/2203.11176v1", "repo_url": null}, "2203.11147": {"publish_time": "2022-03-21", "title": "Teaching language models to support answers with verified quotes", "author": "Jacob Menick et.al.", "abstract": "Recent large language models often answer factual questions correctly. But users can't trust any given claim a model makes without fact-checking, because language models can hallucinate convincing nonsense. In this work we use reinforcement learning from human preferences (RLHP) to train \"open-book\" QA models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness. Supporting evidence is drawn from multiple documents found via a search engine, or from a single user-provided document. Our 280 billion parameter model, GopherCite, is able to produce answers with high quality supporting evidence and abstain from answering when unsure. We measure the performance of GopherCite by conducting human evaluation of answers to questions in a subset of the NaturalQuestions and ELI5 datasets. The model's response is found to be high-quality 80\\% of the time on this Natural Questions subset, and 67\\% of the time on the ELI5 subset. Abstaining from the third of questions for which it is most unsure improves performance to 90\\% and 80\\% respectively, approaching human baselines. However, analysis on the adversarial TruthfulQA dataset shows why citation is only one part of an overall strategy for safety and trustworthiness: not all claims supported by evidence are true.", "paper_url": "http://arxiv.org/abs/2203.11147v1", "pdf_url": "http://arxiv.org/pdf/2203.11147v1", "repo_url": null}, "2203.10949": {"publish_time": "2022-03-21", "title": "Optimizing Trajectories for Highway Driving with Offline Reinforcement Learning", "author": "Branka Mirchevska et.al.", "abstract": "Implementing an autonomous vehicle that is able to output feasible, smooth and efficient trajectories is a long-standing challenge. Several approaches have been considered, roughly falling under two categories: rule-based and learning-based approaches. The rule-based approaches, while guaranteeing safety and feasibility, fall short when it comes to long-term planning and generalization. The learning-based approaches are able to account for long-term planning and generalization to unseen situations, but may fail to achieve smoothness, safety and the feasibility which rule-based approaches ensure. Hence, combining the two approaches is an evident step towards yielding the best compromise out of both. We propose a Reinforcement Learning-based approach, which learns target trajectory parameters for fully autonomous driving on highways. The trained agent outputs continuous trajectory parameters based on which a feasible polynomial-based trajectory is generated and executed. We compare the performance of our agent against four other highway driving agents. The experiments are conducted in the Sumo simulator, taking into consideration various realistic, dynamically changing highway scenarios, including surrounding vehicles with different driver behaviors. We demonstrate that our offline trained agent, with randomly collected data, learns to drive smoothly, achieving velocities as close as possible to the desired velocity, while outperforming the other agents. Code, training data and details available at: https://nrgit.informatik.uni-freiburg. de/branka.mirchevska/offline-rl-tp.", "paper_url": "http://arxiv.org/abs/2203.10949v1", "pdf_url": "http://arxiv.org/pdf/2203.10949v1", "repo_url": null}, "2203.10905": {"publish_time": "2022-03-21", "title": "Self-Imitation Learning from Demonstrations", "author": "Georgiy Pshikhachev et.al.", "abstract": "Despite the numerous breakthroughs achieved with Reinforcement Learning (RL), solving environments with sparse rewards remains a challenging task that requires sophisticated exploration. Learning from Demonstrations (LfD) remedies this issue by guiding the agent's exploration towards states experienced by an expert. Naturally, the benefits of this approach hinge on the quality of demonstrations, which are rarely optimal in realistic scenarios. Modern LfD algorithms require meticulous tuning of hyperparameters that control the influence of demonstrations and, as we show in the paper, struggle with learning from suboptimal demonstrations. To address these issues, we extend Self-Imitation Learning (SIL), a recent RL algorithm that exploits the agent's past good experience, to the LfD setup by initializing its replay buffer with demonstrations. We denote our algorithm as SIL from Demonstrations (SILfD). We empirically show that SILfD can learn from demonstrations that are noisy or far from optimal and can automatically adjust the influence of demonstrations throughout the training without additional hyperparameters or handcrafted schedules. We also find SILfD superior to the existing state-of-the-art LfD algorithms in sparse environments, especially when demonstrations are highly suboptimal.", "paper_url": "http://arxiv.org/abs/2203.10905v1", "pdf_url": "http://arxiv.org/pdf/2203.10905v1", "repo_url": null}, "2203.10844": {"publish_time": "2022-03-21", "title": "Lean Evolutionary Reinforcement Learning by Multitasking with Importance Sampling", "author": "Nick Zhang et.al.", "abstract": "Studies have shown evolution strategies (ES) to be a promising approach for reinforcement learning (RL) with deep neural networks. However, the issue of high sample complexity persists in applications of ES to deep RL. In this paper, we address the shortcoming of today's methods via a novel neuroevolutionary multitasking (NuEMT) algorithm, designed to transfer information from a set of auxiliary tasks (of short episode length) to the target (full length) RL task at hand. The artificially generated auxiliary tasks allow an agent to update and quickly evaluate policies on shorter time horizons. The evolved skills are then transferred to guide the longer and harder task towards an optimal policy. We demonstrate that the NuEMT algorithm achieves data-lean evolutionary RL, reducing expensive agent-environment interaction data requirements. Our key algorithmic contribution in this setting is to introduce, for the first time, a multitask information transfer mechanism based on the statistical importance sampling technique. In addition, an adaptive resource allocation strategy is utilized to assign computational resources to auxiliary tasks based on their gleaned usefulness. Experiments on a range of continuous control tasks from the OpenAI Gym confirm that our proposed algorithm is efficient compared to recent ES baselines.", "paper_url": "http://arxiv.org/abs/2203.10844v1", "pdf_url": "http://arxiv.org/pdf/2203.10844v1", "repo_url": null}, "2203.11889": {"publish_time": "2022-03-22", "title": "Insights From the NeurIPS 2021 NetHack Challenge", "author": "Eric Hambro et.al.", "abstract": "In this report, we summarize the takeaways from the first NeurIPS 2021 NetHack Challenge. Participants were tasked with developing a program or agent that can win (i.e., 'ascend' in) the popular dungeon-crawler game of NetHack by interacting with the NetHack Learning Environment (NLE), a scalable, procedurally generated, and challenging Gym environment for reinforcement learning (RL). The challenge showcased community-driven progress in AI with many diverse approaches significantly beating the previously best results on NetHack. Furthermore, it served as a direct comparison between neural (e.g., deep RL) and symbolic AI, as well as hybrid systems, demonstrating that on NetHack symbolic bots currently outperform deep RL by a large margin. Lastly, no agent got close to winning the game, illustrating NetHack's suitability as a long-term benchmark for AI research.", "paper_url": "http://arxiv.org/abs/2203.11889v1", "pdf_url": "http://arxiv.org/pdf/2203.11889v1", "repo_url": null}, "2203.11842": {"publish_time": "2022-03-22", "title": "X-MEN: Guaranteed XOR-Maximum Entropy Constrained Inverse Reinforcement Learning", "author": "Fan Ding et.al.", "abstract": "Inverse Reinforcement Learning (IRL) is a powerful way of learning from demonstrations. In this paper, we address IRL problems with the availability of prior knowledge that optimal policies will never violate certain constraints. Conventional approaches ignoring these constraints need many demonstrations to converge. We propose XOR-Maximum Entropy Constrained Inverse Reinforcement Learning (X-MEN), which is guaranteed to converge to the optimal policy in linear rate w.r.t. the number of learning iterations. X-MEN embeds XOR-sampling -- a provable sampling approach that transforms the #P complete sampling problem into queries to NP oracles -- into the framework of maximum entropy IRL. X-MEN also guarantees the learned policy will never generate trajectories that violate constraints. Empirical results in navigation demonstrate that X-MEN converges faster to the optimal policies compared to baseline approaches and always generates trajectories that satisfy multi-state combinatorial constraints.", "paper_url": "http://arxiv.org/abs/2203.11842v1", "pdf_url": "http://arxiv.org/pdf/2203.11842v1", "repo_url": null}, "2203.11758": {"publish_time": "2022-03-22", "title": "Linear convergence of a policy gradient method for finite horizon continuous time stochastic control problems", "author": "Christoph Reisinger et.al.", "abstract": "Despite its popularity in the reinforcement learning community, a provably convergent policy gradient method for general continuous space-time stochastic control problems has been elusive. This paper closes the gap by proposing a proximal gradient algorithm for feedback controls of finite-time horizon stochastic control problems. The state dynamics are continuous time nonlinear diffusions with controlled drift and possibly degenerate noise, and the objectives are nonconvex in the state and nonsmooth in the control. We prove under suitable conditions that the algorithm converges linearly to a stationary point of the control problem, and is stable with respect to policy updates by approximate gradient steps. The convergence result justifies the recent reinforcement learning heuristics that adding entropy regularization to the optimization objective accelerates the convergence of policy gradient methods. The proof exploits careful regularity estimates of backward stochastic differential equations.", "paper_url": "http://arxiv.org/abs/2203.11758v1", "pdf_url": "http://arxiv.org/pdf/2203.11758v1", "repo_url": null}, "2203.11658": {"publish_time": "2022-03-22", "title": "A Decentralised Multi-Agent Reinforcement Learning Approach for the Same-Day Delivery Problem", "author": "Elvin Ngu et.al.", "abstract": "Same-Day Delivery services are becoming increasingly popular in recent years. These have been usually modelled by previous studies as a certain class of Dynamic Vehicle Routing Problem (DVRP) where goods must be delivered from a depot to a set of customers in the same day that the orders were placed. Adaptive exact solution methods for DVRPs can become intractable even for small problem instances. In this paper, we formulate the SDDP as a Markov Decision Process (MDP) and solve it using a parameter-sharing Deep Q-Network, which corresponds to a decentralised Multi-Agent Reinforcement Learning (MARL) approach. For this, we create a multi-agent grid-based SDD environment, consisting of multiple vehicles, a central depot and dynamic order generation. In addition, we introduce zone-specific order generation and reward probabilities. We compare the performance of our proposed MARL approach against a Mixed Inter Programming (MIP) solution. Results show that our proposed MARL framework performs on par with MIP-based policy when the number of orders is relatively low. For problem instances with higher order arrival rates, computational results show that the MARL approach underperforms the MIP by up to 30%. The performance gap between both methods becomes smaller when zone-specific parameters are employed. The gap is reduced from 30% to 3% for a 5x5 grid scenario with 30 orders. Execution time results indicate that the MARL approach is, on average, 65 times faster than the MIP-based policy, and therefore may be more advantageous for real-time control, at least for small-sized instances.", "paper_url": "http://arxiv.org/abs/2203.11658v1", "pdf_url": "http://arxiv.org/pdf/2203.11658v1", "repo_url": null}, "2203.11656": {"publish_time": "2022-03-22", "title": "Is Vanilla Policy Gradient Overlooked? Analyzing Deep Reinforcement Learning for Hanabi", "author": "Bram Grooten et.al.", "abstract": "In pursuit of enhanced multi-agent collaboration, we analyze several on-policy deep reinforcement learning algorithms in the recently published Hanabi benchmark. Our research suggests a perhaps counter-intuitive finding, where Proximal Policy Optimization (PPO) is outperformed by Vanilla Policy Gradient over multiple random seeds in a simplified environment of the multi-agent cooperative card game. In our analysis of this behavior we look into Hanabi-specific metrics and hypothesize a reason for PPO's plateau. In addition, we provide proofs for the maximum length of a perfect game (71 turns) and any game (89 turns). Our code can be found at: https://github.com/bramgrooten/DeepRL-for-Hanabi", "paper_url": "http://arxiv.org/abs/2203.11656v1", "pdf_url": "http://arxiv.org/pdf/2203.11656v1", "repo_url": "https://github.com/bramgrooten/deeprl-for-hanabi"}, "2203.12592": {"publish_time": "2022-03-23", "title": "Your Policy Regularizer is Secretly an Adversary", "author": "Rob Brekelmans et.al.", "abstract": "Policy regularization methods such as maximum entropy regularization are widely used in reinforcement learning to improve the robustness of a learned policy. In this paper, we show how this robustness arises from hedging against worst-case perturbations of the reward function, which are chosen from a limited set by an imagined adversary. Using convex duality, we characterize this robust set of adversarial reward perturbations under KL- and {\\alpha}-divergence regularization, which includes Shannon and Tsallis entropy regularization as special cases. Importantly, generalization guarantees can be given within this robust set. We provide detailed discussion of the worst-case reward perturbations, and present intuitive empirical examples to illustrate this robustness and its relationship with generalization. Finally, we discuss how our analysis complements and extends previous results on adversarial reward robustness and path consistency optimality conditions.", "paper_url": "http://arxiv.org/abs/2203.12592v1", "pdf_url": "http://arxiv.org/pdf/2203.12592v1", "repo_url": null}, "2203.12315": {"publish_time": "2022-03-23", "title": "Resource allocation optimization using artificial intelligence methods in various computing paradigms: A Review", "author": "Javad Hassannataj Joloudari et.al.", "abstract": "With the advent of smart devices, the demand for various computational paradigms such as the Internet of Things, fog, and cloud computing has increased. However, effective resource allocation remains challenging in these paradigms. This paper presents a comprehensive literature review on the application of artificial intelligence (AI) methods such as deep learning (DL) and machine learning (ML) for resource allocation optimization in computational paradigms. To the best of our knowledge, there are no existing reviews on AI-based resource allocation approaches in different computational paradigms. The reviewed ML-based approaches are categorized as supervised and reinforcement learning (RL). Moreover, DL-based approaches and their combination with RL are surveyed. The review ends with a discussion on open research directions and a conclusion.", "paper_url": "http://arxiv.org/abs/2203.12315v1", "pdf_url": "http://arxiv.org/pdf/2203.12315v1", "repo_url": null}, "2203.12299": {"publish_time": "2022-03-23", "title": "NavDreams: Towards Camera-Only RL Navigation Among Humans", "author": "Daniel Dugas et.al.", "abstract": "Autonomously navigating a robot in everyday crowded spaces requires solving complex perception and planning challenges. When using only monocular image sensor data as input, classical two-dimensional planning approaches cannot be used. While images present a significant challenge when it comes to perception and planning, they also allow capturing potentially important details, such as complex geometry, body movement, and other visual cues. In order to successfully solve the navigation task from only images, algorithms must be able to model the scene and its dynamics using only this channel of information. We investigate whether the world model concept, which has shown state-of-the-art results for modeling and learning policies in Atari games as well as promising results in 2D LiDAR-based crowd navigation, can also be applied to the camera-based navigation problem. To this end, we create simulated environments where a robot must navigate past static and moving humans without colliding in order to reach its goal. We find that state-of-the-art methods are able to achieve success in solving the navigation problem, and can generate dream-like predictions of future image-sequences which show consistent geometry and moving persons. We are also able to show that policy performance in our high-fidelity sim2real simulation scenario transfers to the real world by testing the policy on a real robot. We make our simulator, models and experiments available at https://github.com/danieldugas/NavDreams.", "paper_url": "http://arxiv.org/abs/2203.12299v1", "pdf_url": "http://arxiv.org/pdf/2203.12299v1", "repo_url": "https://github.com/danieldugas/navdreams"}, "2203.12133": {"publish_time": "2022-03-23", "title": "Congestion-aware motion planning game with Markov decision process dynamics", "author": "Sarah H. Q. Li et.al.", "abstract": "To model a competitive multi-agent motion planning scenario for heterogeneous players with distinct dynamics and objectives, we propose a novel atomic congestion game framework with Markov decision process (MDP) dynamics. We assume that all players share the state-action space but have unique costs and transition dynamics. Each player's cost is a function of the joint state-action density, and thus is implicitly coupled to the opponents' policies. With proper cost design, the resulting Nash equilibrium avoids congesting the state-action space and each player optimally achieves its individual objective with respect to the presence of its opponents. For a class of player cost functions, we demonstrate equivalence between players' Q-value functions and the KKT point of a potential minimization problem, and derive sufficient conditions for the existence of a unique Nash equilibrium. Finally, we outline a learning algorithm for finding the Nash equilibrium that extends single-agent MDP/reinforcement learning algorithms and has linear complexity in the number of players. Throughout the paper, we provide examples for multi-agent motion planning, accumulating in a multi-robot warehouse demonstration, in which robots autonomously retrieve and deliver packages while avoiding collisions.", "paper_url": "http://arxiv.org/abs/2203.12133v1", "pdf_url": "http://arxiv.org/pdf/2203.12133v1", "repo_url": null}, "2203.12117": {"publish_time": "2022-03-23", "title": "NovGrid: A Flexible Grid World for Evaluating Agent Response to Novelty", "author": "Jonathan Balloch et.al.", "abstract": "A robust body of reinforcement learning techniques have been developed to solve complex sequential decision making problems. However, these methods assume that train and evaluation tasks come from similarly or identically distributed environments. This assumption does not hold in real life where small novel changes to the environment can make a previously learned policy fail or introduce simpler solutions that might never be found. To that end we explore the concept of {\\em novelty}, defined in this work as the sudden change to the mechanics or properties of environment. We provide an ontology of for novelties most relevant to sequential decision making, which distinguishes between novelties that affect objects versus actions, unary properties versus non-unary relations, and the distribution of solutions to a task. We introduce NovGrid, a novelty generation framework built on MiniGrid, acting as a toolkit for rapidly developing and evaluating novelty-adaptation-enabled reinforcement learning techniques. Along with the core NovGrid we provide exemplar novelties aligned with our ontology and instantiate them as novelty templates that can be applied to many MiniGrid-compliant environments. Finally, we present a set of metrics built into our framework for the evaluation of novelty-adaptation-enabled machine-learning techniques, and show characteristics of a baseline RL model using these metrics.", "paper_url": "http://arxiv.org/abs/2203.12117v1", "pdf_url": "http://arxiv.org/pdf/2203.12117v1", "repo_url": null}, "2203.13251": {"publish_time": "2022-03-24", "title": "Dexterous Imitation Made Easy: A Learning-Based Framework for Efficient Dexterous Manipulation", "author": "Sridhar Pandian Arunachalam et.al.", "abstract": "Optimizing behaviors for dexterous manipulation has been a longstanding challenge in robotics, with a variety of methods from model-based control to model-free reinforcement learning having been previously explored in literature. Perhaps one of the most powerful techniques to learn complex manipulation strategies is imitation learning. However, collecting and learning from demonstrations in dexterous manipulation is quite challenging. The complex, high-dimensional action-space involved with multi-finger control often leads to poor sample efficiency of learning-based methods. In this work, we propose 'Dexterous Imitation Made Easy' (DIME) a new imitation learning framework for dexterous manipulation. DIME only requires a single RGB camera to observe a human operator and teleoperate our robotic hand. Once demonstrations are collected, DIME employs standard imitation learning methods to train dexterous manipulation policies. On both simulation and real robot benchmarks we demonstrate that DIME can be used to solve complex, in-hand manipulation tasks such as 'flipping', 'spinning', and 'rotating' objects with the Allegro hand. Our framework along with pre-collected demonstrations is publicly available at https://nyu-robot-learning.github.io/dime.", "paper_url": "http://arxiv.org/abs/2203.13251v1", "pdf_url": "http://arxiv.org/pdf/2203.13251v1", "repo_url": null}, "2203.13055": {"publish_time": "2022-03-24", "title": "Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory", "author": "Li Siyao et.al.", "abstract": "Driving 3D characters to dance following a piece of music is highly challenging due to the spatial constraints applied to poses by choreography norms. In addition, the generated dance sequence also needs to maintain temporal coherency with different music genres. To tackle these challenges, we propose a novel music-to-dance framework, Bailando, with two powerful components: 1) a choreographic memory that learns to summarize meaningful dancing units from 3D pose sequence to a quantized codebook, 2) an actor-critic Generative Pre-trained Transformer (GPT) that composes these units to a fluent dance coherent to the music. With the learned choreographic memory, dance generation is realized on the quantized units that meet high choreography standards, such that the generated dancing sequences are confined within the spatial constraints. To achieve synchronized alignment between diverse motion tempos and music beats, we introduce an actor-critic-based reinforcement learning scheme to the GPT with a newly-designed beat-align reward function. Extensive experiments on the standard benchmark demonstrate that our proposed framework achieves state-of-the-art performance both qualitatively and quantitatively. Notably, the learned choreographic memory is shown to discover human-interpretable dancing-style poses in an unsupervised manner.", "paper_url": "http://arxiv.org/abs/2203.13055v1", "pdf_url": "http://arxiv.org/pdf/2203.13055v1", "repo_url": "https://github.com/lisiyao21/bailando"}, "2203.12980": {"publish_time": "2022-03-24", "title": "MERLIN -- Malware Evasion with Reinforcement LearnINg", "author": "Tony Quertier et.al.", "abstract": "In addition to signature-based and heuristics-based detection techniques, Machine learning (ML) is being widely used to generalize to new never-before-seen malicious software (malware). However, it has been demonstrated that ML models can be fooled by tricking the classifier into returning the incorrect label. These studies usually rely on a prediction score that is fragile to gradient-based attacks for instance. In the context of a more realistic situation where an attacker has very little information about the outputs of a malware detection engine, modest evasion rates are achieved. In this paper, we propose a method using Reinforcement Learning with DQN and REINFORCE algorithms to challenge two state-of-the-art Machine Learning based detection engines (MalConv \\& EMBER) and a commercial AV classified by Gartner as a leader in 2021. Our stateful method combines several actions modifying a Windows Portable Execution (PE) file without breaking its functionalities. Our method also identifies which actions perform better and compiles a detailed vulnerability report to help mitigate the evasion. We demonstrate that REINFORCE achieves very good evasion rates even on a commercial AV with low provided information.", "paper_url": "http://arxiv.org/abs/2203.12980v1", "pdf_url": "http://arxiv.org/pdf/2203.12980v1", "repo_url": null}, "2203.12961": {"publish_time": "2022-03-24", "title": "Multilevel Bayesin Deep Neural Networks", "author": "Neil K. Chada et.al.", "abstract": "In this article we consider Bayesian inference associated to deep neural networks (DNNs) and in particular, trace-class neural network (TNN) priors which were proposed by Sell et al. [39]. Such priors were developed as more robust alternatives to classical architectures in the context of inference problems. For this work we develop multilevel Monte Carlo (MLMC) methods for such models. MLMC is a popular variance reduction technique, with particular applications in Bayesian statistics and uncertainty quantification. We show how a particular advanced MLMC method that was introduced in [4] can be applied to Bayesian inference from DNNs and establish mathematically, that the computational cost to achieve a particular mean square error, associated to posterior expectation computation, can be reduced by several orders, versus more conventional techniques. To verify such results we provide numerous numerical experiments on model problems arising in machine learning. These include Bayesian regression, as well as Bayesian classification and reinforcement learning.", "paper_url": "http://arxiv.org/abs/2203.12961v1", "pdf_url": "http://arxiv.org/pdf/2203.12961v1", "repo_url": null}, "2203.12922": {"publish_time": "2022-03-24", "title": "Horizon-Free Reinforcement Learning in Polynomial Time: the Power of Stationary Policies", "author": "Zihan Zhang et.al.", "abstract": "This paper gives the first polynomial-time algorithm for tabular Markov Decision Processes (MDP) that enjoys a regret bound \\emph{independent on the planning horizon}. Specifically, we consider tabular MDP with $S$ states, $A$ actions, a planning horizon $H$, total reward bounded by $1$, and the agent plays for $K$ episodes. We design an algorithm that achieves an $O\\left(\\mathrm{poly}(S,A,\\log K)\\sqrt{K}\\right)$ regret in contrast to existing bounds which either has an additional $\\mathrm{polylog}(H)$ dependency~\\citep{zhang2020reinforcement} or has an exponential dependency on $S$~\\citep{li2021settling}. Our result relies on a sequence of new structural lemmas establishing the approximation power, stability, and concentration property of stationary policies, which can have applications in other problems related to Markov chains.", "paper_url": "http://arxiv.org/abs/2203.12922v1", "pdf_url": "http://arxiv.org/pdf/2203.12922v1", "repo_url": null}, "2203.13599": {"publish_time": "2022-03-25", "title": "Learning Relational Rules from Rewards", "author": "Guillermo Puebla et.al.", "abstract": "Humans perceive the world in terms of objects and relations between them. In fact, for any given pair of objects, there is a myriad of relations that apply to them. How does the cognitive system learn which relations are useful to characterize the task at hand? And how can it use these representations to build a relational policy to interact effectively with the environment? In this paper we proposed that this problem can be understood through the lens of a sub-field of symbolic machine learning called relational reinforcement learning (RRL). To demonstrate the potential of our approach, we build a simple model of relational policy learning based on a function approximator developed in RRL. We trained and tested our model in three Atari games that required to consider an increasingly number of potential relations: Breakout, Pong and Demon Attack. In each game, our model was able to select adequate relational representations and build a relational policy incrementally. We discuss the relationship between our model with models of relational and analogical reasoning, as well as its limitations and future directions of research.", "paper_url": "http://arxiv.org/abs/2203.13599v1", "pdf_url": "http://arxiv.org/pdf/2203.13599v1", "repo_url": "https://github.com/guillermopuebla/rrl"}, "2203.13573": {"publish_time": "2022-03-25", "title": "Unsupervised Learning of Temporal Abstractions with Slot-based Transformers", "author": "Anand Gopalakrishnan et.al.", "abstract": "The discovery of reusable sub-routines simplifies decision-making and planning in complex reinforcement learning problems. Previous approaches propose to learn such temporal abstractions in a purely unsupervised fashion through observing state-action trajectories gathered from executing a policy. However, a current limitation is that they process each trajectory in an entirely sequential manner, which prevents them from revising earlier decisions about sub-routine boundary points in light of new incoming information. In this work we propose SloTTAr, a fully parallel approach that integrates sequence processing Transformers with a Slot Attention module and adaptive computation for learning about the number of such sub-routines in an unsupervised fashion. We demonstrate how SloTTAr is capable of outperforming strong baselines in terms of boundary point discovery, even for sequences containing variable amounts of sub-routines, while being up to 7x faster to train on existing benchmarks.", "paper_url": "http://arxiv.org/abs/2203.13573v1", "pdf_url": "http://arxiv.org/pdf/2203.13573v1", "repo_url": null}, "2203.13572": {"publish_time": "2022-03-25", "title": "A Visual Navigation Perspective for Category-Level Object Pose Estimation", "author": "Jiaxin Guo et.al.", "abstract": "This paper studies category-level object pose estimation based on a single monocular image. Recent advances in pose-aware generative models have paved the way for addressing this challenging task using analysis-by-synthesis. The idea is to sequentially update a set of latent variables, e.g., pose, shape, and appearance, of the generative model until the generated image best agrees with the observation. However, convergence and efficiency are two challenges of this inference procedure. In this paper, we take a deeper look at the inference of analysis-by-synthesis from the perspective of visual navigation, and investigate what is a good navigation policy for this specific task. We evaluate three different strategies, including gradient descent, reinforcement learning and imitation learning, via thorough comparisons in terms of convergence, robustness and efficiency. Moreover, we show that a simple hybrid approach leads to an effective and efficient solution. We further compare these strategies to state-of-the-art methods, and demonstrate superior performance on synthetic and real-world datasets leveraging off-the-shelf pose-aware generative models.", "paper_url": "http://arxiv.org/abs/2203.13572v1", "pdf_url": "http://arxiv.org/pdf/2203.13572v1", "repo_url": null}, "2203.13563": {"publish_time": "2022-03-25", "title": "An Intelligent End-to-End Neural Architecture Search Framework for Electricity Forecasting Model Development", "author": "Jin Yang et.al.", "abstract": "Recent years have witnessed an exponential growth in developing deep learning (DL) models for the time-series electricity forecasting in power systems. However, most of the proposed models are designed based on the designers' inherent knowledge and experience without elaborating on the suitability of the proposed neural architectures. Moreover, these models cannot be self-adjusted to the dynamically changing data patterns due to an inflexible design of their structures. Even though several latest studies have considered application of the neural architecture search (NAS) technique for obtaining a network with an optimized structure in the electricity forecasting sector, their training process is quite time-consuming, computationally expensive and not intelligent, indicating that the NAS application in electricity forecasting area is still at an infancy phase. In this research study, we propose an intelligent automated architecture search (IAAS) framework for the development of time-series electricity forecasting models. The proposed framework contains two primary components, i.e., network function-preserving transformation operation and reinforcement learning (RL)-based network transformation control. In the first component, we introduce a theoretical function-preserving transformation of recurrent neural networks (RNN) to the literature for capturing the hidden temporal patterns within the time-series data. In the second component, we develop three RL-based transformation actors and a net pool to intelligently and effectively search a high-quality neural architecture. After conducting comprehensive experiments on two publicly-available electricity load datasets and two wind power datasets, we demonstrate that the proposed IAAS framework significantly outperforms the ten existing models or methods in terms of forecasting accuracy and stability.", "paper_url": "http://arxiv.org/abs/2203.13563v1", "pdf_url": "http://arxiv.org/pdf/2203.13563v1", "repo_url": null}, "2203.13424": {"publish_time": "2022-03-25", "title": "Dealing with Sparse Rewards Using Graph Neural Networks", "author": "Matvey Gerasyov et.al.", "abstract": "Deep reinforcement learning in partially observable environments is a difficult task in itself, and can be further complicated by a sparse reward signal. Most tasks involving navigation in three-dimensional environments provide the agent with extremely limited information. Typically, the agent receives a visual observation input from the environment and is rewarded once at the end of the episode. A good reward function could substantially improve the convergence of reinforcement learning algorithms for such tasks. The classic approach to increase the density of the reward signal is to augment it with supplementary rewards. This technique is called the reward shaping. In this study, we propose two modifications of one of the recent reward shaping methods based on graph convolutional networks: the first involving advanced aggregation functions, and the second utilizing the attention mechanism. We empirically validate the effectiveness of our solutions for the task of navigation in a 3D environment with sparse rewards. For the solution featuring attention mechanism, we are also able to show that the learned attention is concentrated on edges corresponding to important transitions in 3D environment.", "paper_url": "http://arxiv.org/abs/2203.13424v1", "pdf_url": "http://arxiv.org/pdf/2203.13424v1", "repo_url": null}, "2203.14908": {"publish_time": "2022-03-28", "title": "Heterogeneous Random Laser with Switching Activity Visualized by Replica Symmetry Breaking Maps", "author": "Loredana M. Massaro et.al.", "abstract": "In the past decade, complex networks of light emitters are proposed as novel platforms for photonic circuits and lab-on-chip active devices. Lasing networks made by connected multiple gain components and graphs of nanoscale random lasers (RLs) obtained from complex meshes of polymeric nanofibers are successful prototypes. However, in the reported research, mainly collective emission from a whole network of resonators is investigated, and only in a few cases, the emission from single points showing, although homogeneous and broad, spatial emission. In all cases, simultaneous activation of the miniaturized lasers is observed. Here, differently, we realize heterogeneous random lasers made of ribbon-like and highly porous fibers with evident RL action from separated micrometric domains that alternatively switch on and off by tuning the pumping light intensity. We visualize this novel effect by building for the first time replica symmetry breaking (RSB) maps of the emitting fibers with 2 {\\mu}m spatial resolution. In addition, we calculate the spatial correlations of the laser regions showing clearly an average extension of 50 {\\mu}m. The observed blinking effect is due to mode interaction along light guiding fibers and opens new avenues in the fabrication of flexible photonic networks with specific and adaptable activity.", "paper_url": "http://arxiv.org/abs/2203.14908v1", "pdf_url": "http://arxiv.org/pdf/2203.14908v1", "repo_url": null}, "2203.14817": {"publish_time": "2022-03-28", "title": "Sketching without Worrying: Noise-Tolerant Sketch-Based Image Retrieval", "author": "Ayan Kumar Bhunia et.al.", "abstract": "Sketching enables many exciting applications, notably, image retrieval. The fear-to-sketch problem (i.e., \"I can't sketch\") has however proven to be fatal for its widespread adoption. This paper tackles this \"fear\" head on, and for the first time, proposes an auxiliary module for existing retrieval models that predominantly lets the users sketch without having to worry. We first conducted a pilot study that revealed the secret lies in the existence of noisy strokes, but not so much of the \"I can't sketch\". We consequently design a stroke subset selector that {detects noisy strokes, leaving only those} which make a positive contribution towards successful retrieval. Our Reinforcement Learning based formulation quantifies the importance of each stroke present in a given subset, based on the extent to which that stroke contributes to retrieval. When combined with pre-trained retrieval models as a pre-processing module, we achieve a significant gain of 8%-10% over standard baselines and in turn report new state-of-the-art performance. Last but not least, we demonstrate the selector once trained, can also be used in a plug-and-play manner to empower various sketch applications in ways that were not previously possible.", "paper_url": "http://arxiv.org/abs/2203.14817v1", "pdf_url": "http://arxiv.org/pdf/2203.14817v1", "repo_url": "https://github.com/ayankumarbhunia/stroke_subset_selector-for-fgsbir"}, "2203.14794": {"publish_time": "2022-03-28", "title": "Limited Parameter Denoising for Low-dose X-ray Computed Tomography Using Deep Reinforcement Learning", "author": "Mayank Patwari et.al.", "abstract": "The use of deep learning has successfully solved several problems in the field of medical imaging. Deep learning has been applied to the CT denoising problem successfully. However, the use of deep learning requires large amounts of data to train deep convolutional networks (CNNs). Moreover, due to large parameter count, such deep CNNs may cause unexpected results. In this study, we introduce a novel CT denoising framework, which has interpretable behaviour, and provides useful results with limited data. We employ bilateral filtering in both the projection and volume domains to remove noise. To account for non-stationary noise, we tune the $\\sigma$ parameters of the volume for every projection view, and for every volume pixel. The tuning is carried out by two deep CNNs. Due to impracticality of labelling, the two deep CNNs are trained via a Deep-Q reinforcement learning task. The reward for the task is generated by using a custom reward function represented by a neural network. Our experiments were carried out on abdominal scans for the Mayo Clinic TCIA dataset, and the AAPM Low Dose CT Grand Challenge. Our denoising framework has excellent denoising performance increasing the PSNR from 28.53 to 28.93, and increasing the SSIM from 0.8952 to 0.9204. We outperform several state-of-the-art deep CNNs, which have several orders of magnitude higher number of parameters (p-value (PSNR) = 0.000, p-value (SSIM) = 0.000). Our method does not introduce any blurring, which is introduced by MSE loss based methods, or any deep learning artifacts, which are introduced by WGAN based models. Our ablation studies show that parameter tuning and using our reward network results in the best possible results.", "paper_url": "http://arxiv.org/abs/2203.14794v1", "pdf_url": "http://arxiv.org/pdf/2203.14794v1", "repo_url": null}, "2203.14790": {"publish_time": "2022-03-28", "title": "5G Routing Interfered Environment", "author": "Barak Gahtan et.al.", "abstract": "5G is the next-generation cellular network technology, with the goal of meeting the critical demand for bandwidth required to accommodate a high density of users. It employs flexible architectures to accommodate the high density \\cite{7390965}. 5G is enabled by mmWave communication, which operates at frequencies ranging from 30 to 300 GHz. This paper discusses the creation of a python-based environment known as the 5G Routing Interfered Environment (5GRIE). The environment can run different algorithms to route packets with source and destination pairs using a formulated interference model. Deep Reinforcement Learning algorithms that use Stable-Baselines 3 \\cite{Raffin_Stable_Baselines3_2020}, as well as heuristic-based algorithms like random or greedy, can be run on it. Profitable is an algorithm that is provided.", "paper_url": "http://arxiv.org/abs/2203.14790v1", "pdf_url": "http://arxiv.org/pdf/2203.14790v1", "repo_url": "https://github.com/BarakGahtan/5GRIE"}, "2203.14749": {"publish_time": "2022-03-28", "title": "Adaptive Risk Tendency: Nano Drone Navigation in Cluttered Environments with Distributional Reinforcement Learning", "author": "Cheng Liu et.al.", "abstract": "Enabling robots with the capability of assessing risk and making risk-aware decisions is widely considered a key step toward ensuring robustness for robots operating under uncertainty. In this paper, we consider the specific case of a nano drone robot learning to navigate an apriori unknown environment while avoiding obstacles under partial observability. We present a distributional reinforcement learning framework in order to learn adaptive risk tendency policies. Specifically, we propose to use tail conditional variance of the learnt action-value distribution as an uncertainty measurement, and use a exponentially weighted average forecasting algorithm to automatically adapt the risk-tendency at run-time based on the observed uncertainty in the environment. We show our algorithm can adjust its risk-sensitivity on the fly both in simulation and real-world experiments and achieving better performance than risk-neutral policy or risk-averse policies. Code and real-world experiment video can be found in this repository: \\url{https://github.com/tudelft/risk-sensitive-rl.git}", "paper_url": "http://arxiv.org/abs/2203.14749v1", "pdf_url": "http://arxiv.org/pdf/2203.14749v1", "repo_url": "https://github.com/tudelft/risk-sensitive-rl"}, "2203.15778": {"publish_time": "2022-03-29", "title": "Text-Driven Video Acceleration: A Weakly-Supervised Reinforcement Learning Method", "author": "Washington Ramos et.al.", "abstract": "The growth of videos in our digital age and the users' limited time raise the demand for processing untrimmed videos to produce shorter versions conveying the same information. Despite the remarkable progress that summarization methods have made, most of them can only select a few frames or skims, creating visual gaps and breaking the video context. This paper presents a novel weakly-supervised methodology based on a reinforcement learning formulation to accelerate instructional videos using text. A novel joint reward function guides our agent to select which frames to remove and reduce the input video to a target length without creating gaps in the final video. We also propose the Extended Visually-guided Document Attention Network (VDAN+), which can generate a highly discriminative embedding space to represent both textual and visual data. Our experiments show that our method achieves the best performance in Precision, Recall, and F1 Score against the baselines while effectively controlling the video's output length. Visit https://www.verlab.dcc.ufmg.br/semantic-hyperlapse/tpami2022/ for code and extra results.", "paper_url": "http://arxiv.org/abs/2203.15778v1", "pdf_url": "http://arxiv.org/pdf/2203.15778v1", "repo_url": null}, "2203.15755": {"publish_time": "2022-03-29", "title": "Demonstration-Bootstrapped Autonomous Practicing via Multi-Task Reinforcement Learning", "author": "Abhishek Gupta et.al.", "abstract": "Reinforcement learning systems have the potential to enable continuous improvement in unstructured environments, leveraging data collected autonomously. However, in practice these systems require significant amounts of instrumentation or human intervention to learn in the real world. In this work, we propose a system for reinforcement learning that leverages multi-task reinforcement learning bootstrapped with prior data to enable continuous autonomous practicing, minimizing the number of resets needed while being able to learn temporally extended behaviors. We show how appropriately provided prior data can help bootstrap both low-level multi-task policies and strategies for sequencing these tasks one after another to enable learning with minimal resets. This mechanism enables our robotic system to practice with minimal human intervention at training time while being able to solve long horizon tasks at test time. We show the efficacy of the proposed system on a challenging kitchen manipulation task both in simulation and in the real world, demonstrating the ability to practice autonomously in order to solve temporally extended problems.", "paper_url": "http://arxiv.org/abs/2203.15755v1", "pdf_url": "http://arxiv.org/pdf/2203.15755v1", "repo_url": null}, "2203.15722": {"publish_time": "2022-03-29", "title": "Transformer Network-based Reinforcement Learning Method for Power Distribution Network (PDN) Optimization of High Bandwidth Memory (HBM)", "author": "Hyunwook Park et.al.", "abstract": "In this article, for the first time, we propose a transformer network-based reinforcement learning (RL) method for power distribution network (PDN) optimization of high bandwidth memory (HBM). The proposed method can provide an optimal decoupling capacitor (decap) design to maximize the reduction of PDN self- and transfer impedance seen at multiple ports. An attention-based transformer network is implemented to directly parameterize decap optimization policy. The optimality performance is significantly improved since the attention mechanism has powerful expression to explore massive combinatorial space for decap assignments. Moreover, it can capture sequential relationships between the decap assignments. The computing time for optimization is dramatically reduced due to the reusable network on positions of probing ports and decap assignment candidates. This is because the transformer network has a context embedding process to capture meta-features including probing ports positions. In addition, the network is trained with randomly generated data sets. Therefore, without additional training, the trained network can solve new decap optimization problems. The computing time for training and data cost are critically decreased due to the scalability of the network. Thanks to its shared weight property, the network can adapt to a larger scale of problems without additional training. For verification, we compare the results with conventional genetic algorithm (GA), random search (RS), and all the previous RL-based methods. As a result, the proposed method outperforms in all the following aspects: optimality performance, computing time, and data efficiency.", "paper_url": "http://arxiv.org/abs/2203.15722v1", "pdf_url": "http://arxiv.org/pdf/2203.15722v1", "repo_url": null}, "2203.15625": {"publish_time": "2022-03-29", "title": "PoseTriplet: Co-evolving 3D Human Pose Estimation, Imitation, and Hallucination under Self-supervision", "author": "Kehong Gong et.al.", "abstract": "Existing self-supervised 3D human pose estimation schemes have largely relied on weak supervisions like consistency loss to guide the learning, which, inevitably, leads to inferior results in real-world scenarios with unseen poses. In this paper, we propose a novel self-supervised approach that allows us to explicitly generate 2D-3D pose pairs for augmenting supervision, through a self-enhancing dual-loop learning framework. This is made possible via introducing a reinforcement-learning-based imitator, which is learned jointly with a pose estimator alongside a pose hallucinator; the three components form two loops during the training process, complementing and strengthening one another. Specifically, the pose estimator transforms an input 2D pose sequence to a low-fidelity 3D output, which is then enhanced by the imitator that enforces physical constraints. The refined 3D poses are subsequently fed to the hallucinator for producing even more diverse data, which are, in turn, strengthened by the imitator and further utilized to train the pose estimator. Such a co-evolution scheme, in practice, enables training a pose estimator on self-generated motion data without relying on any given 3D data. Extensive experiments across various benchmarks demonstrate that our approach yields encouraging results significantly outperforming the state of the art and, in some cases, even on par with results of fully-supervised methods. Notably, it achieves 89.1% 3D PCK on MPI-INF-3DHP under self-supervised cross-dataset evaluation setup, improving upon the previous best self-supervised methods by 8.6%. Code can be found at: https://github.com/Garfield-kh/PoseTriplet", "paper_url": "http://arxiv.org/abs/2203.15625v1", "pdf_url": "http://arxiv.org/pdf/2203.15625v1", "repo_url": null}, "2203.15426": {"publish_time": "2022-03-29", "title": "On Reinforcement Learning, Effect Handlers, and the State Monad", "author": "Ugo Dal Lago et.al.", "abstract": "We study the algebraic effects and handlers as a way to support decision-making abstractions in functional programs, whereas a user can ask a learning algorithm to resolve choices without implementing the underlying selection mechanism, and give a feedback by way of rewards. Differently from some recently proposed approach to the problem based on the selection monad [Abadi and Plotkin, LICS 2021], we express the underlying intelligence as a reinforcement learning algorithm implemented as a set of handlers for some of these algebraic operations, including those for choices and rewards. We show how we can in practice use algebraic operations and handlers -- as available in the programming language EFF -- to clearly separate the learning algorithm from its environment, thus allowing for a good level of modularity. We then show how the host language can be taken as a lambda-calculus with handlers, this way showing what the essential linguistic features are. We conclude by hinting at how type and effect systems could ensure safety properties, at the same time pointing at some directions for further work.", "paper_url": "http://arxiv.org/abs/2203.15426v1", "pdf_url": "http://arxiv.org/pdf/2203.15426v1", "repo_url": null}, "2203.16464": {"publish_time": "2022-03-30", "title": "Towards Interpretable Deep Reinforcement Learning Models via Inverse Reinforcement Learning", "author": "Yuansheng Xie et.al.", "abstract": "Artificial intelligence, particularly through recent advancements in deep learning, has achieved exceptional performances in many tasks in fields such as natural language processing and computer vision. In addition to desirable evaluation metrics, a high level of interpretability is often required for these models to be reliably utilized. Therefore, explanations that offer insight into the process by which a model maps its inputs onto its outputs are much sought-after. Unfortunately, current black box nature of machine learning models is still an unresolved issue and this very nature prevents researchers from learning and providing explicative descriptions for a model's behavior and final predictions. In this work, we propose a novel framework utilizing Adversarial Inverse Reinforcement Learning that can provide global explanations for decisions made by a Reinforcement Learning model and capture intuitive tendencies that the model follows by summarizing the model's decision-making process.", "paper_url": "http://arxiv.org/abs/2203.16464v1", "pdf_url": "http://arxiv.org/pdf/2203.16464v1", "repo_url": null}, "2203.16319": {"publish_time": "2022-03-30", "title": "Multi-Robot Active Mapping via Neural Bipartite Graph Matching", "author": "Kai Ye et.al.", "abstract": "We study the problem of multi-robot active mapping, which aims for complete scene map construction in minimum time steps. The key to this problem lies in the goal position estimation to enable more efficient robot movements. Previous approaches either choose the frontier as the goal position via a myopic solution that hinders the time efficiency, or maximize the long-term value via reinforcement learning to directly regress the goal position, but does not guarantee the complete map construction. In this paper, we propose a novel algorithm, namely NeuralCoMapping, which takes advantage of both approaches. We reduce the problem to bipartite graph matching, which establishes the node correspondences between two graphs, denoting robots and frontiers. We introduce a multiplex graph neural network (mGNN) that learns the neural distance to fill the affinity matrix for more effective graph matching. We optimize the mGNN with a differentiable linear assignment layer by maximizing the long-term values that favor time efficiency and map completeness via reinforcement learning. We compare our algorithm with several state-of-the-art multi-robot active mapping approaches and adapted reinforcement-learning baselines. Experimental results demonstrate the superior performance and exceptional generalization ability of our algorithm on various indoor scenes and unseen number of robots, when only trained with 9 indoor scenes.", "paper_url": "http://arxiv.org/abs/2203.16319v1", "pdf_url": "http://arxiv.org/pdf/2203.16319v1", "repo_url": null}, "2203.16289": {"publish_time": "2022-03-30", "title": "One-Step Two-Critic Deep Reinforcement Learning for Inverter-based Volt-Var Control in Active Distribution Networks", "author": "Qiong Liu et.al.", "abstract": "A one-step two-critic deep reinforcement learning (OSTC-DRL) approach for inverter-based volt-var control (IB-VVC) in active distribution networks is proposed in this paper. Firstly, considering IB-VVC can be formulated as a single-period optimization problem, we formulate the IB-VVC as a one-step Markov decision process rather than the standard Markov decision process, which simplifies the DRL learning task. Then we design the one-step actor-critic DRL scheme which is a simplified version of recent DRL algorithms, and it avoids the issue of Q value overestimation successfully. Furthermore, considering two objectives of VVC: minimizing power loss and eliminating voltage violation, we utilize two critics to approximate the rewards of two objectives separately. It simplifies the approximation tasks of each critic, and avoids the interaction effect between two objectives in the learning process of critic. The OSTC-DRL approach integrates the one-step actor-critic DRL scheme and the two-critic technology. Based on the OSTC-DRL, we design two centralized DRL algorithms. Further, we extend the OSTC-DRL to multi-agent OSTC-DRL for decentralized IB-VVC and design two multi-agent DRL algorithms. Simulations demonstrate that the proposed OSTC-DRL has a faster convergence rate and a better control performance, and the multi-agent OSTC-DRL works well for decentralized IB-VVC problems.", "paper_url": "http://arxiv.org/abs/2203.16289v1", "pdf_url": "http://arxiv.org/pdf/2203.16289v1", "repo_url": null}, "2203.16275": {"publish_time": "2022-03-30", "title": "Reinforcement Learning Guided by Provable Normative Compliance", "author": "Emery Neufeld et.al.", "abstract": "Reinforcement learning (RL) has shown promise as a tool for engineering safe, ethical, or legal behaviour in autonomous agents. Its use typically relies on assigning punishments to state-action pairs that constitute unsafe or unethical choices. Despite this assignment being a crucial step in this approach, however, there has been limited discussion on generalizing the process of selecting punishments and deciding where to apply them. In this paper, we adopt an approach that leverages an existing framework -- the normative supervisor of (Neufeld et al., 2021) -- during training. This normative supervisor is used to dynamically translate states and the applicable normative system into defeasible deontic logic theories, feed these theories to a theorem prover, and use the conclusions derived to decide whether or not to assign a punishment to the agent. We use multi-objective RL (MORL) to balance the ethical objective of avoiding violations with a non-ethical objective; we will demonstrate that our approach works for a multiplicity of MORL techniques, and show that it is effective regardless of the magnitude of the punishment we assign.", "paper_url": "http://arxiv.org/abs/2203.16275v1", "pdf_url": "http://arxiv.org/pdf/2203.16275v1", "repo_url": null}, "2203.16199": {"publish_time": "2022-03-30", "title": "Momentum transport properties of a hot and dense QCD matter in a weak magnetic field", "author": "Shubhalaxmi Rath et.al.", "abstract": "We have studied the momentum transport properties of a hot and dense QCD matter in a weak magnetic field by determining the shear ($\\eta$) and bulk ($\\zeta$) viscosities in the relaxation time approximation of kinetic theory. The dependence of $\\eta$ and $\\zeta$ on the temperature has been explored in the presence of weak magnetic field ($B$-field) and finite chemical potential ($\\mu$). It is observed that both shear and bulk viscosities get decreased in the presence of a weak magnetic field, whereas the finite chemical potential increases these viscosities, specifically at low temperatures. This study is important to understand the sound attenuation through the Prandtl number (Pl), the nature of the flow through the Reynolds number (Rl), the fluidity and location of transition point of the matter through the ratios $\\eta/s$ and $\\zeta/s$ ($s$ is the entropy density), respectively. The Prandtl number gets increased in the weak magnetic field, whereas the finite chemical potential reduces its magnitude as compared to the counterpart in the absence of $B$-field and $\\mu$. However, Pl still remains larger than unity, indicating that the energy dissipation due to the sound attenuation is mostly governed by the momentum diffusion. It is noticed that the weak magnetic field makes the Reynolds number larger, whereas the chemical potential makes it smaller than that in the absence of $B$-field and $\\mu$. We have observed that the ratio $\\eta/s$ gets decreased in the weak magnetic field regime, whereas the finite chemical potential increases its value, but the ratio $\\zeta/s$ is found to be decreased by both weak magnetic field and finite chemical potential.", "paper_url": "http://arxiv.org/abs/2203.16199v1", "pdf_url": "http://arxiv.org/pdf/2203.16199v1", "repo_url": null}, "2203.17275": {"publish_time": "2022-03-31", "title": "DiffSkill: Skill Abstraction from Differentiable Physics for Deformable Object Manipulations with Tools", "author": "Xingyu Lin et.al.", "abstract": "We consider the problem of sequential robotic manipulation of deformable objects using tools. Previous works have shown that differentiable physics simulators provide gradients to the environment state and help trajectory optimization to converge orders of magnitude faster than model-free reinforcement learning algorithms for deformable object manipulation. However, such gradient-based trajectory optimization typically requires access to the full simulator states and can only solve short-horizon, single-skill tasks due to local optima. In this work, we propose a novel framework, named DiffSkill, that uses a differentiable physics simulator for skill abstraction to solve long-horizon deformable object manipulation tasks from sensory observations. In particular, we first obtain short-horizon skills using individual tools from a gradient-based optimizer, using the full state information in a differentiable simulator; we then learn a neural skill abstractor from the demonstration trajectories which takes RGBD images as input. Finally, we plan over the skills by finding the intermediate goals and then solve long-horizon tasks. We show the advantages of our method in a new set of sequential deformable object manipulation tasks compared to previous reinforcement learning algorithms and compared to the trajectory optimizer.", "paper_url": "http://arxiv.org/abs/2203.17275v1", "pdf_url": "http://arxiv.org/pdf/2203.17275v1", "repo_url": null}, "2203.17193": {"publish_time": "2022-03-31", "title": "Learning from many trajectories", "author": "Stephen Tu et.al.", "abstract": "We initiate a study of supervised learning from many independent sequences (\"trajectories\") of non-independent covariates, reflecting tasks in sequence modeling, control, and reinforcement learning. Conceptually, our multi-trajectory setup sits between two traditional settings in statistical learning theory: learning from independent examples and learning from a single auto-correlated sequence. Our conditions for efficient learning generalize the former setting--trajectories must be non-degenerate in ways that extend standard requirements for independent examples. They do not require that trajectories be ergodic, long, nor strictly stable.   For linear least-squares regression, given $n$-dimensional examples produced by $m$ trajectories, each of length $T$, we observe a notable change in statistical efficiency as the number of trajectories increases from a few (namely $m \\lesssim n$) to many (namely $m \\gtrsim n$). Specifically, we establish that the worst-case error rate this problem is $\\Theta(n / m T)$ whenever $m \\gtrsim n$. Meanwhile, when $m \\lesssim n$, we establish a (sharp) lower bound of $\\Omega(n^2 / m^2 T)$ on the worst-case error rate, realized by a simple, marginally unstable linear dynamical system. A key upshot is that, in domains where trajectories regularly reset, the error rate eventually behaves as if all of the examples were independent altogether, drawn from their marginals. As a corollary of our analysis, we also improve guarantees for the linear system identification problem.", "paper_url": "http://arxiv.org/abs/2203.17193v1", "pdf_url": "http://arxiv.org/pdf/2203.17193v1", "repo_url": null}, "2203.17128": {"publish_time": "2022-03-31", "title": "Neural Q-learning for solving elliptic PDEs", "author": "Samuel N. Cohen et.al.", "abstract": "Solving high-dimensional partial differential equations (PDEs) is a major challenge in scientific computing. We develop a new numerical method for solving elliptic-type PDEs by adapting the Q-learning algorithm in reinforcement learning. Our \"Q-PDE\" algorithm is mesh-free and therefore has the potential to overcome the curse of dimensionality. Using a neural tangent kernel (NTK) approach, we prove that the neural network approximator for the PDE solution, trained with the Q-PDE algorithm, converges to the trajectory of an infinite-dimensional ordinary differential equation (ODE) as the number of hidden units $\\rightarrow \\infty$. For monotone PDE (i.e. those given by monotone operators, which may be nonlinear), despite the lack of a spectral gap in the NTK, we then prove that the limit neural network, which satisfies the infinite-dimensional ODE, converges in $L^2$ to the PDE solution as the training time $\\rightarrow \\infty$. More generally, we can prove that any fixed point of the wide-network limit for the Q-PDE algorithm is a solution of the PDE (not necessarily under the monotone condition). The numerical performance of the Q-PDE algorithm is studied for several elliptic PDEs.", "paper_url": "http://arxiv.org/abs/2203.17128v1", "pdf_url": "http://arxiv.org/pdf/2203.17128v1", "repo_url": null}, "2203.17106": {"publish_time": "2022-03-31", "title": "A Cooperative Optimal Control Framework for Connected and Automated Vehicles in Mixed Traffic Using Social Value Orientation", "author": "Viet-Anh Le et.al.", "abstract": "In this paper, we develop a socially cooperative optimal control framework for connected and automated vehicles (CAVs) in mixed traffic using social value orientation (SVO). In our approach, we formulate the interaction between a CAV and a human-driven vehicle (HDV) as a simultaneous game to facilitate the derivation of a Nash equilibrium. In the imposed game, each vehicle minimizes a weighted sum of its egoistic objective and a cooperative objective. The SVO angles are used to quantify preferences of the vehicles toward the egoistic and cooperative objectives which lead to an appropriate design of weighting factors in a multi-objective optimal control problem. We prove that by solving the proposed optimal control problem, a Nash equilibrium can be obtained. To estimate the SVO angle of the HDV, we develop a receding horizon estimation based on maximum entropy inverse reinforcement learning. The effectiveness of the proposed approach is demonstrated by numerical simulations at a highway on-ramp merging scenario.", "paper_url": "http://arxiv.org/abs/2203.17106v1", "pdf_url": "http://arxiv.org/pdf/2203.17106v1", "repo_url": null}, "2203.16942": {"publish_time": "2022-03-31", "title": "Sequential Recommendation with User Evolving Preference Decomposition", "author": "Weiqi Shao et.al.", "abstract": "Modeling user sequential behaviors has recently attracted increasing attention in the recommendation domain. Existing methods mostly assume coherent preference in the same sequence. However, user personalities are volatile and easily changed, and there can be multiple mixed preferences underlying user behaviors. To solve this problem, in this paper, we propose a novel sequential recommender model via decomposing and modeling user independent preferences. To achieve this goal, we highlight three practical challenges considering the inconsistent, evolving and uneven nature of the user behavior, which are seldom noticed by the previous work. For overcoming these challenges in a unified framework, we introduce a reinforcement learning module to simulate the evolution of user preference. More specifically, the action aims to allocate each item into a sub-sequence or create a new one according to how the previous items are decomposed as well as the time interval between successive behaviors. The reward is associated with the final loss of the learning objective, aiming to generate sub-sequences which can better fit the training data. We conduct extensive experiments based on six real-world datasets across different domains. Compared with the state-of-the-art methods, empirical studies manifest that our model can on average improve the performance by about 8.21%, 10.08%, 10.32%, and 9.82% on the metrics of Precision, Recall, NDCG and MRR, respectively.", "paper_url": "http://arxiv.org/abs/2203.16942v1", "pdf_url": "http://arxiv.org/pdf/2203.16942v1", "repo_url": null}, "2204.00565": {"publish_time": "2022-04-01", "title": "What makes useful auxiliary tasks in reinforcement learning: investigating the effect of the target policy", "author": "Banafsheh Rafiee et.al.", "abstract": "Auxiliary tasks have been argued to be useful for representation learning in reinforcement learning. Although many auxiliary tasks have been empirically shown to be effective for accelerating learning on the main task, it is not yet clear what makes useful auxiliary tasks. Some of the most promising results are on the pixel control, reward prediction, and the next state prediction auxiliary tasks; however, the empirical results are mixed, showing substantial improvements in some cases and marginal improvements in others. Careful investigations of how auxiliary tasks help the learning of the main task is necessary. In this paper, we take a step studying the effect of the target policies on the usefulness of the auxiliary tasks formulated as general value functions. General value functions consist of three core elements: 1) policy 2) cumulant 3) continuation function. Our focus on the role of the target policy of the auxiliary tasks is motivated by the fact that the target policy determines the behavior about which the agent wants to make a prediction and the state-action distribution that the agent is trained on, which further affects the main task learning. Our study provides insights about questions such as: Does a greedy policy result in bigger improvement gains compared to other policies? Is it best to set the auxiliary task policy to be the same as the main task policy? Does the choice of the target policy have a substantial effect on the achieved performance gain or simple strategies for setting the policy, such as using a uniformly random policy, work as well? Our empirical results suggest that: 1) Auxiliary tasks with the greedy policy tend to be useful. 2) Most policies, including a uniformly random policy, tend to improve over the baseline. 3) Surprisingly, the main task policy tends to be less useful compared to other policies.", "paper_url": "http://arxiv.org/abs/2204.00565v1", "pdf_url": "http://arxiv.org/pdf/2204.00565v1", "repo_url": null}, "2204.00377": {"publish_time": "2022-04-01", "title": "Deep Page-Level Interest Network in Reinforcement Learning for Ads Allocation", "author": "Guogang Liao et.al.", "abstract": "A mixed list of ads and organic items is usually displayed in feed and how to allocate the limited slots to maximize the overall revenue is a key problem. Meanwhile, modeling user preference with historical behavior is essential in recommendation and advertising (e.g., CTR prediction and ads allocation). Most previous works for user behavior modeling only model user's historical point-level positive feedback (i.e., click), which neglect the page-level information of feedback and other types of feedback. To this end, we propose Deep Page-level Interest Network (DPIN) to model the page-level user preference and exploit multiple types of feedback. Specifically, we introduce four different types of page-level feedback as input, and capture user preference for item arrangement under different receptive fields through the multi-channel interaction module. Through extensive offline and online experiments on Meituan food delivery platform, we demonstrate that DPIN can effectively model the page-level user preference and increase the revenue for the platform.", "paper_url": "http://arxiv.org/abs/2204.00377v1", "pdf_url": "http://arxiv.org/pdf/2204.00377v1", "repo_url": null}, "2204.00308": {"publish_time": "2022-04-01", "title": "Model-agnostic Counterfactual Synthesis Policy for Interactive Recommendation", "author": "Siyu Wang et.al.", "abstract": "Interactive recommendation is able to learn from the interactive processes between users and systems to confront the dynamic interests of users. Recent advances have convinced that the ability of reinforcement learning to handle the dynamic process can be effectively applied in the interactive recommendation. However, the sparsity of interactive data may hamper the performance of the system. We propose to train a Model-agnostic Counterfactual Synthesis Policy to generate counterfactual data and address the data sparsity problem by modelling from observation and counterfactual distribution. The proposed policy can identify and replace the trivial components for any state in the training process with other agents, which can be deployed in any RL-based algorithm. The experimental results demonstrate the effectiveness and generality of our proposed policy.", "paper_url": "http://arxiv.org/abs/2204.00308v1", "pdf_url": "http://arxiv.org/pdf/2204.00308v1", "repo_url": null}, "2204.00306": {"publish_time": "2022-04-01", "title": "Building Decision Forest via Deep Reinforcement Learning", "author": "Guixuan Wen et.al.", "abstract": "Ensemble learning methods whose base classifier is a decision tree usually belong to the bagging or boosting. However, no previous work has ever built the ensemble classifier by maximizing long-term returns to the best of our knowledge. This paper proposes a decision forest building method called MA-H-SAC-DF for binary classification via deep reinforcement learning. First, the building process is modeled as a decentralized partial observable Markov decision process, and a set of cooperative agents jointly constructs all base classifiers. Second, the global state and local observations are defined based on informations of the parent node and the current location. Last, the state-of-the-art deep reinforcement method Hybrid SAC is extended to a multi-agent system under the CTDE architecture to find an optimal decision forest building policy. The experiments indicate that MA-H-SAC-DF has the same performance as random forest, Adaboost, and GBDT on balanced datasets and outperforms them on imbalanced datasets.", "paper_url": "http://arxiv.org/abs/2204.00306v1", "pdf_url": "http://arxiv.org/pdf/2204.00306v1", "repo_url": null}, "2204.00302": {"publish_time": "2022-04-01", "title": "Actual Causality and Responsibility Attribution in Decentralized Partially Observable Markov Decision Processes", "author": "Stelios Triantafyllou et.al.", "abstract": "Actual causality and a closely related concept of responsibility attribution are central to accountable decision making. Actual causality focuses on specific outcomes and aims to identify decisions (actions) that were critical in realizing an outcome of interest. Responsibility attribution is complementary and aims to identify the extent to which decision makers (agents) are responsible for this outcome. In this paper, we study these concepts under a widely used framework for multi-agent sequential decision making under uncertainty: decentralized partially observable Markov decision processes (Dec-POMDPs). Following recent works in RL that show correspondence between POMDPs and Structural Causal Models (SCMs), we first establish a connection between Dec-POMDPs and SCMs. This connection enables us to utilize a language for describing actual causality from prior work and study existing definitions of actual causality in Dec-POMDPs. Given that some of the well-known definitions may lead to counter-intuitive actual causes, we introduce a novel definition that more explicitly accounts for causal dependencies between agents' actions. We then turn to responsibility attribution based on actual causality, where we argue that in ascribing responsibility to an agent it is important to consider both the number of actual causes in which the agent participates, as well as its ability to manipulate its own degree of responsibility. Motivated by these arguments we introduce a family of responsibility attribution methods that extends prior work, while accounting for the aforementioned considerations. Finally, through a simulation-based experiment, we compare different definitions of actual causality and responsibility attribution methods. The empirical results demonstrate the qualitative difference between the considered definitions of actual causality and their impact on attributed responsibility.", "paper_url": "http://arxiv.org/abs/2204.00302v1", "pdf_url": "http://arxiv.org/pdf/2204.00302v1", "repo_url": null}, "2204.01597": {"publish_time": "2022-04-04", "title": "Optimising Energy Efficiency in UAV-Assisted Networks using Deep Reinforcement Learning", "author": "Babatunji Omoniwa et.al.", "abstract": "In this letter, we study the energy efficiency (EE) optimisation of unmanned aerial vehicles (UAVs) providing wireless coverage to static and mobile ground users. Recent multi-agent reinforcement learning approaches optimise the system's EE using a 2D trajectory design, neglecting interference from nearby UAV cells. We aim to maximise the system's EE by jointly optimising each UAV's 3D trajectory, number of connected users, and the energy consumed, while accounting for interference. Thus, we propose a cooperative Multi-Agent Decentralised Double Deep Q-Network (MAD-DDQN) approach. Our approach outperforms existing baselines in terms of EE by as much as 55 -- 80%.", "paper_url": "http://arxiv.org/abs/2204.01597v1", "pdf_url": "http://arxiv.org/pdf/2204.01597v1", "repo_url": null}, "2204.01464": {"publish_time": "2022-04-04", "title": "Value Gradient weighted Model-Based Reinforcement Learning", "author": "Claas Voelcker et.al.", "abstract": "Model-based reinforcement learning (MBRL) is a sample efficient technique to obtain control policies, yet unavoidable modeling errors often lead performance deterioration. The model in MBRL is often solely fitted to reconstruct dynamics, state observations in particular, while the impact of model error on the policy is not captured by the training objective. This leads to a mismatch between the intended goal of MBRL, enabling good policy and value learning, and the target of the loss function employed in practice, future state prediction. Naive intuition would suggest that value-aware model learning would fix this problem and, indeed, several solutions to this objective mismatch problem have been proposed based on theoretical analysis. However, they tend to be inferior in practice to commonly used maximum likelihood (MLE) based approaches. In this paper we propose the Value-gradient weighted Model Learning (VaGraM), a novel method for value-aware model learning which improves the performance of MBRL in challenging settings, such as small model capacity and the presence of distracting state dimensions. We analyze both MLE and value-aware approaches and demonstrate how they fail to account for exploration and the behavior of function approximation when learning value-aware models and highlight the additional goals that must be met to stabilize optimization in the deep learning setting. We verify our analysis by showing that our loss function is able to achieve high returns on the Mujoco benchmark suite while being more robust than maximum likelihood based approaches.", "paper_url": "http://arxiv.org/abs/2204.01464v1", "pdf_url": "http://arxiv.org/pdf/2204.01464v1", "repo_url": null}, "2204.01437": {"publish_time": "2022-04-04", "title": "Disentangling Abstraction from Statistical Pattern Matching in Human and Machine Learning", "author": "Sreejan Kumar et.al.", "abstract": "The ability to acquire abstract knowledge is a hallmark of human intelligence and is believed by many to be one of the core differences between humans and neural network models. Agents can be endowed with an inductive bias towards abstraction through meta-learning, where they are trained on a distribution of tasks that share some abstract structure that can be learned and applied. However, because neural networks are hard to interpret, it can be difficult to tell whether agents have learned the underlying abstraction, or alternatively statistical patterns that are characteristic of that abstraction. In this work, we compare the performance of humans and agents in a meta-reinforcement learning paradigm in which tasks are generated from abstract rules. We define a novel methodology for building \"task metamers\" that closely match the statistics of the abstract tasks but use a different underlying generative process, and evaluate performance on both abstract and metamer tasks. In our first set of experiments, we found that humans perform better at abstract tasks than metamer tasks whereas a widely-used meta-reinforcement learning agent performs worse on the abstract tasks than the matched metamers. In a second set of experiments, we base the tasks on abstractions derived directly from empirically identified human priors. We utilize the same procedure to generate corresponding metamer tasks, and see the same double dissociation between humans and agents. This work provides a foundation for characterizing differences between humans and machine learning that can be used in future work towards developing machines with human-like behavior.", "paper_url": "http://arxiv.org/abs/2204.01437v1", "pdf_url": "http://arxiv.org/pdf/2204.01437v1", "repo_url": "https://github.com/sreejank/Abstract_Neural_Metamers"}, "2204.01409": {"publish_time": "2022-04-04", "title": "Safe Controller for Output Feedback Linear Systems using Model-Based Reinforcement Learning", "author": "S M Nahid Mahmud et.al.", "abstract": "The objective of this research is to enable safety-critical systems to simultaneously learn and execute optimal control policies in a safe manner to achieve complex autonomy. Learning optimal policies via trial and error, i.e., traditional reinforcement learning, is difficult to implement in safety-critical systems, particularly when task restarts are unavailable. Safe model-based reinforcement learning techniques based on a barrier transformation have recently been developed to address this problem. However, these methods rely on full state feedback, limiting their usability in a real-world environment. In this work, an output-feedback safe model-based reinforcement learning technique based on a novel barrier-aware dynamic state estimator has been designed to address this issue. The developed approach facilitates simultaneous learning and execution of safe control policies for safety-critical linear systems. Simulation results indicate that barrier transformation is an effective approach to achieve online reinforcement learning in safety-critical systems using output feedback.", "paper_url": "http://arxiv.org/abs/2204.01409v1", "pdf_url": "http://arxiv.org/pdf/2204.01409v1", "repo_url": null}, "2204.01266": {"publish_time": "2022-04-04", "title": "CIRS: Bursting Filter Bubbles by Counterfactual Interactive Recommender System", "author": "Chongming Gao et.al.", "abstract": "While personalization increases the utility of recommender systems, it also brings the issue of filter bubbles. E.g., if the system keeps exposing and recommending the items that the user is interested in, it may also make the user feel bored and less satisfied. Existing work studies filter bubbles in static recommendation, where the effect of overexposure is hard to capture. In contrast, we believe it is more meaningful to study the issue in interactive recommendation and optimize long-term user satisfaction. Nevertheless, it is unrealistic to train the model online due to the high cost. As such, we have to leverage offline training data and disentangle the causal effect on user satisfaction.   To achieve this goal, we propose a counterfactual interactive recommender system (CIRS) that augments offline reinforcement learning (offline RL) with causal inference. The basic idea is to first learn a causal user model on historical data to capture the overexposure effect of items on user satisfaction. It then uses the learned causal user model to help the planning of the RL policy. To conduct evaluation offline, we innovatively create an authentic RL environment (KuaiEnv) based on a real-world fully observed user rating dataset. The experiments show the effectiveness of CIRS in bursting filter bubbles and achieving long-term success in interactive recommendation. The implementation of CIRS is available via https://github.com/chongminggao/CIRS-codes.", "paper_url": "http://arxiv.org/abs/2204.01266v1", "pdf_url": "http://arxiv.org/pdf/2204.01266v1", "repo_url": "https://github.com/chongminggao/cirs-codes"}, "2204.02395": {"publish_time": "2022-04-05", "title": "A Piecewise Learning Framework for Control of Unknown Nonlinear Systems with Stability Guarantees", "author": "Milad Farsi et.al.", "abstract": "We propose a piecewise learning framework for controlling nonlinear systems with unknown dynamics. While model-based reinforcement learning techniques in terms of some basis functions are well known in the literature, when it comes to more complex dynamics, only a local approximation of the model can be obtained using a limited number of bases. The complexity of the identifier and the controller can be considerably high if obtaining an approximation over a larger domain is desired. To overcome this limitation, we propose a general piecewise nonlinear framework where each piece is responsible for locally learning and controlling over some region of the domain. We obtain rigorous uncertainty bounds for the learned piecewise models. The piecewise affine (PWA) model is then studied as a special case, for which we propose an optimization-based verification technique for stability analysis of the closed-loop system. Accordingly, given a time-discretization of the learned {PWA} system, we iteratively search for a common piecewise Lyapunov function in a set of positive definite functions, where a non-monotonic convergence is allowed. This Lyapunov candidate is verified on the uncertain system to either provide a certificate for stability or find a counter-example when it fails. This counter-example is added to a set of samples to facilitate the further learning of a Lyapunov function. We demonstrate the results on two examples and show that the proposed approach yields a less conservative region of attraction (ROA) compared with alternative state-of-the-art approaches. Moreover, we provide the runtime results to demonstrate potentials of the proposed framework in real-world implementations.", "paper_url": "http://arxiv.org/abs/2204.02395v1", "pdf_url": "http://arxiv.org/pdf/2204.02395v1", "repo_url": null}, "2204.02393": {"publish_time": "2022-04-05", "title": "Action-Conditioned Contrastive Policy Pretraining", "author": "Qihang Zhang et.al.", "abstract": "Deep visuomotor policy learning achieves promising results in control tasks such as robotic manipulation and autonomous driving, where the action is generated from the visual input by the neural policy. However, it requires a huge number of online interactions with the training environment, which limits its real-world application. Compared to the popular unsupervised feature learning for visual recognition, feature pretraining for visuomotor control tasks is much less explored. In this work, we aim to pretrain policy representations for driving tasks using hours-long uncurated YouTube videos. A new contrastive policy pretraining method is developed to learn action-conditioned features from video frames with action pseudo labels. Experiments show that the resulting action-conditioned features bring substantial improvements to the downstream reinforcement learning and imitation learning tasks, outperforming the weights pretrained from previous unsupervised learning methods. Code and models will be made publicly available.", "paper_url": "http://arxiv.org/abs/2204.02393v1", "pdf_url": "http://arxiv.org/pdf/2204.02393v1", "repo_url": null}, "2204.02390": {"publish_time": "2022-04-05", "title": "Learning Pneumatic Non-Prehensile Manipulation with a Mobile Blower", "author": "Jimmy Wu et.al.", "abstract": "We investigate pneumatic non-prehensile manipulation (i.e., blowing) as a means of efficiently moving scattered objects into a target receptacle. Due to the chaotic nature of aerodynamic forces, a blowing controller must (i) continually adapt to unexpected changes from its actions, (ii) maintain fine-grained control, since the slightest misstep can result in large unintended consequences (e.g., scatter objects already in a pile), and (iii) infer long-range plans (e.g., move the robot to strategic blowing locations). We tackle these challenges in the context of deep reinforcement learning, introducing a multi-frequency version of the spatial action maps framework. This allows for efficient learning of vision-based policies that effectively combine high-level planning and low-level closed-loop control for dynamic mobile manipulation. Experiments show that our system learns efficient behaviors for the task, demonstrating in particular that blowing achieves better downstream performance than pushing, and that our policies improve performance over baselines. Moreover, we show that our system naturally encourages emergent specialization between the different subpolicies spanning low-level fine-grained control and high-level planning. On a real mobile robot equipped with a miniature air blower, we show that our simulation-trained policies transfer well to a real environment and can generalize to novel objects.", "paper_url": "http://arxiv.org/abs/2204.02390v1", "pdf_url": "http://arxiv.org/pdf/2204.02390v1", "repo_url": null}, "2204.02372": {"publish_time": "2022-04-05", "title": "Jump-Start Reinforcement Learning", "author": "Ikechukwu Uchendu et.al.", "abstract": "Reinforcement learning (RL) provides a theoretical framework for continuously improving an agent's behavior via trial and error. However, efficiently learning policies from scratch can be very difficult, particularly for tasks with exploration challenges. In such settings, it might be desirable to initialize RL with an existing policy, offline data, or demonstrations. However, naively performing such initialization in RL often works poorly, especially for value-based methods. In this paper, we present a meta algorithm that can use offline data, demonstrations, or a pre-existing policy to initialize an RL policy, and is compatible with any RL approach. In particular, we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that employs two policies to solve tasks: a guide-policy, and an exploration-policy. By using the guide-policy to form a curriculum of starting states for the exploration-policy, we are able to efficiently improve performance on a set of simulated robotic tasks. We show via experiments that JSRL is able to significantly outperform existing imitation and reinforcement learning algorithms, particularly in the small-data regime. In addition, we provide an upper bound on the sample complexity of JSRL and show that with the help of a guide-policy, one can improve the sample complexity for non-optimism exploration methods from exponential in horizon to polynomial.", "paper_url": "http://arxiv.org/abs/2204.02372v1", "pdf_url": "http://arxiv.org/pdf/2204.02372v1", "repo_url": null}, "2204.02320": {"publish_time": "2022-04-05", "title": "Learning Generalizable Dexterous Manipulation from Human Grasp Affordance", "author": "Yueh-Hua Wu et.al.", "abstract": "Dexterous manipulation with a multi-finger hand is one of the most challenging problems in robotics. While recent progress in imitation learning has largely improved the sample efficiency compared to Reinforcement Learning, the learned policy can hardly generalize to manipulate novel objects, given limited expert demonstrations. In this paper, we propose to learn dexterous manipulation using large-scale demonstrations with diverse 3D objects in a category, which are generated from a human grasp affordance model. This generalizes the policy to novel object instances within the same category. To train the policy, we propose a novel imitation learning objective jointly with a geometric representation learning objective using our demonstrations. By experimenting with relocating diverse objects in simulation, we show that our approach outperforms baselines with a large margin when manipulating novel objects. We also ablate the importance on 3D object representation learning for manipulation. We include videos, code, and additional information on the project website - https://kristery.github.io/ILAD/ .", "paper_url": "http://arxiv.org/abs/2204.02320v1", "pdf_url": "http://arxiv.org/pdf/2204.02320v1", "repo_url": null}, "2204.02877": {"publish_time": "2022-04-06", "title": "PAnDR: Fast Adaptation to New Environments from Offline Experiences via Decoupling Policy and Environment Representations", "author": "Tong Sang et.al.", "abstract": "Deep Reinforcement Learning (DRL) has been a promising solution to many complex decision-making problems. Nevertheless, the notorious weakness in generalization among environments prevent widespread application of DRL agents in real-world scenarios. Although advances have been made recently, most prior works assume sufficient online interaction on training environments, which can be costly in practical cases. To this end, we focus on an \\textit{offline-training-online-adaptation} setting, in which the agent first learns from offline experiences collected in environments with different dynamics and then performs online policy adaptation in environments with new dynamics. In this paper, we propose Policy Adaptation with Decoupled Representations (PAnDR) for fast policy adaptation. In offline training phase, the environment representation and policy representation are learned through contrastive learning and policy recovery, respectively. The representations are further refined by mutual information optimization to make them more decoupled and complete. With learned representations, a Policy-Dynamics Value Function (PDVF) (Raileanu et al., 2020) network is trained to approximate the values for different combinations of policies and environments. In online adaptation phase, with the environment context inferred from few experiences collected in new environments, the policy is optimized by gradient ascent with respect to the PDVF. Our experiments show that PAnDR outperforms existing algorithms in several representative policy adaptation problems.", "paper_url": "http://arxiv.org/abs/2204.02877v1", "pdf_url": "http://arxiv.org/pdf/2204.02877v1", "repo_url": null}, "2204.02737": {"publish_time": "2022-04-06", "title": "Adversarial Learning to Reason in an Arbitrary Logic", "author": "Stanis\u0142aw J. Purga\u0142 et.al.", "abstract": "Existing approaches to learning to prove theorems focus on particular logics and datasets. In this work, we propose Monte-Carlo simulations guided by reinforcement learning that can work in an arbitrarily specified logic, without any human knowledge or set of problems. Since the algorithm does not need any training dataset, it is able to learn to work with any logical foundation, even when there is no body of proofs or even conjectures available. We practically demonstrate the feasibility of the approach in multiple logical systems. The approach is stronger than training on randomly generated data but weaker than the approaches trained on tailored axiom and conjecture sets. It however allows us to apply machine learning to automated theorem proving for many logics, where no such attempts have been tried to date, such as intuitionistic logic or linear logic.", "paper_url": "http://arxiv.org/abs/2204.02737v1", "pdf_url": "http://arxiv.org/pdf/2204.02737v1", "repo_url": null}, "2204.02654": {"publish_time": "2022-04-06", "title": "Adversarial Analysis of the Differentially-Private Federated Learning in Cyber-Physical Critical Infrastructures", "author": "Md Tamjid Hossain et.al.", "abstract": "Differential privacy (DP) is considered to be an effective privacy-preservation method to secure the promising distributed machine learning (ML) paradigm-federated learning (FL) from privacy attacks (e.g., membership inference attack). Nevertheless, while the DP mechanism greatly alleviates privacy concerns, recent studies have shown that it can be exploited to conduct security attacks (e.g., false data injection attacks). To address such attacks on FL-based applications in critical infrastructures, in this paper, we perform the first systematic study on the DP-exploited poisoning attacks from an adversarial point of view. We demonstrate that the DP method, despite providing a level of privacy guarantee, can effectively open a new poisoning attack vector for the adversary. Our theoretical analysis and empirical evaluation of a smart grid dataset show the FL performance degradation (sub-optimal model generation) scenario due to the differential noise-exploited selective model poisoning attacks. As a countermeasure, we propose a reinforcement learning-based differential privacy level selection (rDP) process. The rDP process utilizes the differential privacy parameters (privacy loss, information leakage probability, etc.) and the losses to intelligently generate an optimal privacy level for the nodes. The evaluation shows the accumulated reward and errors of the proposed technique converge to an optimal privacy policy.", "paper_url": "http://arxiv.org/abs/2204.02654v1", "pdf_url": "http://arxiv.org/pdf/2204.02654v1", "repo_url": null}, "2204.02634": {"publish_time": "2022-04-06", "title": "Federated Reinforcement Learning with Environment Heterogeneity", "author": "Hao Jin et.al.", "abstract": "We study a Federated Reinforcement Learning (FedRL) problem in which $n$ agents collaboratively learn a single policy without sharing the trajectories they collected during agent-environment interaction. We stress the constraint of environment heterogeneity, which means $n$ environments corresponding to these $n$ agents have different state transitions. To obtain a value function or a policy function which optimizes the overall performance in all environments, we propose two federated RL algorithms, \\texttt{QAvg} and \\texttt{PAvg}. We theoretically prove that these algorithms converge to suboptimal solutions, while such suboptimality depends on how heterogeneous these $n$ environments are. Moreover, we propose a heuristic that achieves personalization by embedding the $n$ environments into $n$ vectors. The personalization heuristic not only improves the training but also allows for better generalization to new environments.", "paper_url": "http://arxiv.org/abs/2204.02634v1", "pdf_url": "http://arxiv.org/pdf/2204.02634v1", "repo_url": "https://github.com/pengyang7881187/fedrl"}, "2204.02558": {"publish_time": "2022-04-06", "title": "DouZero+: Improving DouDizhu AI by Opponent Modeling and Coach-guided Learning", "author": "Youpeng Zhao et.al.", "abstract": "Recent years have witnessed the great breakthrough of deep reinforcement learning (DRL) in various perfect and imperfect information games. Among these games, DouDizhu, a popular card game in China, is very challenging due to the imperfect information, large state space, elements of collaboration and a massive number of possible moves from turn to turn. Recently, a DouDizhu AI system called DouZero has been proposed. Trained using traditional Monte Carlo method with deep neural networks and self-play procedure without the abstraction of human prior knowledge, DouZero has outperformed all the existing DouDizhu AI programs. In this work, we propose to enhance DouZero by introducing opponent modeling into DouZero. Besides, we propose a novel coach network to further boost the performance of DouZero and accelerate its training process. With the integration of the above two techniques into DouZero, our DouDizhu AI system achieves better performance and ranks top in the Botzone leaderboard among more than 400 AI agents, including DouZero.", "paper_url": "http://arxiv.org/abs/2204.02558v1", "pdf_url": "http://arxiv.org/pdf/2204.02558v1", "repo_url": null}, "2204.03597": {"publish_time": "2022-04-07", "title": "Imitating, Fast and Slow: Robust learning from demonstrations via decision-time planning", "author": "Carl Qi et.al.", "abstract": "The goal of imitation learning is to mimic expert behavior from demonstrations, without access to an explicit reward signal. A popular class of approach infers the (unknown) reward function via inverse reinforcement learning (IRL) followed by maximizing this reward function via reinforcement learning (RL). The policies learned via these approaches are however very brittle in practice and deteriorate quickly even with small test-time perturbations due to compounding errors. We propose Imitation with Planning at Test-time (IMPLANT), a new meta-algorithm for imitation learning that utilizes decision-time planning to correct for compounding errors of any base imitation policy. In contrast to existing approaches, we retain both the imitation policy and the rewards model at decision-time, thereby benefiting from the learning signal of the two components. Empirically, we demonstrate that IMPLANT significantly outperforms benchmark imitation learning approaches on standard control environments and excels at zero-shot generalization when subject to challenging perturbations in test-time dynamics.", "paper_url": "http://arxiv.org/abs/2204.03597v1", "pdf_url": "http://arxiv.org/pdf/2204.03597v1", "repo_url": null}, "2204.03525": {"publish_time": "2022-04-07", "title": "Temporal Alignment for History Representation in Reinforcement Learning", "author": "Aleksandr Ermolov et.al.", "abstract": "Environments in Reinforcement Learning are usually only partially observable. To address this problem, a possible solution is to provide the agent with information about the past. However, providing complete observations of numerous steps can be excessive. Inspired by human memory, we propose to represent history with only important changes in the environment and, in our approach, to obtain automatically this representation using self-supervision. Our method (TempAl) aligns temporally-close frames, revealing a general, slowly varying state of the environment. This procedure is based on contrastive loss, which pulls embeddings of nearby observations to each other while pushing away other samples from the batch. It can be interpreted as a metric that captures the temporal relations of observations. We propose to combine both common instantaneous and our history representation and we evaluate TempAl on all available Atari games from the Arcade Learning Environment. TempAl surpasses the instantaneous-only baseline in 35 environments out of 49. The source code of the method and of all the experiments is available at https://github.com/htdt/tempal.", "paper_url": "http://arxiv.org/abs/2204.03525v1", "pdf_url": "http://arxiv.org/pdf/2204.03525v1", "repo_url": "https://github.com/htdt/tempal"}, "2204.03516": {"publish_time": "2022-04-07", "title": "Distributed Reinforcement Learning for Robot Teams: A Review", "author": "Yutong Wang et.al.", "abstract": "Purpose of review: Recent advances in sensing, actuation, and computation have opened the door to multi-robot systems consisting of hundreds/thousands of robots, with promising applications to automated manufacturing, disaster relief, harvesting, last-mile delivery, port/airport operations, or search and rescue. The community has leveraged model-free multi-agent reinforcement learning (MARL) to devise efficient, scalable controllers for multi-robot systems (MRS). This review aims to provide an analysis of the state-of-the-art in distributed MARL for multi-robot cooperation.   Recent findings: Decentralized MRS face fundamental challenges, such as non-stationarity and partial observability. Building upon the \"centralized training, decentralized execution\" paradigm, recent MARL approaches include independent learning, centralized critic, value decomposition, and communication learning approaches. Cooperative behaviors are demonstrated through AI benchmarks and fundamental real-world robotic capabilities such as multi-robot motion/path planning.   Summary: This survey reports the challenges surrounding decentralized model-free MARL for multi-robot cooperation and existing classes of approaches. We present benchmarks and robotic applications along with a discussion on current open avenues for research.", "paper_url": "http://arxiv.org/abs/2204.03516v1", "pdf_url": "http://arxiv.org/pdf/2204.03516v1", "repo_url": null}, "2204.03514": {"publish_time": "2022-04-07", "title": "Habitat-Web: Learning Embodied Object-Search Strategies from Human Demonstrations at Scale", "author": "Ram Ramrakhya et.al.", "abstract": "We present a large-scale study of imitating human demonstrations on tasks that require a virtual robot to search for objects in new environments -- (1) ObjectGoal Navigation (e.g. 'find & go to a chair') and (2) Pick&Place (e.g. 'find mug, pick mug, find counter, place mug on counter'). First, we develop a virtual teleoperation data-collection infrastructure -- connecting Habitat simulator running in a web browser to Amazon Mechanical Turk, allowing remote users to teleoperate virtual robots, safely and at scale. We collect 80k demonstrations for ObjectNav and 12k demonstrations for Pick&Place, which is an order of magnitude larger than existing human demonstration datasets in simulation or on real robots.   Second, we attempt to answer the question -- how does large-scale imitation learning (IL) (which hasn't been hitherto possible) compare to reinforcement learning (RL) (which is the status quo)? On ObjectNav, we find that IL (with no bells or whistles) using 70k human demonstrations outperforms RL using 240k agent-gathered trajectories. The IL-trained agent demonstrates efficient object-search behavior -- it peeks into rooms, checks corners for small objects, turns in place to get a panoramic view -- none of these are exhibited as prominently by the RL agent, and to induce these behaviors via RL would require tedious reward engineering. Finally, accuracy vs. training data size plots show promising scaling behavior, suggesting that simply collecting more demonstrations is likely to advance the state of art further. On Pick&Place, the comparison is starker -- IL agents achieve ${\\sim}$18% success on episodes with new object-receptacle locations when trained with 9.5k human demonstrations, while RL agents fail to get beyond 0%. Overall, our work provides compelling evidence for investing in large-scale imitation learning.   Project page: https://ram81.github.io/projects/habitat-web.", "paper_url": "http://arxiv.org/abs/2204.03514v1", "pdf_url": "http://arxiv.org/pdf/2204.03514v1", "repo_url": null}, "2204.03487": {"publish_time": "2022-04-07", "title": "Optimizing the Long-Term Behaviour of Deep Reinforcement Learning for Pushing and Grasping", "author": "Rodrigo Chau et.al.", "abstract": "We investigate the \"Visual Pushing for Grasping\" (VPG) system by Zeng et al. and the \"Hourglass\" system by Ewerton et al., an evolution of the former. The focus of our work is the investigation of the capabilities of both systems to learn long-term rewards and policies. Zeng et al. original task only needs a limited amount of foresight. Ewerton et al. attain their best performance using an agent which only takes the most immediate action under consideration. We are interested in the ability of their models and training algorithms to accurately predict long-term Q-Values. To evaluate this ability, we design a new bin sorting task and reward function. Our task requires agents to accurately estimate future rewards and therefore use high discount factors in their Q-Value calculation. We investigate the behaviour of an adaptation of the VPG training algorithm on our task. We show that this adaptation can not accurately predict the required long-term action sequences. In addition to the limitations identified by Ewerton et al., it suffers from the known Deep Q-Learning problem of overestimated Q-Values. In an effort to solve our task, we turn to the Hourglass models and combine them with the Double Q-Learning approach. We show that this approach enables the models to accurately predict long-term action sequences when trained with large discount factors. Our results show that the Double Q-Learning technique is essential for training with very high discount factors, as the models Q-Value predictions diverge otherwise. We also experiment with different approaches for discount factor scheduling, loss calculation and exploration procedures. Our results show that the latter factors do not visibly influence the model's performance for our task.", "paper_url": "http://arxiv.org/abs/2204.03487v1", "pdf_url": "http://arxiv.org/pdf/2204.03487v1", "repo_url": null}, "2204.04198": {"publish_time": "2022-04-08", "title": "Modern applications of machine learning in quantum sciences", "author": "Anna Dawid et.al.", "abstract": "In these Lecture Notes, we provide a comprehensive introduction to the most recent advances in the application of machine learning methods in quantum sciences. We cover the use of deep learning and kernel methods in supervised, unsupervised, and reinforcement learning algorithms for phase classification, representation of many-body quantum states, quantum feedback control, and quantum circuits optimization. Moreover, we introduce and discuss more specialized topics such as differentiable programming, generative models, statistical approach to machine learning, and quantum machine learning.", "paper_url": "http://arxiv.org/abs/2204.04198v1", "pdf_url": "http://arxiv.org/pdf/2204.04198v1", "repo_url": null}, "2204.04157": {"publish_time": "2022-04-08", "title": "Custom Sine Waves Are Enough for Imitation Learning of Bipedal Gaits with Different Styles", "author": "Qi Wu et.al.", "abstract": "Not until recently, robust bipedal locomotion has been achieved through reinforcement learning. However, existing implementations rely heavily on insights and efforts from human experts, which is costly for the iterative design of robot systems. Also, styles of the learned motion are strictly limited to that of the reference. In this paper, we propose a new way to learn bipedal locomotion from a simple sine wave as the reference for foot heights. With the naive human insight that the two feet should be lifted up alternatively and periodically, we experimentally demonstrate on the Cassie robot that, a simple reward function is able to make the robot learn to walk end-to-end and efficiently without any explicit knowledge of the model. With custom sine waves, the learned gait pattern can also have customized styles. Codes will be released at github.com/WooQi57/sin-cassie-rl.", "paper_url": "http://arxiv.org/abs/2204.04157v1", "pdf_url": "http://arxiv.org/pdf/2204.04157v1", "repo_url": null}, "2204.03991": {"publish_time": "2022-04-08", "title": "The Complexity of Markov Equilibrium in Stochastic Games", "author": "Constantinos Daskalakis et.al.", "abstract": "We show that computing approximate stationary Markov coarse correlated equilibria (CCE) in general-sum stochastic games is computationally intractable, even when there are two players, the game is turn-based, the discount factor is an absolute constant, and the approximation is an absolute constant. Our intractability results stand in sharp contrast to normal-form games where exact CCEs are efficiently computable. A fortiori, our results imply that there are no efficient algorithms for learning stationary Markov CCE policies in multi-agent reinforcement learning (MARL), even when the interaction is two-player and turn-based, and both the discount factor and the desired approximation of the learned policies is an absolute constant. In turn, these results stand in sharp contrast to single-agent reinforcement learning (RL) where near-optimal stationary Markov policies can be efficiently learned. Complementing our intractability results for stationary Markov CCEs, we provide a decentralized algorithm (assuming shared randomness among players) for learning a nonstationary Markov CCE policy with polynomial time and sample complexity in all problem parameters. Previous work for learning Markov CCE policies all required exponential time and sample complexity in the number of players.", "paper_url": "http://arxiv.org/abs/2204.03991v1", "pdf_url": "http://arxiv.org/pdf/2204.03991v1", "repo_url": null}, "2204.03973": {"publish_time": "2022-04-08", "title": "Gradient dynamics in reinforcement learning", "author": "Riccardo Fabbricatore et.al.", "abstract": "Despite the success achieved by the analysis of supervised learning algorithms in the framework of statistical mechanics, reinforcement learning has remained largely untouched. Here we move towards closing the gap by analyzing the dynamics of the policy gradient algorithm. For a convex problem, we show that it obeys a drift-diffusion motion with coeffcients tuned by learning rate. Furthermore, we propose a mapping between a non-convex reinforcement learning problem and a disordered system. This mapping enables us to show how the learning rate acts as an effective temperature and thus is capable of smoothing rough landscapes, corroborating what is displayed by the drift-diffusive description and paving the way for physics-inspired algorithmic optimization based on annealing procedures in disordered systems.", "paper_url": "http://arxiv.org/abs/2204.03973v1", "pdf_url": "http://arxiv.org/pdf/2204.03973v1", "repo_url": null}, "2204.03897": {"publish_time": "2022-04-08", "title": "Sim-to-Real Learning of Robust Compliant Bipedal Locomotion on Torque Sensor-Less Gear-Driven Humanoid", "author": "Shimpei Masuda et.al.", "abstract": "In deep reinforcement learning, sim-to-real is the mainstream method as it needs a large number of trials, however, it is challenging to transfer trained policy due to reality gap. In particular, it is known that the characteristics of actuators in leg robots have a considerable influence on the reality gap, and this is also noticeable in high reduction ratio gears. Therefore, we propose a new simulation model of high reduction ratio gears to reduce the reality gap. The instability of the bipedal locomotion causes the sim-to-real transfer to fail catastrophically, making system identification of the physical parameters of the simulation difficult. Thus, we also propose a system identification method that utilizes the failure experience. The realistic simulations obtained by these improvements allow the robot to perform compliant bipedal locomotion by reinforcement learning. The effectiveness of the method is verified using a actual biped robot, ROBOTIS-OP3, and the sim-to-real transferred policy archived to stabilize the robot under severe disturbances and walk on uneven terrain without force and torque sensors.", "paper_url": "http://arxiv.org/abs/2204.03897v1", "pdf_url": "http://arxiv.org/pdf/2204.03897v1", "repo_url": null}, "2204.05275": {"publish_time": "2022-04-11", "title": "Settling the Sample Complexity of Model-Based Offline Reinforcement Learning", "author": "Gen Li et.al.", "abstract": "This paper is concerned with offline reinforcement learning (RL), which learns using pre-collected data without further exploration. Effective offline RL would be able to accommodate distribution shift and limited data coverage. However, prior algorithms or analyses either suffer from suboptimal sample complexities or incur high burn-in cost to reach sample optimality, thus posing an impediment to efficient offline RL in sample-starved applications.   We demonstrate that the model-based (or \"plug-in\") approach achieves minimax-optimal sample complexity without burn-in cost for tabular Markov decision processes (MDPs). Concretely, consider a finite-horizon (resp. $\\gamma$-discounted infinite-horizon) MDP with $S$ states and horizon $H$ (resp. effective horizon $\\frac{1}{1-\\gamma}$), and suppose the distribution shift of data is reflected by some single-policy clipped concentrability coefficient $C^{\\star}_{\\text{clipped}}$. We prove that model-based offline RL yields $\\varepsilon$-accuracy with a sample complexity of \\[ \\begin{cases} \\frac{H^{4}SC_{\\text{clipped}}^{\\star}}{\\varepsilon^{2}} & (\\text{finite-horizon MDPs}) \\frac{SC_{\\text{clipped}}^{\\star}}{(1-\\gamma)^{3}\\varepsilon^{2}} & (\\text{infinite-horizon MDPs}) \\end{cases} \\] up to log factor, which is minimax optimal for the entire $\\varepsilon$-range. Our algorithms are \"pessimistic\" variants of value iteration with Bernstein-style penalties, and do not require sophisticated variance reduction.", "paper_url": "http://arxiv.org/abs/2204.05275v1", "pdf_url": "http://arxiv.org/pdf/2204.05275v1", "repo_url": null}, "2204.05263": {"publish_time": "2022-04-11", "title": "Maximum entropy optimal density control of discrete-time linear systems and Schr\u00f6dinger bridges", "author": "Kaito Ito et.al.", "abstract": "We consider an entropy-regularized version of optimal density control of deterministic discrete-time linear systems. Entropy regularization, or a maximum entropy (MaxEnt) method for optimal control has attracted much attention especially in reinforcement learning due to its many advantages such as a natural exploration strategy. Despite the merits, high-entropy control policies introduce probabilistic uncertainty into systems, which severely limits the applicability of MaxEnt optimal control to safety-critical systems. To remedy this situation, we impose a Gaussian density constraint at a specified time on the MaxEnt optimal control to directly control state uncertainty. Specifically, we derive the explicit form of the MaxEnt optimal density control. In addition, we also consider the case where a density constraint is replaced by a fixed point constraint. Then, we characterize the associated state process as a pinned process, which is a generalization of the Brownian bridge to linear systems. Finally, we reveal that the MaxEnt optimal density control induces the so-called Schr\\\"odinger bridge associated to a discrete-time linear system.", "paper_url": "http://arxiv.org/abs/2204.05263v1", "pdf_url": "http://arxiv.org/pdf/2204.05263v1", "repo_url": null}, "2204.05196": {"publish_time": "2022-04-11", "title": "Automatically Learning Fallback Strategies with Model-Free Reinforcement Learning in Safety-Critical Driving Scenarios", "author": "Ugo Lecerf et.al.", "abstract": "When learning to behave in a stochastic environment where safety is critical, such as driving a vehicle in traffic, it is natural for human drivers to plan fallback strategies as a backup to use if ever there is an unexpected change in the environment. Knowing to expect the unexpected, and planning for such outcomes, increases our capability for being robust to unseen scenarios and may help prevent catastrophic failures. Control of Autonomous Vehicles (AVs) has a particular interest in knowing when and how to use fallback strategies in the interest of safety. Due to imperfect information available to an AV about its environment, it is important to have alternate strategies at the ready which might not have been deduced from the original training data distribution.   In this paper we present a principled approach for a model-free Reinforcement Learning (RL) agent to capture multiple modes of behaviour in an environment. We introduce an extra pseudo-reward term to the reward model, to encourage exploration to areas of state-space different from areas privileged by the optimal policy. We base this reward term on a distance metric between the trajectories of agents, in order to force policies to focus on different areas of state-space than the initial exploring agent. Throughout the paper, we refer to this particular training paradigm as learning fallback strategies.   We apply this method to an autonomous driving scenario, and show that we are able to learn useful policies that would have otherwise been missed out on during training, and unavailable to use when executing the control algorithm.", "paper_url": "http://arxiv.org/abs/2204.05196v1", "pdf_url": "http://arxiv.org/pdf/2204.05196v1", "repo_url": null}, "2204.05036": {"publish_time": "2022-04-11", "title": "Pareto Conditioned Networks", "author": "Mathieu Reymond et.al.", "abstract": "In multi-objective optimization, learning all the policies that reach Pareto-efficient solutions is an expensive process. The set of optimal policies can grow exponentially with the number of objectives, and recovering all solutions requires an exhaustive exploration of the entire state space. We propose Pareto Conditioned Networks (PCN), a method that uses a single neural network to encompass all non-dominated policies. PCN associates every past transition with its episode's return. It trains the network such that, when conditioned on this same return, it should reenact said transition. In doing so we transform the optimization problem into a classification problem. We recover a concrete policy by conditioning the network on the desired Pareto-efficient solution. Our method is stable as it learns in a supervised fashion, thus avoiding moving target issues. Moreover, by using a single network, PCN scales efficiently with the number of objectives. Finally, it makes minimal assumptions on the shape of the Pareto front, which makes it suitable to a wider range of problems than previous state-of-the-art multi-objective reinforcement learning algorithms.", "paper_url": "http://arxiv.org/abs/2204.05036v1", "pdf_url": "http://arxiv.org/pdf/2204.05036v1", "repo_url": null}, "2204.05027": {"publish_time": "2022-04-11", "title": "Exploring the Pareto front of multi-objective COVID-19 mitigation policies using reinforcement learning", "author": "Mathieu Reymond et.al.", "abstract": "Infectious disease outbreaks can have a disruptive impact on public health and societal processes. As decision making in the context of epidemic mitigation is hard, reinforcement learning provides a methodology to automatically learn prevention strategies in combination with complex epidemic models. Current research focuses on optimizing policies w.r.t. a single objective, such as the pathogen's attack rate. However, as the mitigation of epidemics involves distinct, and possibly conflicting criteria (i.a., prevalence, mortality, morbidity, cost), a multi-objective approach is warranted to learn balanced policies. To lift this decision-making process to real-world epidemic models, we apply deep multi-objective reinforcement learning and build upon a state-of-the-art algorithm, Pareto Conditioned Networks (PCN), to learn a set of solutions that approximates the Pareto front of the decision problem. We consider the first wave of the Belgian COVID-19 epidemic, which was mitigated by a lockdown, and study different deconfinement strategies, aiming to minimize both COVID-19 cases (i.e., infections and hospitalizations) and the societal burden that is induced by the applied mitigation measures. We contribute a multi-objective Markov decision process that encapsulates the stochastic compartment model that was used to inform policy makers during the COVID-19 epidemic. As these social mitigation measures are implemented in a continuous action space that modulates the contact matrix of the age-structured epidemic model, we extend PCN to this setting. We evaluate the solution returned by PCN, and observe that it correctly learns to reduce the social burden whenever the hospitalization rates are sufficiently low. In this work, we thus show that multi-objective reinforcement learning is attainable in complex epidemiological models and provides essential insights to balance complex mitigation policies.", "paper_url": "http://arxiv.org/abs/2204.05027v1", "pdf_url": "http://arxiv.org/pdf/2204.05027v1", "repo_url": null}, "2204.05951": {"publish_time": "2022-04-12", "title": "RL-CoSeg : A Novel Image Co-Segmentation Algorithm with Deep Reinforcement Learning", "author": "Xin Duan et.al.", "abstract": "This paper proposes an automatic image co-segmentation algorithm based on deep reinforcement learning (RL). Existing co-segmentation tasks mainly rely on deep learning methods, and the obtained foreground edges are often rough. In order to obtain more precise foreground edges, we use deep RL to solve this problem and achieve the finer segmentation. To our best knowledge, this is the first work to apply RL methods to co-segmentation. We define the problem as a Markov Decision Process (MDP) and optimize it by RL with asynchronous advantage actor-critic (A3C). The RL image co-segmentation network uses the correlation between images to segment common and salient objects from a set of related images. In order to achieve automatic segmentation, our RL-CoSeg method eliminates user's hints. For the image co-segmentation problem, we propose a collaborative RL algorithm based on the A3C model. We propose a Siamese RL co-segmentation network structure to obtain the co-attention of images for co-segmentation. We improve the self-attention for automatic RL algorithm to obtain long-distance dependence and enlarge the receptive field. The image feature information obtained by self-attention can be used to supplement the deleted user's hints and help to obtain more accurate actions. Experimental results have shown that our method can improve the performance effectively on both coarse and fine initial segmentations, and it achieves the state-of-the-art performance on Internet dataset, iCoseg dataset and MLMR-COS dataset.", "paper_url": "http://arxiv.org/abs/2204.05951v1", "pdf_url": "http://arxiv.org/pdf/2204.05951v1", "repo_url": null}, "2204.05928": {"publish_time": "2022-04-12", "title": "Dynamic Dialogue Policy Transformer for Continual Reinforcement Learning", "author": "Christian Geishauser et.al.", "abstract": "Continual learning is one of the key components of human learning and a necessary requirement of artificial intelligence. As dialogue can potentially span infinitely many topics and tasks, a task-oriented dialogue system must have the capability to continually learn, dynamically adapting to new challenges while preserving the knowledge it already acquired. Despite the importance, continual reinforcement learning of the dialogue policy has remained largely unaddressed. The lack of a framework with training protocols, baseline models and suitable metrics, has so far hindered research in this direction. In this work we fill precisely this gap, enabling research in dialogue policy optimisation to go from static to dynamic learning. We provide a continual learning algorithm, baseline architectures and metrics for assessing continual learning models. Moreover, we propose the dynamic dialogue policy transformer (DDPT), a novel dynamic architecture that can integrate new knowledge seamlessly, is capable of handling large state spaces and obtains significant zero-shot performance when being exposed to unseen domains, without any growth in network parameter size.", "paper_url": "http://arxiv.org/abs/2204.05928v1", "pdf_url": "http://arxiv.org/pdf/2204.05928v1", "repo_url": null}, "2204.05862": {"publish_time": "2022-04-12", "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback", "author": "Yuntao Bai et.al.", "abstract": "We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.", "paper_url": "http://arxiv.org/abs/2204.05862v1", "pdf_url": "http://arxiv.org/pdf/2204.05862v1", "repo_url": "https://github.com/anthropics/hh-rlhf"}, "2204.05669": {"publish_time": "2022-04-12", "title": "An Analysis of Discretization Methods for Communication Learning with Multi-Agent Reinforcement Learning", "author": "Astrid Vanneste et.al.", "abstract": "Communication is crucial in multi-agent reinforcement learning when agents are not able to observe the full state of the environment. The most common approach to allow learned communication between agents is the use of a differentiable communication channel that allows gradients to flow between agents as a form of feedback. However, this is challenging when we want to use discrete messages to reduce the message size since gradients cannot flow through a discrete communication channel. Previous work proposed methods to deal with this problem. However, these methods are tested in different communication learning architectures and environments, making it hard to compare them. In this paper, we compare several state-of-the-art discretization methods as well as two methods that have not been used for communication learning before. We do this comparison in the context of communication learning using gradients from other agents and perform tests on several environments. Our results show that none of the methods is best in all environments. The best choice in discretization method greatly depends on the environment. However, the discretize regularize unit (DRU), straight through DRU and the straight through gumbel softmax show the most consistent results across all the tested environments. Therefore, these methods prove to be the best choice for general use while the straight through estimator and the gumbel softmax may provide better results in specific environments but fail completely in others.", "paper_url": "http://arxiv.org/abs/2204.05669v1", "pdf_url": "http://arxiv.org/pdf/2204.05669v1", "repo_url": null}, "2204.05627": {"publish_time": "2022-04-12", "title": "Proximal Policy Optimization Learning based Control of Congested Freeway Traffic", "author": "Shurong Mo et.al.", "abstract": "This study proposes a delay-compensated feedback controller based on proximal policy optimization (PPO) reinforcement learning to stabilize traffic flow in the congested regime by manipulating the time-gap of adaptive cruise control-equipped (ACC-equipped) vehicles.The traffic dynamics on a freeway segment are governed by an Aw-Rascle-Zhang (ARZ) model, consisting of $2\\times 2$ nonlinear first-order partial differential equations (PDEs).Inspired by the backstepping delay compensator [18] but different from whose complex segmented control scheme, the PPO control is composed of three feedbacks, namely the current traffic flow velocity, the current traffic flow density and previous one step control input. The control gains for the three feedbacks are learned from the interaction between the PPO and the numerical simulator of the traffic system without knowing the system dynamics. Numerical simulation experiments are designed to compare the Lyapunov control, the backstepping control and the PPO control. The results show that for a delay-free system, the PPO control has faster convergence rate and less control effort than the Lyapunov control. For a traffic system with input delay, the performance of the PPO controller is comparable to that of the Backstepping controller, even for the situation that the delay value does not match. However, the PPO is robust to parameter perturbations, while the Backstepping controller cannot stabilize a system where one of the parameters is disturbed by Gaussian noise.", "paper_url": "http://arxiv.org/abs/2204.05627v1", "pdf_url": "http://arxiv.org/pdf/2204.05627v1", "repo_url": null}}}, "Unsupervised Learning": {"Continual Learning": {"2202.11918": {"publish_time": "2022-02-24", "title": "Phase Continuity: Learning Derivatives of Phase Spectrum for Speech Enhancement", "author": "Doyeon Kim et.al.", "abstract": "Modern neural speech enhancement models usually include various forms of phase information in their training loss terms, either explicitly or implicitly. However, these loss terms are typically designed to reduce the distortion of phase spectrum values at specific frequencies, which ensures they do not significantly affect the quality of the enhanced speech. In this paper, we propose an effective phase reconstruction strategy for neural speech enhancement that can operate in noisy environments. Specifically, we introduce a phase continuity loss that considers relative phase variations across the time and frequency axes. By including this phase continuity loss in a state-of-the-art neural speech enhancement system trained with reconstruction loss and a number of magnitude spectral losses, we show that our proposed method further improves the quality of enhanced speech signals over the baseline, especially when training is done jointly with a magnitude spectrum loss.", "paper_url": "http://arxiv.org/abs/2202.11918v1", "pdf_url": "http://arxiv.org/pdf/2202.11918v1", "repo_url": null}, "2202.11295": {"publish_time": "2022-02-23", "title": "Continual learning-based probabilistic slow feature analysis for multimode dynamic process monitoring", "author": "Jingxin Zhang et.al.", "abstract": "In this paper, a novel multimode dynamic process monitoring approach is proposed by extending elastic weight consolidation (EWC) to probabilistic slow feature analysis (PSFA) in order to extract multimode slow features for online monitoring. EWC was originally introduced in the setting of machine learning of sequential multi-tasks with the aim of avoiding catastrophic forgetting issue, which equally poses as a major challenge in multimode dynamic process monitoring. When a new mode arrives, a set of data should be collected so that this mode can be identified by PSFA and prior knowledge. Then, a regularization term is introduced to prevent new data from significantly interfering with the learned knowledge, where the parameter importance measures are estimated. The proposed method is denoted as PSFA-EWC, which is updated continually and capable of achieving excellent performance for successive modes. Different from traditional multimode monitoring algorithms, PSFA-EWC furnishes backward and forward transfer ability. The significant features of previous modes are retained while consolidating new information, which may contribute to learning new relevant modes. Compared with several known methods, the effectiveness of the proposed method is demonstrated via a continuous stirred tank heater and a practical coal pulverizing system.", "paper_url": "http://arxiv.org/abs/2202.11295v1", "pdf_url": "http://arxiv.org/pdf/2202.11295v1", "repo_url": null}, "2202.10821": {"publish_time": "2022-02-22", "title": "Increasing Depth of Neural Networks for Life-long Learning", "author": "J\u0119drzej Kozal et.al.", "abstract": "Increasing neural network depth is a well-known method for improving neural network performance. Modern deep architectures contain multiple mechanisms that allow hundreds or even thousands of layers to train. This work is trying to answer if extending neural network depth may be beneficial in a life-long learning setting. In particular, we propose a novel method based on adding new layers on top of existing ones to enable the forward transfer of knowledge and adapting previously learned representations for new tasks. We utilize a method of determining the most similar tasks for selecting the best location in our network to add new nodes with trainable parameters. This approach allows for creating a tree-like model, where each node is a set of neural network parameters dedicated to a specific task. The proposed method is inspired by Progressive Neural Network (PNN) concept, therefore it is rehearsal-free and benefits from dynamic change of network structure. However, it requires fewer parameters per task than PNN. Experiments on Permuted MNIST and SplitCIFAR show that the proposed algorithm is on par with other continual learning methods. We also perform ablation studies to clarify the contributions of each system part.", "paper_url": "http://arxiv.org/abs/2202.10821v1", "pdf_url": "http://arxiv.org/pdf/2202.10821v1", "repo_url": null}, "2202.10788": {"publish_time": "2022-02-22", "title": "Explicit Regularization via Regularizer Mirror Descent", "author": "Navid Azizan et.al.", "abstract": "Despite perfectly interpolating the training data, deep neural networks (DNNs) can often generalize fairly well, in part due to the \"implicit regularization\" induced by the learning algorithm. Nonetheless, various forms of regularization, such as \"explicit regularization\" (via weight decay), are often used to avoid overfitting, especially when the data is corrupted. There are several challenges with explicit regularization, most notably unclear convergence properties. Inspired by convergence properties of stochastic mirror descent (SMD) algorithms, we propose a new method for training DNNs with regularization, called regularizer mirror descent (RMD). In highly overparameterized DNNs, SMD simultaneously interpolates the training data and minimizes a certain potential function of the weights. RMD starts with a standard cost which is the sum of the training loss and a convex regularizer of the weights. Reinterpreting this cost as the potential of an \"augmented\" overparameterized network and applying SMD yields RMD. As a result, RMD inherits the properties of SMD and provably converges to a point \"close\" to the minimizer of this cost. RMD is computationally comparable to stochastic gradient descent (SGD) and weight decay, and is parallelizable in the same manner. Our experimental results on training sets with various levels of corruption suggest that the generalization performance of RMD is remarkably robust and significantly better than both SGD and weight decay, which implicitly and explicitly regularize the $\\ell_2$ norm of the weights. RMD can also be used to regularize the weights to a desired weight vector, which is particularly relevant for continual learning.", "paper_url": "http://arxiv.org/abs/2202.10788v1", "pdf_url": "http://arxiv.org/pdf/2202.10788v1", "repo_url": null}, "2202.10688": {"publish_time": "2022-02-22", "title": "Graph Lifelong Learning: A Survey", "author": "Falih Gozi Febrinanto et.al.", "abstract": "Graph learning substantially contributes to solving artificial intelligence (AI) tasks in various graph-related domains such as social networks, biological networks, recommender systems, and computer vision. However, despite its unprecedented prevalence, addressing the dynamic evolution of graph data over time remains a challenge. In many real-world applications, graph data continuously evolves. Current graph learning methods that assume graph representation is complete before the training process begins are not applicable in this setting. This challenge in graph learning motivates the development of a continuous learning process called graph lifelong learning to accommodate the future and refine the previous knowledge in graph data. Unlike existing survey papers that focus on either lifelong learning or graph learning separately, this survey paper covers the motivations, potentials, state-of-the-art approaches (that are well categorized), and open issues of graph lifelong learning. We expect extensive research and development interest in this emerging field.", "paper_url": "http://arxiv.org/abs/2202.10688v1", "pdf_url": "http://arxiv.org/pdf/2202.10688v1", "repo_url": null}, "2202.13657": {"publish_time": "2022-02-28", "title": "Avalanche RL: a Continual Reinforcement Learning Library", "author": "Nicol\u00f2 Lucchesi et.al.", "abstract": "Continual Reinforcement Learning (CRL) is a challenging setting where an agent learns to interact with an environment that is constantly changing over time (the stream of experiences). In this paper, we describe Avalanche RL, a library for Continual Reinforcement Learning which allows to easily train agents on a continuous stream of tasks. Avalanche RL is based on PyTorch and supports any OpenAI Gym environment. Its design is based on Avalanche, one of the more popular continual learning libraries, which allow us to reuse a large number of continual learning strategies and improve the interaction between reinforcement learning and continual learning researchers. Additionally, we propose Continual Habitat-Lab, a novel benchmark and a high-level library which enables the usage of the photorealistic simulator Habitat-Sim for CRL research. Overall, Avalanche RL attempts to unify under a common framework continual reinforcement learning applications, which we hope will foster the growth of the field.", "paper_url": "http://arxiv.org/abs/2202.13657v1", "pdf_url": "http://arxiv.org/pdf/2202.13657v1", "repo_url": "https://github.com/nicklucche/continual-habitat-lab"}, "2202.13369": {"publish_time": "2022-02-27", "title": "Robust Continual Learning through a Comprehensively Progressive Bayesian Neural Network", "author": "Guo Yang et.al.", "abstract": "This work proposes a comprehensively progressive Bayesian neural network for robust continual learning of a sequence of tasks. A Bayesian neural network is progressively pruned and grown such that there are sufficient network resources to represent a sequence of tasks, while the network does not explode. It starts with the contention that similar tasks should have the same number of total network resources, to ensure fair representation of all tasks in a continual learning scenario. Thus, as the data for new task streams in, sufficient neurons are added to the network such that the total number of neurons in each layer of the network, including the shared representations with previous tasks and individual task related representation, are equal for all tasks. The weights that are redundant at the end of training each task are also pruned through re-initialization, in order to be efficiently utilized in the subsequent task. Thus, the network grows progressively, but ensures effective utilization of network resources. We refer to our proposed method as 'Robust Continual Learning through a Comprehensively Progressive Bayesian Neural Network (RCL-CPB)' and evaluate the proposed approach on the MNIST data set, under three different continual learning scenarios. Further to this, we evaluate the performance of RCL-CPB on a homogeneous sequence of tasks using split CIFAR100 (20 tasks of 5 classes each), and a heterogeneous sequence of tasks using MNIST, SVHN and CIFAR10 data sets. The demonstrations and the performance results show that the proposed strategies for progressive BNN enable robust continual learning.", "paper_url": "http://arxiv.org/abs/2202.13369v1", "pdf_url": "http://arxiv.org/pdf/2202.13369v1", "repo_url": null}, "2203.01012": {"publish_time": "2022-03-02", "title": "Continual Feature Selection: Spurious Features in Continual Learning", "author": "Timoth\u00e9e Lesort et.al.", "abstract": "Continual Learning (CL) is the research field addressing learning settings where the data distribution is not static. This paper studies spurious features' influence on continual learning algorithms. Indeed, we show that learning algorithms solve tasks by overfitting features that are not generalizable. To better understand these phenomena and their impact, we propose a domain incremental scenario that we study through various out-of-distribution generalizations and continual learning algorithms. The experiments of this paper show that continual learning algorithms face two related challenges: (1) the spurious features challenge: some features are well correlated with labels in train data but not in test data due to a covariate shift between train and test. (2) the local spurious features challenge: some features correlate well with labels within a task but not within the whole task sequence. The challenge is to learn general features that are neither spurious (in general) nor locally spurious. We prove that the latter is a major cause of performance decrease in continual learning along with catastrophic forgetting. Our results indicate that the best solution to overcome the feature selection problems varies depending on the correlation between spurious features (SFs) and labels. The vanilla replay approach seems to be a powerful approach to deal with SFs, which could explain its good performance in the continual learning literature. This paper presents a different way of understanding performance decrease in continual learning by describing the influence of spurious/local spurious features.", "paper_url": "http://arxiv.org/abs/2203.01012v1", "pdf_url": "http://arxiv.org/pdf/2203.01012v1", "repo_url": null}, "2203.00936": {"publish_time": "2022-03-02", "title": "Continual Learning of Multi-modal Dynamics with External Memory", "author": "Abdullah Akg\u00fcl et.al.", "abstract": "We study the problem of fitting a model to a dynamical environment when new modes of behavior emerge sequentially. The learning model is aware when a new mode appears, but it does not have access to the true modes of individual training sequences. We devise a novel continual learning method that maintains a descriptor of the mode of an encountered sequence in a neural episodic memory. We employ a Dirichlet Process prior on the attention weights of the memory to foster efficient storage of the mode descriptors. Our method performs continual learning by transferring knowledge across tasks by retrieving the descriptors of similar modes of past tasks to the mode of a current sequence and feeding this descriptor into its transition kernel as control input. We observe the continual learning performance of our method to compare favorably to the mainstream parameter transfer approach.", "paper_url": "http://arxiv.org/abs/2203.00936v1", "pdf_url": "http://arxiv.org/pdf/2203.00936v1", "repo_url": null}, "2203.01578": {"publish_time": "2022-03-03", "title": "Continual SLAM: Beyond Lifelong Simultaneous Localization and Mapping through Continual Learning", "author": "Niclas V\u00f6disch et.al.", "abstract": "While lifelong SLAM addresses the capability of a robot to adapt to changes within a single environment over time, in this paper we introduce the task of continual SLAM. Here, a robot is deployed sequentially in a variety of different environments and has to transfer its knowledge of previously experienced environments to thus far unseen environments, while avoiding catastrophic forgetting. This is particularly relevant in the context of vision-based approaches, where the relevant features vary widely between different environments. We propose a novel approach for solving the continual SLAM problem by introducing CL-SLAM. Our approach consists of a dual-network architecture that handles both short-term adaptation and long-term memory retention by incorporating a replay buffer. Extensive evaluations of CL-SLAM in three different environments demonstrate that it outperforms several baselines inspired by existing continual learning-based visual odometry methods. The code of our work is publicly available at http://continual-slam.cs.uni-freiburg.de.", "paper_url": "http://arxiv.org/abs/2203.01578v1", "pdf_url": "http://arxiv.org/pdf/2203.01578v1", "repo_url": "https://github.com/robot-learning-freiburg/CL-SLAM"}, "2203.02108": {"publish_time": "2022-03-04", "title": "Continual Horizontal Federated Learning for Heterogeneous Data", "author": "Junki Mori et.al.", "abstract": "Federated learning is a promising machine learning technique that enables multiple clients to collaboratively build a model without revealing the raw data to each other. Among various types of federated learning methods, horizontal federated learning (HFL) is the best-studied category and handles homogeneous feature spaces. However, in the case of heterogeneous feature spaces, HFL uses only common features and leaves client-specific features unutilized. In this paper, we propose a HFL method using neural networks named continual horizontal federated learning (CHFL), a continual learning approach to improve the performance of HFL by taking advantage of unique features of each client. CHFL splits the network into two columns corresponding to common features and unique features, respectively. It jointly trains the first column by using common features through vanilla HFL and locally trains the second column by using unique features and leveraging the knowledge of the first one via lateral connections without interfering with the federated training of it. We conduct experiments on various real world datasets and show that CHFL greatly outperforms vanilla HFL that only uses common features and local learning that uses all features that each client has.", "paper_url": "http://arxiv.org/abs/2203.02108v1", "pdf_url": "http://arxiv.org/pdf/2203.02108v1", "repo_url": null}, "2203.02026": {"publish_time": "2022-03-03", "title": "Provable and Efficient Continual Representation Learning", "author": "Yingcong Li et.al.", "abstract": "In continual learning (CL), the goal is to design models that can learn a sequence of tasks without catastrophic forgetting. While there is a rich set of techniques for CL, relatively little understanding exists on how representations built by previous tasks benefit new tasks that are added to the network. To address this, we study the problem of continual representation learning (CRL) where we learn an evolving representation as new tasks arrive. Focusing on zero-forgetting methods where tasks are embedded in subnetworks (e.g., PackNet), we first provide experiments demonstrating CRL can significantly boost sample efficiency when learning new tasks. To explain this, we establish theoretical guarantees for CRL by providing sample complexity and generalization error bounds for new tasks by formalizing the statistical benefits of previously-learned representations. Our analysis and experiments also highlight the importance of the order in which we learn the tasks. Specifically, we show that CL benefits if the initial tasks have large sample size and high \"representation diversity\". Diversity ensures that adding new tasks incurs small representation mismatch and can be learned with few samples while training only few additional nonzero weights. Finally, we ask whether one can ensure each task subnetwork to be efficient during inference time while retaining the benefits of representation learning. To this end, we propose an inference-efficient variation of PackNet called Efficient Sparse PackNet (ESPN) which employs joint channel & weight pruning. ESPN embeds tasks in channel-sparse subnets requiring up to 80% less FLOPs to compute while approximately retaining accuracy and is very competitive with a variety of baselines. In summary, this work takes a step towards data and compute-efficient CL with a representation learning perspective. GitHub page: https://github.com/ucr-optml/CtRL", "paper_url": "http://arxiv.org/abs/2203.02026v1", "pdf_url": "http://arxiv.org/pdf/2203.02026v1", "repo_url": "https://github.com/ucr-optml/ctrl"}, "2203.03303": {"publish_time": "2022-03-07", "title": "PAC-Bayesian Lifelong Learning For Multi-Armed Bandits", "author": "Hamish Flynn et.al.", "abstract": "We present a PAC-Bayesian analysis of lifelong learning. In the lifelong learning problem, a sequence of learning tasks is observed one-at-a-time, and the goal is to transfer information acquired from previous tasks to new learning tasks. We consider the case when each learning task is a multi-armed bandit problem. We derive lower bounds on the expected average reward that would be obtained if a given multi-armed bandit algorithm was run in a new task with a particular prior and for a set number of steps. We propose lifelong learning algorithms that use our new bounds as learning objectives. Our proposed algorithms are evaluated in several lifelong multi-armed bandit problems and are found to perform better than a baseline method that does not use generalisation bounds.", "paper_url": "http://arxiv.org/abs/2203.03303v1", "pdf_url": "http://arxiv.org/pdf/2203.03303v1", "repo_url": null}, "2203.03970": {"publish_time": "2022-03-08", "title": "On Generalizing Beyond Domains in Cross-Domain Continual Learning", "author": "Christian Simon et.al.", "abstract": "Humans have the ability to accumulate knowledge of new tasks in varying conditions, but deep neural networks often suffer from catastrophic forgetting of previously learned knowledge after learning a new task. Many recent methods focus on preventing catastrophic forgetting under the assumption of train and test data following similar distributions. In this work, we consider a more realistic scenario of continual learning under domain shifts where the model must generalize its inference to an unseen domain. To this end, we encourage learning semantically meaningful features by equipping the classifier with class similarity metrics as learning parameters which are obtained through Mahalanobis similarity computations. Learning of the backbone representation along with these extra parameters is done seamlessly in an end-to-end manner. In addition, we propose an approach based on the exponential moving average of the parameters for better knowledge distillation. We demonstrate that, to a great extent, existing continual learning algorithms fail to handle the forgetting issue under multiple distributions, while our proposed approach learns new tasks under domain shift with accuracy boosts up to 10% on challenging datasets such as DomainNet and OfficeHome.", "paper_url": "http://arxiv.org/abs/2203.03970v1", "pdf_url": "http://arxiv.org/pdf/2203.03970v1", "repo_url": null}, "2203.03910": {"publish_time": "2022-03-08", "title": "Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation", "author": "Chenze Shao et.al.", "abstract": "Neural networks tend to gradually forget the previously learned knowledge when learning multiple tasks sequentially from dynamic data distributions. This problem is called \\textit{catastrophic forgetting}, which is a fundamental challenge in the continual learning of neural networks. In this work, we observe that catastrophic forgetting not only occurs in continual learning but also affects the traditional static training. Neural networks, especially neural machine translation models, suffer from catastrophic forgetting even if they learn from a static training set. To be specific, the final model pays imbalanced attention to training samples, where recently exposed samples attract more attention than earlier samples. The underlying cause is that training samples do not get balanced training in each model update, so we name this problem \\textit{imbalanced training}. To alleviate this problem, we propose Complementary Online Knowledge Distillation (COKD), which uses dynamically updated teacher models trained on specific data orders to iteratively provide complementary knowledge to the student model. Experimental results on multiple machine translation tasks show that our method successfully alleviates the problem of imbalanced training and achieves substantial improvements over strong baseline systems.", "paper_url": "http://arxiv.org/abs/2203.03910v1", "pdf_url": "http://arxiv.org/pdf/2203.03910v1", "repo_url": "https://github.com/ictnlp/cokd"}, "2203.03798": {"publish_time": "2022-03-08", "title": "New Insights on Reducing Abrupt Representation Change in Online Continual Learning", "author": "Lucas Caccia et.al.", "abstract": "In the online continual learning paradigm, agents must learn from a changing distribution while respecting memory and compute constraints. Experience Replay (ER), where a small subset of past data is stored and replayed alongside new data, has emerged as a simple and effective learning strategy. In this work, we focus on the change in representations of observed data that arises when previously unobserved classes appear in the incoming data stream, and new classes must be distinguished from previous ones. We shed new light on this question by showing that applying ER causes the newly added classes' representations to overlap significantly with the previous classes, leading to highly disruptive parameter updates. Based on this empirical analysis, we propose a new method which mitigates this issue by shielding the learned representations from drastic adaptation to accommodate new classes. We show that using an asymmetric update rule pushes new classes to adapt to the older ones (rather than the reverse), which is more effective especially at task boundaries, where much of the forgetting typically occurs. Empirical results show significant gains over strong baselines on standard continual learning benchmarks", "paper_url": "http://arxiv.org/abs/2203.03798v1", "pdf_url": "http://arxiv.org/pdf/2203.03798v1", "repo_url": "https://github.com/pclucas14/aml"}, "2203.04640": {"publish_time": "2022-03-09", "title": "Memory Efficient Continual Learning for Neural Text Classification", "author": "Beyza Ermis et.al.", "abstract": "Learning text classifiers based on pre-trained language models has become the standard practice in natural language processing applications. Unfortunately, training large neural language models, such as transformers, from scratch is very costly and requires a vast amount of training data, which might not be available in the application domain of interest. Moreover, in many real-world scenarios, classes are uncovered as more data is seen, calling for class-incremental modelling approaches. In this work we devise a method to perform text classification using pre-trained models on a sequence of classification tasks provided in sequence. We formalize the problem as a continual learning problem where the algorithm learns new tasks without performance degradation on the previous ones and without re-training the model from scratch. We empirically demonstrate that our method requires significantly less model parameters compared to other state of the art methods and that it is significantly faster at inference time. The tight control on the number of model parameters, and so the memory, is not only improving efficiency. It is making possible the usage of the algorithm in real-world applications where deploying a solution with a constantly increasing memory consumption is just unrealistic. While our method suffers little forgetting, it retains a predictive performance on-par with state of the art but less memory efficient methods.", "paper_url": "http://arxiv.org/abs/2203.04640v1", "pdf_url": "http://arxiv.org/pdf/2203.04640v1", "repo_url": null}, "2203.06053": {"publish_time": "2022-03-11", "title": "A Machine Learning Approach for Prosumer Management in Intraday Electricity Markets", "author": "Saeed Mohammadi et.al.", "abstract": "Prosumer operators are dealing with extensive challenges to participate in short-term electricity markets while taking uncertainties into account. Challenges such as variation in demand, solar energy, wind power, and electricity prices as well as faster response time in intraday electricity markets. Machine learning approaches could resolve these challenges due to their ability to continuous learning of complex relations and providing a real-time response. Such approaches are applicable with presence of the high performance computing and big data. To tackle these challenges, a Markov decision process is proposed and solved with a reinforcement learning algorithm with proper observations and actions employing tabular Q-learning. Trained agent converges to a policy which is similar to the global optimal solution. It increases the prosumer's profit by 13.39% compared to the well-known stochastic optimization approach.", "paper_url": "http://arxiv.org/abs/2203.06053v1", "pdf_url": "http://arxiv.org/pdf/2203.06053v1", "repo_url": null}, "2203.05692": {"publish_time": "2022-03-11", "title": "Lifelong Adaptive Machine Learning for Sensor-based Human Activity Recognition Using Prototypical Networks", "author": "Rebecca Adaimi et.al.", "abstract": "Continual learning, also known as lifelong learning, is an emerging research topic that has been attracting increasing interest in the field of machine learning. With human activity recognition (HAR) playing a key role in enabling numerous real-world applications, an essential step towards the long-term deployment of such recognition systems is to extend the activity model to dynamically adapt to changes in people's everyday behavior. Current research in continual learning applied to HAR domain is still under-explored with researchers exploring existing methods developed for computer vision in HAR. Moreover, analysis has so far focused on task-incremental or class-incremental learning paradigms where task boundaries are known. This impedes the applicability of such methods for real-world systems since data is presented in a randomly streaming fashion. To push this field forward, we build on recent advances in the area of continual machine learning and design a lifelong adaptive learning framework using Prototypical Networks, LAPNet-HAR, that processes sensor-based data streams in a task-free data-incremental fashion and mitigates catastrophic forgetting using experience replay and continual prototype adaptation. Online learning is further facilitated using contrastive loss to enforce inter-class separation. LAPNet-HAR is evaluated on 5 publicly available activity datasets in terms of the framework's ability to acquire new information while preserving previous knowledge. Our extensive empirical results demonstrate the effectiveness of LAPNet-HAR in task-free continual learning and uncover useful insights for future challenges.", "paper_url": "http://arxiv.org/abs/2203.05692v1", "pdf_url": "http://arxiv.org/pdf/2203.05692v1", "repo_url": null}, "2203.06855": {"publish_time": "2022-03-14", "title": "DIAS: A Domain-Independent Alife-Based Problem-Solving System", "author": "Babak Hodjat et.al.", "abstract": "A domain-independent problem-solving system based on principles of Artificial Life is introduced. In this system, DIAS, the input and output dimensions of the domain are laid out in a spatial medium. A population of actors, each seeing only part of this medium, solves problems collectively in it. The process is independent of the domain and can be implemented through different kinds of actors. Through a set of experiments on various problem domains, DIAS is shown able to solve problems with different dimensionality and complexity, to require no hyperparameter tuning for new problems, and to exhibit lifelong learning, i.e. adapt rapidly to run-time changes in the problem domain, and do it better than a standard non-collective approach. DIAS therefore demonstrates a role for Alife in building scalable, general, and adaptive problem-solving systems.", "paper_url": "http://arxiv.org/abs/2203.06855v1", "pdf_url": "http://arxiv.org/pdf/2203.06855v1", "repo_url": null}, "2203.06852": {"publish_time": "2022-03-14", "title": "Continual Learning for Multivariate Time Series Tasks with Variable Input Dimensions", "author": "Vibhor Gupta et.al.", "abstract": "We consider a sequence of related multivariate time series learning tasks, such as predicting failures for different instances of a machine from time series of multi-sensor data, or activity recognition tasks over different individuals from multiple wearable sensors. We focus on two under-explored practical challenges arising in such settings: (i) Each task may have a different subset of sensors, i.e., providing different partial observations of the underlying 'system'. This restriction can be due to different manufacturers in the former case, and people wearing more or less measurement devices in the latter (ii) We are not allowed to store or re-access data from a task once it has been observed at the task level. This may be due to privacy considerations in the case of people, or legal restrictions placed by machine owners. Nevertheless, we would like to (a) improve performance on subsequent tasks using experience from completed tasks as well as (b) continue to perform better on past tasks, e.g., update the model and improve predictions on even the first machine after learning from subsequently observed ones. We note that existing continual learning methods do not take into account variability in input dimensions arising due to different subsets of sensors being available across tasks, and struggle to adapt to such variable input dimensions (VID) tasks. In this work, we address this shortcoming of existing methods. To this end, we learn task-specific generative models and classifiers, and use these to augment data for target tasks. Since the input dimensions across tasks vary, we propose a novel conditioning module based on graph neural networks to aid a standard recurrent neural network. We evaluate the efficacy of the proposed approach on three publicly available datasets corresponding to two activity recognition tasks (classification) and one prognostics task (regression).", "paper_url": "http://arxiv.org/abs/2203.06852v1", "pdf_url": "http://arxiv.org/pdf/2203.06852v1", "repo_url": null}, "2203.06654": {"publish_time": "2022-03-13", "title": "Continual Prompt Tuning for Dialog State Tracking", "author": "Qi Zhu et.al.", "abstract": "A desirable dialog system should be able to continually learn new skills without forgetting old ones, and thereby adapt to new domains or tasks in its life cycle. However, continually training a model often leads to a well-known catastrophic forgetting issue. In this paper, we present Continual Prompt Tuning, a parameter-efficient framework that not only avoids forgetting but also enables knowledge transfer between tasks. To avoid forgetting, we only learn and store a few prompt tokens' embeddings for each task while freezing the backbone pre-trained model. To achieve bi-directional knowledge transfer among tasks, we propose several techniques (continual prompt initialization, query fusion, and memory replay) to transfer knowledge from preceding tasks and a memory-guided technique to transfer knowledge from subsequent tasks. Extensive experiments demonstrate the effectiveness and efficiency of our proposed method on continual learning for dialog state tracking, compared with state-of-the-art baselines.", "paper_url": "http://arxiv.org/abs/2203.06654v1", "pdf_url": "http://arxiv.org/pdf/2203.06654v1", "repo_url": "https://github.com/thu-coai/cpt4dst"}, "2203.06514": {"publish_time": "2022-03-12", "title": "Sparsity and Heterogeneous Dropout for Continual Learning in the Null Space of Neural Activations", "author": "Ali Abbasi et.al.", "abstract": "Continual/lifelong learning from a non-stationary input data stream is a cornerstone of intelligence. Despite their phenomenal performance in a wide variety of applications, deep neural networks are prone to forgetting their previously learned information upon learning new ones. This phenomenon is called \"catastrophic forgetting\" and is deeply rooted in the stability-plasticity dilemma. Overcoming catastrophic forgetting in deep neural networks has become an active field of research in recent years. In particular, gradient projection-based methods have recently shown exceptional performance at overcoming catastrophic forgetting. This paper proposes two biologically-inspired mechanisms based on sparsity and heterogeneous dropout that significantly increase a continual learner's performance over a long sequence of tasks. Our proposed approach builds on the Gradient Projection Memory (GPM) framework. We leverage K-winner activations in each layer of a neural network to enforce layer-wise sparse activations for each task, together with a between-task heterogeneous dropout that encourages the network to use non-overlapping activation patterns between different tasks. In addition, we introduce Continual Swiss Roll as a lightweight and interpretable -- yet challenging -- synthetic benchmark for continual learning. Lastly, we provide an in-depth analysis of our proposed method and demonstrate a significant performance boost on various benchmark continual learning problems.", "paper_url": "http://arxiv.org/abs/2203.06514v1", "pdf_url": "http://arxiv.org/pdf/2203.06514v1", "repo_url": null}, "2203.06311": {"publish_time": "2022-03-12", "title": "ELLE: Efficient Lifelong Pre-training for Emerging Data", "author": "Yujia Qin et.al.", "abstract": "Current pre-trained language models (PLM) are typically trained with static data, ignoring that in real-world scenarios, streaming data of various sources may continuously grow. This requires PLMs to integrate the information from all the sources in a lifelong manner. Although this goal could be achieved by exhaustive pre-training on all the existing data, such a process is known to be computationally expensive. To this end, we propose ELLE, aiming at efficient lifelong pre-training for emerging data. Specifically, ELLE consists of (1) function preserved model expansion, which flexibly expands an existing PLM's width and depth to improve the efficiency of knowledge acquisition; and (2) pre-trained domain prompts, which disentangle the versatile knowledge learned during pre-training and stimulate the proper knowledge for downstream tasks. We experiment ELLE with streaming data from 5 domains on BERT and GPT. The results show the superiority of ELLE over various lifelong learning baselines in both pre-training efficiency and downstream performances. The codes are publicly available at https://github.com/thunlp/ELLE.", "paper_url": "http://arxiv.org/abs/2203.06311v1", "pdf_url": "http://arxiv.org/pdf/2203.06311v1", "repo_url": "https://github.com/thunlp/elle"}, "2203.07667": {"publish_time": "2022-03-15", "title": "SATS: Self-Attention Transfer for Continual Semantic Segmentation", "author": "Yiqiao Qiu et.al.", "abstract": "Continually learning to segment more and more types of image regions is a desired capability for many intelligent systems. However, such continual semantic segmentation suffers from the same catastrophic forgetting issue as in continual classification learning. While multiple knowledge distillation strategies originally for continual classification have been well adapted to continual semantic segmentation, they only consider transferring old knowledge based on the outputs from one or more layers of deep fully convolutional networks. Different from existing solutions, this study proposes to transfer a new type of information relevant to knowledge, i.e. the relationships between elements (Eg. pixels or small local regions) within each image which can capture both within-class and between-class knowledge. The relationship information can be effectively obtained from the self-attention maps in a Transformer-style segmentation model. Considering that pixels belonging to the same class in each image often share similar visual properties, a class-specific region pooling is applied to provide more efficient relationship information for knowledge transfer. Extensive evaluations on multiple public benchmarks support that the proposed self-attention transfer method can further effectively alleviate the catastrophic forgetting issue, and its flexible combination with one or more widely adopted strategies significantly outperforms state-of-the-art solu", "paper_url": "http://arxiv.org/abs/2203.07667v1", "pdf_url": "http://arxiv.org/pdf/2203.07667v1", "repo_url": null}, "2203.07454": {"publish_time": "2022-03-14", "title": "L2Explorer: A Lifelong Reinforcement Learning Assessment Environment", "author": "Erik C. Johnson et.al.", "abstract": "Despite groundbreaking progress in reinforcement learning for robotics, gameplay, and other complex domains, major challenges remain in applying reinforcement learning to the evolving, open-world problems often found in critical application spaces. Reinforcement learning solutions tend to generalize poorly when exposed to new tasks outside of the data distribution they are trained on, prompting an interest in continual learning algorithms. In tandem with research on continual learning algorithms, there is a need for challenge environments, carefully designed experiments, and metrics to assess research progress. We address the latter need by introducing a framework for continual reinforcement-learning development and assessment using Lifelong Learning Explorer (L2Explorer), a new, Unity-based, first-person 3D exploration environment that can be continuously reconfigured to generate a range of tasks and task variants structured into complex and evolving evaluation curricula. In contrast to procedurally generated worlds with randomized components, we have developed a systematic approach to defining curricula in response to controlled changes with accompanying metrics to assess transfer, performance recovery, and data efficiency. Taken together, the L2Explorer environment and evaluation approach provides a framework for developing future evaluation methodologies in open-world settings and rigorously evaluating approaches to lifelong learning.", "paper_url": "http://arxiv.org/abs/2203.07454v1", "pdf_url": "http://arxiv.org/pdf/2203.07454v1", "repo_url": null}, "2203.08796": {"publish_time": "2022-03-16", "title": "A Continual Learning Framework for Adaptive Defect Classification and Inspection", "author": "Wenbo Sun et.al.", "abstract": "Machine-vision-based defect classification techniques have been widely adopted for automatic quality inspection in manufacturing processes. This article describes a general framework for classifying defects from high volume data batches with efficient inspection of unlabelled samples. The concept is to construct a detector to identify new defect types, send them to the inspection station for labelling, and dynamically update the classifier in an efficient manner that reduces both storage and computational needs imposed by data samples of previously observed batches. Both a simulation study on image classification and a case study on surface defect detection via 3D point clouds are performed to demonstrate the effectiveness of the proposed method.", "paper_url": "http://arxiv.org/abs/2203.08796v1", "pdf_url": "http://arxiv.org/pdf/2203.08796v1", "repo_url": null}, "2203.08512": {"publish_time": "2022-03-16", "title": "ConTinTin: Continual Learning from Task Instructions", "author": "Wenpeng Yin et.al.", "abstract": "The mainstream machine learning paradigms for NLP often work with two underlying presumptions. First, the target task is predefined and static, a system just needs to learn to solve it exclusively. Second, the supervision of a task mainly comes from a set of labeled examples. A question arises: how to build a system that can keep learning new tasks from their instructions? This work defines a new learning paradigm ConTinTin (Continual Learning from Task Instructions), in which a system should learn a sequence of new tasks one by one, each task is explained by a piece of textual instruction. The system is required to (i) generate the expected outputs of a new task by learning from its instruction, (ii) transfer the knowledge acquired from upstream tasks to help solve downstream tasks (i.e, forward-transfer), and (iii) retain or even improve the performance on earlier tasks after learning new tasks (i.e., backward-transfer). This new problem is studied on a stream of more than 60 tasks, each equipped with an instruction. Technically, our method InstructionSpeak contains two strategies that make full use of task instructions to improve forward-transfer and backward-transfer: one is to learn from the negative output, the other is to re-visit instructions of prior tasks. To our knowledge, this is the first time to study ConTinTin in NLP. In addition to the problem formulation and our promising approach, this work also contributes to providing rich analyses for the community to better understand this novel learning problem.", "paper_url": "http://arxiv.org/abs/2203.08512v1", "pdf_url": "http://arxiv.org/pdf/2203.08512v1", "repo_url": null}, "2203.08399": {"publish_time": "2022-03-16", "title": "Privacy-preserving Online AutoML for Domain-Specific Face Detection", "author": "Chenqian Yan et.al.", "abstract": "Despite the impressive progress of general face detection, the tuning of hyper-parameters and architectures is still critical for the performance of a domain-specific face detector. Though existing AutoML works can speedup such process, they either require tuning from scratch for a new scenario or do not consider data privacy. To scale up, we derive a new AutoML setting from a platform perspective. In such setting, new datasets sequentially arrive at the platform, where an architecture and hyper-parameter configuration is recommended to train the optimal face detector for each dataset. This, however, brings two major challenges: (1) how to predict the best configuration for any given dataset without touching their raw images due to the privacy concern? and (2) how to continuously improve the AutoML algorithm from previous tasks and offer a better warm-up for future ones? We introduce \"HyperFD\", a new privacy-preserving online AutoML framework for face detection. At its core part, a novel meta-feature representation of a dataset as well as its learning paradigm is proposed. Thanks to HyperFD, each local task (client) is able to effectively leverage the learning \"experience\" of previous tasks without uploading raw images to the platform; meanwhile, the meta-feature extractor is continuously learned to better trade off the bias and variance. Extensive experiments demonstrate the effectiveness and efficiency of our design.", "paper_url": "http://arxiv.org/abs/2203.08399v1", "pdf_url": "http://arxiv.org/pdf/2203.08399v1", "repo_url": null}, "2203.09450": {"publish_time": "2022-03-17", "title": "Continual Learning Based on OOD Detection and Task Masking", "author": "Gyuhak Kim et.al.", "abstract": "Existing continual learning techniques focus on either task incremental learning (TIL) or class incremental learning (CIL) problem, but not both. CIL and TIL differ mainly in that the task-id is provided for each test sample during testing for TIL, but not provided for CIL. Continual learning methods intended for one problem have limitations on the other problem. This paper proposes a novel unified approach based on out-of-distribution (OOD) detection and task masking, called CLOM, to solve both problems. The key novelty is that each task is trained as an OOD detection model rather than a traditional supervised learning model, and a task mask is trained to protect each task to prevent forgetting. Our evaluation shows that CLOM outperforms existing state-of-the-art baselines by large margins. The average TIL/CIL accuracy of CLOM over six experiments is 87.6/67.9% while that of the best baselines is only 82.4/55.0%.", "paper_url": "http://arxiv.org/abs/2203.09450v1", "pdf_url": "http://arxiv.org/pdf/2203.09450v1", "repo_url": "https://github.com/k-gyuhak/clom"}, "2203.08994": {"publish_time": "2022-03-17", "title": "AI Autonomy: Self-Initiation, Adaptation and Continual Learning", "author": "Bing Liu et.al.", "abstract": "As more and more AI agents are used in practice, it is time to think about how to make these agents fully autonomous so that they can (1) learn by themselves continually in a self-motivated and self-initiated manner rather than being retrained offline periodically on the initiation of human engineers and (2) accommodate or adapt to unexpected or novel circumstances. As the real-world is an open environment that is full of unknowns or novelties, detecting novelties, characterizing them, accommodating or adapting to them, and gathering ground-truth training data and incrementally learning the unknowns/novelties are critical to making the AI agent more and more knowledgeable and powerful over time. The key challenge is how to automate the process so that it is carried out continually on the agent's own initiative and through its own interactions with humans, other agents and the environment just like human on-the-job learning. This paper proposes a framework (called SOLA) for this learning paradigm to promote the research of building autonomous and continual learning enabled AI agents. To show feasibility, an implemented agent is also described.", "paper_url": "http://arxiv.org/abs/2203.08994v1", "pdf_url": "http://arxiv.org/pdf/2203.08994v1", "repo_url": null}, "2203.09879": {"publish_time": "2022-03-18", "title": "Class-wise Classifier Design Capable of Continual Learning using Adaptive Resonance Theory-based Topological Clustering", "author": "Naoki Masuyama et.al.", "abstract": "This paper proposes a supervised classification algorithm capable of continual learning by utilizing an Adaptive Resonance Theory (ART)-based growing self-organizing clustering algorithm. The ART-based clustering algorithm is theoretically capable of continual learning, and the proposed algorithm independently applies it to each class of training data for generating classifiers. Whenever an additional training data set from a new class is given, a new ART-based clustering will be defined in a different learning space. Thanks to the above-mentioned features, the proposed algorithm realizes continual learning capability. Simulation experiments showed that the proposed algorithm has superior classification performance compared with state-of-the-art clustering-based classification algorithms capable of continual learning.", "paper_url": "http://arxiv.org/abs/2203.09879v1", "pdf_url": "http://arxiv.org/pdf/2203.09879v1", "repo_url": null}, "2203.10681": {"publish_time": "2022-03-21", "title": "Online Continual Learning for Embedded Devices", "author": "Tyler L. Hayes et.al.", "abstract": "Real-time on-device continual learning is needed for new applications such as home robots, user personalization on smartphones, and augmented/virtual reality headsets. However, this setting poses unique challenges: embedded devices have limited memory and compute capacity and conventional machine learning models suffer from catastrophic forgetting when updated on non-stationary data streams. While several online continual learning models have been developed, their effectiveness for embedded applications has not been rigorously studied. In this paper, we first identify criteria that online continual learners must meet to effectively perform real-time, on-device learning. We then study the efficacy of several online continual learning methods when used with mobile neural networks. We measure their performance, memory usage, compute requirements, and ability to generalize to out-of-domain inputs.", "paper_url": "http://arxiv.org/abs/2203.10681v1", "pdf_url": "http://arxiv.org/pdf/2203.10681v1", "repo_url": null}, "2203.10652": {"publish_time": "2022-03-20", "title": "Continual Sequence Generation with Adaptive Compositional Modules", "author": "Yanzhe Zhang et.al.", "abstract": "Continual learning is essential for real-world deployment when there is a need to quickly adapt the model to new tasks without forgetting knowledge of old tasks. Existing work on continual sequence generation either always reuses existing parameters to learn new tasks, which is vulnerable to catastrophic forgetting on dissimilar tasks, or blindly adds new parameters for every new task, which could prevent knowledge sharing between similar tasks. To get the best of both worlds, in this work, we propose continual sequence generation with adaptive compositional modules to adaptively add modules in transformer architectures and compose both old and new modules for new tasks. We also incorporate pseudo experience replay to facilitate knowledge transfer in those shared modules. Experiment results on various sequences of generation tasks show that our framework can adaptively add modules or reuse modules based on task similarity, outperforming state-of-the-art baselines in terms of both performance and parameter efficiency. We make our code public at https://github.com/GT-SALT/Adaptive-Compositional-Modules.", "paper_url": "http://arxiv.org/abs/2203.10652v1", "pdf_url": "http://arxiv.org/pdf/2203.10652v1", "repo_url": null}, "2203.10317": {"publish_time": "2022-03-19", "title": "Practical Recommendations for Replay-based Continual Learning Methods", "author": "Gabriele Merlin et.al.", "abstract": "Continual Learning requires the model to learn from a stream of dynamic, non-stationary data without forgetting previous knowledge. Several approaches have been developed in the literature to tackle the Continual Learning challenge. Among them, Replay approaches have empirically proved to be the most effective ones. Replay operates by saving some samples in memory which are then used to rehearse knowledge during training in subsequent tasks. However, an extensive comparison and deeper understanding of different replay implementation subtleties is still missing in the literature. The aim of this work is to compare and analyze existing replay-based strategies and provide practical recommendations on developing efficient, effective and generally applicable replay-based strategies. In particular, we investigate the role of the memory size value, different weighting policies and discuss about the impact of data augmentation, which allows reaching better performance with lower memory sizes.", "paper_url": "http://arxiv.org/abs/2203.10317v1", "pdf_url": "http://arxiv.org/pdf/2203.10317v1", "repo_url": null}, "2203.10297": {"publish_time": "2022-03-19", "title": "Incremental Few-Shot Learning via Implanting and Compressing", "author": "Yiting Li et.al.", "abstract": "This work focuses on tackling the challenging but realistic visual task of Incremental Few-Shot Learning (IFSL), which requires a model to continually learn novel classes from only a few examples while not forgetting the base classes on which it was pre-trained. Our study reveals that the challenges of IFSL lie in both inter-class separation and novel-class representation. Dur to intra-class variation, a novel class may implicitly leverage the knowledge from multiple base classes to construct its feature representation. Hence, simply reusing the pre-trained embedding space could lead to a scattered feature distribution and result in category confusion. To address such issues, we propose a two-step learning strategy referred to as \\textbf{Im}planting and \\textbf{Co}mpressing (\\textbf{IMCO}), which optimizes both feature space partition and novel class reconstruction in a systematic manner. Specifically, in the \\textbf{Implanting} step, we propose to mimic the data distribution of novel classes with the assistance of data-abundant base set, so that a model could learn semantically-rich features that are beneficial for discriminating between the base and other unseen classes. In the \\textbf{Compressing} step, we adapt the feature extractor to precisely represent each novel class for enhancing intra-class compactness, together with a regularized parameter updating rule for preventing aggressive model updating. Finally, we demonstrate that IMCO outperforms competing baselines with a significant margin, both in image classification task and more challenging object detection task.", "paper_url": "http://arxiv.org/abs/2203.10297v1", "pdf_url": "http://arxiv.org/pdf/2203.10297v1", "repo_url": null}, "2203.11684": {"publish_time": "2022-03-22", "title": "Meta-attention for ViT-backed Continual Learning", "author": "Mengqi Xue et.al.", "abstract": "Continual learning is a longstanding research topic due to its crucial role in tackling continually arriving tasks. Up to now, the study of continual learning in computer vision is mainly restricted to convolutional neural networks (CNNs). However, recently there is a tendency that the newly emerging vision transformers (ViTs) are gradually dominating the field of computer vision, which leaves CNN-based continual learning lagging behind as they can suffer from severe performance degradation if straightforwardly applied to ViTs. In this paper, we study ViT-backed continual learning to strive for higher performance riding on recent advances of ViTs. Inspired by mask-based continual learning methods in CNNs, where a mask is learned per task to adapt the pre-trained ViT to the new task, we propose MEta-ATtention (MEAT), i.e., attention to self-attention, to adapt a pre-trained ViT to new tasks without sacrificing performance on already learned tasks. Unlike prior mask-based methods like Piggyback, where all parameters are associated with corresponding masks, MEAT leverages the characteristics of ViTs and only masks a portion of its parameters. It renders MEAT more efficient and effective with less overhead and higher accuracy. Extensive experiments demonstrate that MEAT exhibits significant superiority to its state-of-the-art CNN counterparts, with 4.0~6.0% absolute boosts in accuracy. Our code has been released at https://github.com/zju-vipa/MEAT-TIL.", "paper_url": "http://arxiv.org/abs/2203.11684v1", "pdf_url": "http://arxiv.org/pdf/2203.11684v1", "repo_url": "https://github.com/zju-vipa/meat-til"}, "2203.11560": {"publish_time": "2022-03-22", "title": "Modelling continual learning in humans with Hebbian context gating and exponentially decaying task signals", "author": "Timo Flesch et.al.", "abstract": "Humans can learn several tasks in succession with minimal mutual interference but perform more poorly when trained on multiple tasks at once. The opposite is true for standard deep neural networks. Here, we propose novel computational constraints for artificial neural networks, inspired by earlier work on gating in the primate prefrontal cortex, that capture the cost of interleaved training and allow the network to learn two tasks in sequence without forgetting. We augment standard stochastic gradient descent with two algorithmic motifs, so-called \"sluggish\" task units and a Hebbian training step that strengthens connections between task units and hidden units that encode task-relevant information. We found that the \"sluggish\" units introduce a switch-cost during training, which biases representations under interleaved training towards a joint representation that ignores the contextual cue, while the Hebbian step promotes the formation of a gating scheme from task units to the hidden layer that produces orthogonal representations which are perfectly guarded against interference. Validating the model on previously published human behavioural data revealed that it matches performance of participants who had been trained on blocked or interleaved curricula, and that these performance differences were driven by misestimation of the true category boundary.", "paper_url": "http://arxiv.org/abs/2203.11560v1", "pdf_url": "http://arxiv.org/pdf/2203.11560v1", "repo_url": "https://github.com/summerfieldlab/flesch_nagy_etal_hebbcl"}, "2203.11997": {"publish_time": "2022-03-22", "title": "Federated Self-Supervised Learning for Acoustic Event Classification", "author": "Meng Feng et.al.", "abstract": "Standard acoustic event classification (AEC) solutions require large-scale collection of data from client devices for model optimization. Federated learning (FL) is a compelling framework that decouples data collection and model training to enhance customer privacy. In this work, we investigate the feasibility of applying FL to improve AEC performance while no customer data can be directly uploaded to the server. We assume no pseudo labels can be inferred from on-device user inputs, aligning with the typical use cases of AEC. We adapt self-supervised learning to the FL framework for on-device continual learning of representations, and it results in improved performance of the downstream AEC classifiers without labeled/pseudo-labeled data available. Compared to the baseline w/o FL, the proposed method improves precision up to 20.3\\% relatively while maintaining the recall. Our work differs from prior work in FL in that our approach does not require user-generated learning targets, and the data we use is collected from our Beta program and is de-identified, to maximally simulate the production settings.", "paper_url": "http://arxiv.org/abs/2203.11997v1", "pdf_url": "http://arxiv.org/pdf/2203.11997v1", "repo_url": null}, "2203.11992": {"publish_time": "2022-03-22", "title": "Resonance in Weight Space: Covariate Shift Can Drive Divergence of SGD with Momentum", "author": "Kirby Banman et.al.", "abstract": "Most convergence guarantees for stochastic gradient descent with momentum (SGDm) rely on iid sampling. Yet, SGDm is often used outside this regime, in settings with temporally correlated input samples such as continual learning and reinforcement learning. Existing work has shown that SGDm with a decaying step-size can converge under Markovian temporal correlation. In this work, we show that SGDm under covariate shift with a fixed step-size can be unstable and diverge. In particular, we show SGDm under covariate shift is a parametric oscillator, and so can suffer from a phenomenon known as resonance. We approximate the learning system as a time varying system of ordinary differential equations, and leverage existing theory to characterize the system's divergence/convergence as resonant/nonresonant modes. The theoretical result is limited to the linear setting with periodic covariate shift, so we empirically supplement this result to show that resonance phenomena persist even under non-periodic covariate shift, nonlinear dynamics with neural networks, and optimizers other than SGDm.", "paper_url": "http://arxiv.org/abs/2203.11992v1", "pdf_url": "http://arxiv.org/pdf/2203.11992v1", "repo_url": null}, "2203.13167": {"publish_time": "2022-03-24", "title": "Towards Exemplar-Free Continual Learning in Vision Transformers: an Account of Attention, Functional and Weight Regularization", "author": "Francesco Pelosin et.al.", "abstract": "In this paper, we investigate the continual learning of Vision Transformers (ViT) for the challenging exemplar-free scenario, with special focus on how to efficiently distill the knowledge of its crucial self-attention mechanism (SAM). Our work takes an initial step towards a surgical investigation of SAM for designing coherent continual learning methods in ViTs. We first carry out an evaluation of established continual learning regularization techniques. We then examine the effect of regularization when applied to two key enablers of SAM: (a) the contextualized embedding layers, for their ability to capture well-scaled representations with respect to the values, and (b) the prescaled attention maps, for carrying value-independent global contextual information. We depict the perks of each distilling strategy on two image recognition benchmarks (CIFAR100 and ImageNet-32) -- while (a) leads to a better overall accuracy, (b) helps enhance the rigidity by maintaining competitive performances. Furthermore, we identify the limitation imposed by the symmetric nature of regularization losses. To alleviate this, we propose an asymmetric variant and apply it to the pooled output distillation (POD) loss adapted for ViTs. Our experiments confirm that introducing asymmetry to POD boosts its plasticity while retaining stability across (a) and (b). Moreover, we acknowledge low forgetting measures for all the compared methods, indicating that ViTs might be naturally inclined continual learner", "paper_url": "http://arxiv.org/abs/2203.13167v1", "pdf_url": "http://arxiv.org/pdf/2203.13167v1", "repo_url": null}, "2203.12817": {"publish_time": "2022-03-24", "title": "Continual Learning and Private Unlearning", "author": "Bo Liu et.al.", "abstract": "As intelligent agents become autonomous over longer periods of time, they may eventually become lifelong counterparts to specific people. If so, it may be common for a user to want the agent to master a task temporarily but later on to forget the task due to privacy concerns. However enabling an agent to \\emph{forget privately} what the user specified without degrading the rest of the learned knowledge is a challenging problem. With the aim of addressing this challenge, this paper formalizes this continual learning and private unlearning (CLPU) problem. The paper further introduces a straightforward but exactly private solution, CLPU-DER++, as the first step towards solving the CLPU problem, along with a set of carefully designed benchmark problems to evaluate the effectiveness of the proposed solution.", "paper_url": "http://arxiv.org/abs/2203.12817v1", "pdf_url": "http://arxiv.org/pdf/2203.12817v1", "repo_url": null}, "2203.13611": {"publish_time": "2022-03-25", "title": "Class-Incremental Learning for Action Recognition in Videos", "author": "Jaeyoo Park et.al.", "abstract": "We tackle catastrophic forgetting problem in the context of class-incremental learning for video recognition, which has not been explored actively despite the popularity of continual learning. Our framework addresses this challenging task by introducing time-channel importance maps and exploiting the importance maps for learning the representations of incoming examples via knowledge distillation. We also incorporate a regularization scheme in our objective function, which encourages individual features obtained from different time steps in a video to be uncorrelated and eventually improves accuracy by alleviating catastrophic forgetting. We evaluate the proposed approach on brand-new splits of class-incremental action recognition benchmarks constructed upon the UCF101, HMDB51, and Something-Something V2 datasets, and demonstrate the effectiveness of our algorithm in comparison to the existing continual learning methods that are originally designed for image data.", "paper_url": "http://arxiv.org/abs/2203.13611v1", "pdf_url": "http://arxiv.org/pdf/2203.13611v1", "repo_url": null}, "2203.13381": {"publish_time": "2022-03-24", "title": "Probing Representation Forgetting in Supervised and Unsupervised Continual Learning", "author": "MohammadReza Davari et.al.", "abstract": "Continual Learning research typically focuses on tackling the phenomenon of catastrophic forgetting in neural networks. Catastrophic forgetting is associated with an abrupt loss of knowledge previously learned by a model when the task, or more broadly the data distribution, being trained on changes. In supervised learning problems this forgetting, resulting from a change in the model's representation, is typically measured or observed by evaluating the decrease in old task performance. However, a model's representation can change without losing knowledge about prior tasks. In this work we consider the concept of representation forgetting, observed by using the difference in performance of an optimal linear classifier before and after a new task is introduced. Using this tool we revisit a number of standard continual learning benchmarks and observe that, through this lens, model representations trained without any explicit control for forgetting often experience small representation forgetting and can sometimes be comparable to methods which explicitly control for forgetting, especially in longer task sequences. We also show that representation forgetting can lead to new insights on the effect of model capacity and loss function used in continual learning. Based on our results, we show that a simple yet competitive approach is to learn representations continually with standard supervised contrastive learning while constructing prototypes of class samples when queried on old samples.", "paper_url": "http://arxiv.org/abs/2203.13381v1", "pdf_url": "http://arxiv.org/pdf/2203.13381v1", "repo_url": null}, "2203.13321": {"publish_time": "2022-03-24", "title": "Addressing Client Drift in Federated Continual Learning with Adaptive Optimization", "author": "Yeshwanth Venkatesha et.al.", "abstract": "Federated learning has been extensively studied and is the prevalent method for privacy-preserving distributed learning in edge devices. Correspondingly, continual learning is an emerging field targeted towards learning multiple tasks sequentially. However, there is little attention towards additional challenges emerging when federated aggregation is performed in a continual learning system. We identify \\textit{client drift} as one of the key weaknesses that arise when vanilla federated averaging is applied in such a system, especially since each client can independently have different order of tasks. We outline a framework for performing Federated Continual Learning (FCL) by using NetTailor as a candidate continual learning approach and show the extent of the problem of client drift. We show that adaptive federated optimization can reduce the adverse impact of client drift and showcase its effectiveness on CIFAR100, MiniImagenet, and Decathlon benchmarks. Further, we provide an empirical analysis highlighting the interplay between different hyperparameters such as client and server learning rates, the number of local training iterations, and communication rounds. Finally, we evaluate our framework on useful characteristics of federated learning systems such as scalability, robustness to the skewness in clients' data distribution, and stragglers.", "paper_url": "http://arxiv.org/abs/2203.13321v1", "pdf_url": "http://arxiv.org/pdf/2203.13321v1", "repo_url": null}, "2203.13307": {"publish_time": "2022-03-24", "title": "Tackling Online One-Class Incremental Learning by Removing Negative Contrasts", "author": "Nader Asadi et.al.", "abstract": "Recent work studies the supervised online continual learning setting where a learner receives a stream of data whose class distribution changes over time. Distinct from other continual learning settings the learner is presented new samples only once and must distinguish between all seen classes. A number of successful methods in this setting focus on storing and replaying a subset of samples alongside incoming data in a computationally efficient manner. One recent proposal ER-AML achieved strong performance in this setting by applying an asymmetric loss based on contrastive learning to the incoming data and replayed data. However, a key ingredient of the proposed method is avoiding contrasts between incoming data and stored data, which makes it impractical for the setting where only one new class is introduced in each phase of the stream. In this work we adapt a recently proposed approach (\\textit{BYOL}) from self-supervised learning to the supervised learning setting, unlocking the constraint on contrasts. We then show that supplementing this with additional regularization on class prototypes yields a new method that achieves strong performance in the one-class incremental learning setting and is competitive with the top performing methods in the multi-class incremental setting.", "paper_url": "http://arxiv.org/abs/2203.13307v1", "pdf_url": "http://arxiv.org/pdf/2203.13307v1", "repo_url": null}, "2203.14544": {"publish_time": "2022-03-28", "title": "Gradient-Matching Coresets for Rehearsal-Based Continual Learning", "author": "Lukas Balles et.al.", "abstract": "The goal of continual learning (CL) is to efficiently update a machine learning model with new data without forgetting previously-learned knowledge. Most widely-used CL methods rely on a rehearsal memory of data points to be reused while training on new data. Curating such a rehearsal memory to maintain a small, informative subset of all the data seen so far is crucial to the success of these methods. We devise a coreset selection method for rehearsal-based continual learning. Our method is based on the idea of gradient matching: The gradients induced by the coreset should match, as closely as possible, those induced by the original training dataset. Inspired by the neural tangent kernel theory, we perform this gradient matching across the model's initialization distribution, allowing us to extract a coreset without having to train the model first. We evaluate the method on a wide range of continual learning scenarios and demonstrate that it improves the performance of rehearsal-based CL methods compared to competing memory management strategies such as reservoir sampling.", "paper_url": "http://arxiv.org/abs/2203.14544v1", "pdf_url": "http://arxiv.org/pdf/2203.14544v1", "repo_url": null}, "2203.14383": {"publish_time": "2022-03-27", "title": "Continual learning: a feature extraction formalization, an efficient algorithm, and fundamental obstructions", "author": "Binghui Peng et.al.", "abstract": "Continual learning is an emerging paradigm in machine learning, wherein a model is exposed in an online fashion to data from multiple different distributions (i.e. environments), and is expected to adapt to the distribution change. Precisely, the goal is to perform well in the new environment, while simultaneously retaining the performance on the previous environments (i.e. avoid \"catastrophic forgetting\") -- without increasing the size of the model.   While this setup has enjoyed a lot of attention in the applied community, there hasn't be theoretical work that even formalizes the desired guarantees. In this paper, we propose a framework for continual learning through the framework of feature extraction -- namely, one in which features, as well as a classifier, are being trained with each environment. When the features are linear, we design an efficient gradient-based algorithm $\\mathsf{DPGD}$, that is guaranteed to perform well on the current environment, as well as avoid catastrophic forgetting. In the general case, when the features are non-linear, we show such an algorithm cannot exist, whether efficient or not.", "paper_url": "http://arxiv.org/abs/2203.14383v1", "pdf_url": "http://arxiv.org/pdf/2203.14383v1", "repo_url": null}, "2203.14327": {"publish_time": "2022-03-27", "title": "Local-Adaptive Face Recognition via Graph-based Meta-Clustering and Regularized Adaptation", "author": "Wenbin Zhu et.al.", "abstract": "Due to the rising concern of data privacy, it's reasonable to assume the local client data can't be transferred to a centralized server, nor their associated identity label is provided. To support continuous learning and fill the last-mile quality gap, we introduce a new problem setup called Local-Adaptive Face Recognition (LaFR). Leveraging the environment-specific local data after the deployment of the initial global model, LaFR aims at getting optimal performance by training local-adapted models automatically and un-supervisely, as opposed to fixing their initial global model. We achieve this by a newly proposed embedding cluster model based on Graph Convolution Network (GCN), which is trained via meta-optimization procedure. Compared with previous works, our meta-clustering model can generalize well in unseen local environments. With the pseudo identity labels from the clustering results, we further introduce novel regularization techniques to improve the model adaptation performance. Extensive experiments on racial and internal sensor adaptation demonstrate that our proposed solution is more effective for adapting face recognition models in each specific environment. Meanwhile, we show that LaFR can further improve the global model by a simple federated aggregation over the updated local models.", "paper_url": "http://arxiv.org/abs/2203.14327v1", "pdf_url": "http://arxiv.org/pdf/2203.14327v1", "repo_url": null}, "2203.14032": {"publish_time": "2022-03-26", "title": "Continual learning of quantum state classification with gradient episodic memory", "author": "Haozhen Situ et.al.", "abstract": "Continual learning is one of the many areas of machine learning research. For the goal of strong artificial intelligence that can mimic human-level intelligence, AI systems would have the ability to adapt to ever-changing scenarios and learn new knowledge continuously without forgetting previously acquired knowledge. A phenomenon called catastrophic forgetting emerges when a machine learning model is trained across multiple tasks. The model's performance on previously learned tasks may drop dramatically during the learning process of the newly seen task. Some continual learning strategies have been proposed to address the catastrophic forgetting problem. Recently, continual learning has also been studied in the context of quantum machine learning. By leveraging the elastic weight consolidation method, a single quantum classifier can perform multiple tasks after being trained consecutively on those tasks. In this work, we incorporate the gradient episodic memory method to train a variational quantum classifier. The gradient of the current task is projected to the closest gradient, avoiding the increase of the loss at previous tasks, but allowing the decrease. We use six quantum state classification tasks to benchmark this method. Numerical simulation results show that better performance is obtained compared to the elastic weight consolidation method. Furthermore, positive transfer of knowledge to previous tasks is observed, which means the classifier's performance on previous tasks is enhanced rather than compromised while learning a new task.", "paper_url": "http://arxiv.org/abs/2203.14032v1", "pdf_url": "http://arxiv.org/pdf/2203.14032v1", "repo_url": null}, "2203.15355": {"publish_time": "2022-03-29", "title": "Online Continual Learning on a Contaminated Data Stream with Blurry Task Boundaries", "author": "Jihwan Bang et.al.", "abstract": "Learning under a continuously changing data distribution with incorrect labels is a desirable real-world problem yet challenging. A large body of continual learning (CL) methods, however, assumes data streams with clean labels, and online learning scenarios under noisy data streams are yet underexplored. We consider a more practical CL task setup of an online learning from blurry data stream with corrupted labels, where existing CL methods struggle. To address the task, we first argue the importance of both diversity and purity of examples in the episodic memory of continual learning models. To balance diversity and purity in the episodic memory, we propose a novel strategy to manage and use the memory by a unified approach of label noise aware diverse sampling and robust learning with semi-supervised learning. Our empirical validations on four real-world or synthetic noise datasets (CIFAR10 and 100, mini-WebVision, and Food-101N) exhibit that our method significantly outperforms prior arts in this realistic and challenging continual learning scenario. Code and data splits are available in https://github.com/clovaai/puridiver.", "paper_url": "http://arxiv.org/abs/2203.15355v2", "pdf_url": "http://arxiv.org/pdf/2203.15355v2", "repo_url": "https://github.com/clovaai/puridiver"}, "2203.16195": {"publish_time": "2022-03-30", "title": "On the Road to Online Adaptation for Semantic Image Segmentation", "author": "Riccardo Volpi et.al.", "abstract": "We propose a new problem formulation and a corresponding evaluation framework to advance research on unsupervised domain adaptation for semantic image segmentation. The overall goal is fostering the development of adaptive learning systems that will continuously learn, without supervision, in ever-changing environments. Typical protocols that study adaptation algorithms for segmentation models are limited to few domains, adaptation happens offline, and human intervention is generally required, at least to annotate data for hyper-parameter tuning. We argue that such constraints are incompatible with algorithms that can continuously adapt to different real-world situations. To address this, we propose a protocol where models need to learn online, from sequences of temporally correlated images, requiring continuous, frame-by-frame adaptation. We accompany this new protocol with a variety of baselines to tackle the proposed formulation, as well as an extensive analysis of their behaviors, which can serve as a starting point for future research.", "paper_url": "http://arxiv.org/abs/2203.16195v1", "pdf_url": "http://arxiv.org/pdf/2203.16195v1", "repo_url": "https://github.com/naver/oasis"}, "2203.16102": {"publish_time": "2022-03-30", "title": "Continual Normalization: Rethinking Batch Normalization for Online Continual Learning", "author": "Quang Pham et.al.", "abstract": "Existing continual learning methods use Batch Normalization (BN) to facilitate training and improve generalization across tasks. However, the non-i.i.d and non-stationary nature of continual learning data, especially in the online setting, amplify the discrepancy between training and testing in BN and hinder the performance of older tasks. In this work, we study the cross-task normalization effect of BN in online continual learning where BN normalizes the testing data using moments biased towards the current task, resulting in higher catastrophic forgetting. This limitation motivates us to propose a simple yet effective method that we call Continual Normalization (CN) to facilitate training similar to BN while mitigating its negative effect. Extensive experiments on different continual learning algorithms and online scenarios show that CN is a direct replacement for BN and can provide substantial performance improvements. Our implementation is available at \\url{https://github.com/phquang/Continual-Normalization}.", "paper_url": "http://arxiv.org/abs/2203.16102v1", "pdf_url": "http://arxiv.org/pdf/2203.16102v1", "repo_url": null}, "2203.17269": {"publish_time": "2022-03-31", "title": "A Closer Look at Rehearsal-Free Continual Learning", "author": "James Seale Smith et.al.", "abstract": "Continual learning describes a setting where machine learning models learn novel concepts from continuously shifting training data, while simultaneously avoiding degradation of knowledge on previously seen classes (a phenomenon known as the catastrophic forgetting problem) which may disappear from the training data for extended periods of time. Current approaches for continual learning of a single expanding task (aka class-incremental continual learning) require extensive rehearsal of previously seen data to avoid this degradation of knowledge. Unfortunately, rehearsal comes at a sharp cost to memory and computation, and it may also violate data-privacy. Instead, we explore combining knowledge distillation and parameter regularization in new ways to achieve strong continual learning performance without rehearsal. Specifically, we take a deep dive into common continual learning techniques: prediction distillation, feature distillation, L2 parameter regularization, and EWC parameter regularization. We first disprove the common assumption that parameter regularization techniques fail for rehearsal-free continual learning of a single, expanding task. Next, we explore how to leverage knowledge from a pre-trained model in rehearsal-free continual learning and find that vanilla L2 parameter regularization outperforms EWC parameter regularization and feature distillation. We then highlight the impact of the rehearsal-free continual learning settings with a classifier expansion benchmark, showing that a strategy based on our findings combined with a positive/negative label balancing heuristic can close the performance gap between the upper bound and the existing strategies by up to roughly 50%. Finally, we show that a simple method consisting of pre-training, L2 regularization, and prediction distillation can even outperform rehearsal-based methods on the common CIFAR-100 benchmark.", "paper_url": "http://arxiv.org/abs/2203.17269v1", "pdf_url": "http://arxiv.org/pdf/2203.17269v1", "repo_url": null}, "2203.16588": {"publish_time": "2022-03-30", "title": "Constrained Few-shot Class-incremental Learning", "author": "Michael Hersche et.al.", "abstract": "Continually learning new classes from fresh data without forgetting previous knowledge of old classes is a very challenging research problem. Moreover, it is imperative that such learning must respect certain memory and computational constraints such as (i) training samples are limited to only a few per class, (ii) the computational cost of learning a novel class remains constant, and (iii) the memory footprint of the model grows at most linearly with the number of classes observed. To meet the above constraints, we propose C-FSCIL, which is architecturally composed of a frozen meta-learned feature extractor, a trainable fixed-size fully connected layer, and a rewritable dynamically growing memory that stores as many vectors as the number of encountered classes. C-FSCIL provides three update modes that offer a trade-off between accuracy and compute-memory cost of learning novel classes. C-FSCIL exploits hyperdimensional embedding that allows to continually express many more classes than the fixed dimensions in the vector space, with minimal interference. The quality of class vector representations is further improved by aligning them quasi-orthogonally to each other by means of novel loss functions. Experiments on the CIFAR100, miniImageNet, and Omniglot datasets show that C-FSCIL outperforms the baselines with remarkable accuracy and compression. It also scales up to the largest problem size ever tried in this few-shot setting by learning 423 novel classes on top of 1200 base classes with less than 1.6% accuracy drop. Our code is available at https://github.com/IBM/constrained-FSCIL.", "paper_url": "http://arxiv.org/abs/2203.16588v1", "pdf_url": "http://arxiv.org/pdf/2203.16588v1", "repo_url": "https://github.com/ibm/constrained-fscil"}, "2204.00545": {"publish_time": "2022-04-01", "title": "A Novel Multimodal Approach for Studying the Dynamics of Curiosity in Small Group Learning", "author": "Tanmay Sinha et.al.", "abstract": "Curiosity is a vital metacognitive skill in educational contexts, leading to creativity, and a love of learning. And while many school systems increasingly undercut curiosity by teaching to the test, teachers are increasingly interested in how to evoke curiosity in their students to prepare them for a world in which lifelong learning and reskilling will be more and more important. One aspect of curiosity that has received little attention, however, is the role of peers in eliciting curiosity. We present what we believe to be the first theoretical framework that articulates an integrated socio-cognitive account of curiosity that ties observable behaviors in peers to underlying curiosity states. We make a bipartite distinction between individual and interpersonal functions that contribute to curiosity, and multimodal behaviors that fulfill these functions. We validate the proposed framework by leveraging a longitudinal latent variable modeling approach. Findings confirm a positive predictive relationship between the latent variables of individual and interpersonal functions and curiosity, with the interpersonal functions exercising a comparatively stronger influence. Prominent behavioral realizations of these functions are also discovered in a data-driven manner. We instantiate the proposed theoretical framework in a set of strategies and tactics that can be incorporated into learning technologies to indicate, evoke, and scaffold curiosity. This work is a step towards designing learning technologies that can recognize and evoke moment-by-moment curiosity during learning in social contexts and towards a more complete multimodal learning analytics. The underlying rationale is applicable more generally for developing computer support for other metacognitive and socio-emotional skills.", "paper_url": "http://arxiv.org/abs/2204.00545v1", "pdf_url": "http://arxiv.org/pdf/2204.00545v1", "repo_url": null}, "2204.00121": {"publish_time": "2022-03-31", "title": "An MPSoC-based on-line Edge Infrastructure for Embedded Neuromorphic Robotic Controllers", "author": "Enrique Pinero-Fuentes et.al.", "abstract": "In this work, an all-in-one neuromorphic controller system with reduced latency and power consumption for a robotic arm is presented. Biological muscle movement consists of stretching and shrinking fibres via spike-commanded signals that come from motor neurons, which in turn are connected to a central pattern generator neural structure. In addition, biological systems are able to respond to diverse stimuli rather fast and efficiently, and this is based on the way information is coded within neural processes. As opposed to human-created encoding systems, neural ones use neurons and spikes to process the information and make weighted decisions based on a continuous learning process. The Event-Driven Scorbot platform (ED-Scorbot) consists of a 6 Degrees of Freedom (DoF) robotic arm whose controller implements a Spiking Proportional-Integrative- Derivative algorithm, mimicking in this way the previously commented biological systems. In this paper, we present an infrastructure upgrade to the ED-Scorbot platform, replacing the controller hardware, which was comprised of two Spartan Field Programmable Gate Arrays (FPGAs) and a barebone computer, with an edge device, the Xilinx Zynq-7000 SoC (System on Chip) which reduces the response time, power consumption and overall complexity.", "paper_url": "http://arxiv.org/abs/2204.00121v1", "pdf_url": "http://arxiv.org/pdf/2204.00121v1", "repo_url": null}, "2204.00895": {"publish_time": "2022-04-02", "title": "Class-Incremental Learning by Knowledge Distillation with Adaptive Feature Consolidation", "author": "Minsoo Kang et.al.", "abstract": "We present a novel class incremental learning approach based on deep neural networks, which continually learns new tasks with limited memory for storing examples in the previous tasks. Our algorithm is based on knowledge distillation and provides a principled way to maintain the representations of old models while adjusting to new tasks effectively. The proposed method estimates the relationship between the representation changes and the resulting loss increases incurred by model updates. It minimizes the upper bound of the loss increases using the representations, which exploits the estimated importance of each feature map within a backbone model. Based on the importance, the model restricts updates of important features for robustness while allowing changes in less critical features for flexibility. This optimization strategy effectively alleviates the notorious catastrophic forgetting problem despite the limited accessibility of data in the previous tasks. The experimental results show significant accuracy improvement of the proposed algorithm over the existing methods on the standard datasets. Code is available.", "paper_url": "http://arxiv.org/abs/2204.00895v1", "pdf_url": "http://arxiv.org/pdf/2204.00895v1", "repo_url": "https://github.com/kminsoo/afc"}, "2204.02296": {"publish_time": "2022-04-05", "title": "iSDF: Real-Time Neural Signed Distance Fields for Robot Perception", "author": "Joseph Ortiz et.al.", "abstract": "We present iSDF, a continual learning system for real-time signed distance field (SDF) reconstruction. Given a stream of posed depth images from a moving camera, it trains a randomly initialised neural network to map input 3D coordinate to approximate signed distance. The model is self-supervised by minimising a loss that bounds the predicted signed distance using the distance to the closest sampled point in a batch of query points that are actively sampled. In contrast to prior work based on voxel grids, our neural method is able to provide adaptive levels of detail with plausible filling in of partially observed regions and denoising of observations, all while having a more compact representation. In evaluations against alternative methods on real and synthetic datasets of indoor environments, we find that iSDF produces more accurate reconstructions, and better approximations of collision costs and gradients useful for downstream planners in domains from navigation to manipulation. Code and video results can be found at our project page: https://joeaortiz.github.io/iSDF/ .", "paper_url": "http://arxiv.org/abs/2204.02296v1", "pdf_url": "http://arxiv.org/pdf/2204.02296v1", "repo_url": null}, "2204.01934": {"publish_time": "2022-04-05", "title": "Attention Distraction: Watermark Removal Through Continual Learning with Selective Forgetting", "author": "Qi Zhong et.al.", "abstract": "Fine-tuning attacks are effective in removing the embedded watermarks in deep learning models. However, when the source data is unavailable, it is challenging to just erase the watermark without jeopardizing the model performance. In this context, we introduce Attention Distraction (AD), a novel source data-free watermark removal attack, to make the model selectively forget the embedded watermarks by customizing continual learning. In particular, AD first anchors the model's attention on the main task using some unlabeled data. Then, through continual learning, a small number of \\textit{lures} (randomly selected natural images) that are assigned a new label distract the model's attention away from the watermarks. Experimental results from different datasets and networks corroborate that AD can thoroughly remove the watermark with a small resource budget without compromising the model's performance on the main task, which outperforms the state-of-the-art works.", "paper_url": "http://arxiv.org/abs/2204.01934v1", "pdf_url": "http://arxiv.org/pdf/2204.01934v1", "repo_url": null}, "2204.04078": {"publish_time": "2022-04-08", "title": "General Incremental Learning with Domain-aware Categorical Representations", "author": "Jiangwei Xie et.al.", "abstract": "Continual learning is an important problem for achieving human-level intelligence in real-world applications as an agent must continuously accumulate knowledge in response to streaming data/tasks. In this work, we consider a general and yet under-explored incremental learning problem in which both the class distribution and class-specific domain distribution change over time. In addition to the typical challenges in class incremental learning, this setting also faces the intra-class stability-plasticity dilemma and intra-class domain imbalance problems. To address above issues, we develop a novel domain-aware continual learning method based on the EM framework. Specifically, we introduce a flexible class representation based on the von Mises-Fisher mixture model to capture the intra-class structure, using an expansion-and-reduction strategy to dynamically increase the number of components according to the class complexity. Moreover, we design a bi-level balanced memory to cope with data imbalances within and across classes, which combines with a distillation loss to achieve better inter- and intra-class stability-plasticity trade-off. We conduct exhaustive experiments on three benchmarks: iDigits, iDomainNet and iCIFAR-20. The results show that our approach consistently outperforms previous methods by a significant margin, demonstrating its superiority.", "paper_url": "http://arxiv.org/abs/2204.04078v1", "pdf_url": "http://arxiv.org/pdf/2204.04078v1", "repo_url": null}, "2204.03895": {"publish_time": "2022-04-08", "title": "SoundBeam: Target sound extraction conditioned on sound-class labels and enrollment clues for increased performance and continuous learning", "author": "Marc Delcroix et.al.", "abstract": "In many situations, we would like to hear desired sound events (SEs) while being able to ignore interference. Target sound extraction (TSE) aims at tackling this problem by estimating the sound of target SE classes in a mixture while suppressing all other sounds. We can achieve this with a neural network that extracts the target SEs by conditioning it on clues representing the target SE classes. Two types of clues have been proposed, i.e., target SE class labels and enrollment sound samples similar to the target sound. Systems based on SE class labels can directly optimize embedding vectors representing the SE classes, resulting in high extraction performance. However, extending these systems to the extraction of new SE classes not encountered during training is not easy. Enrollment-based approaches extract SEs by finding sounds in the mixtures that share similar characteristics to the enrollment. These approaches do not explicitly rely on SE class definitions and can thus handle new SE classes. In this paper, we introduce a TSE framework, SoundBeam, that combines the advantages of both approaches. We also perform an extensive evaluation of the different TSE schemes using synthesized and real mixtures, which shows the potential of SoundBeam.", "paper_url": "http://arxiv.org/abs/2204.03895v1", "pdf_url": "http://arxiv.org/pdf/2204.03895v1", "repo_url": null}, "2204.05220": {"publish_time": "2022-04-11", "title": "CFA: Constraint-based Finetuning Approach for Generalized Few-Shot Object Detection", "author": "Karim Guirguis et.al.", "abstract": "Few-shot object detection (FSOD) seeks to detect novel categories with limited data by leveraging prior knowledge from abundant base data. Generalized few-shot object detection (G-FSOD) aims to tackle FSOD without forgetting previously seen base classes and, thus, accounts for a more realistic scenario, where both classes are encountered during test time. While current FSOD methods suffer from catastrophic forgetting, G-FSOD addresses this limitation yet exhibits a performance drop on novel tasks compared to the state-of-the-art FSOD. In this work, we propose a constraint-based finetuning approach (CFA) to alleviate catastrophic forgetting, while achieving competitive results on the novel task without increasing the model capacity. CFA adapts a continual learning method, namely Average Gradient Episodic Memory (A-GEM) to G-FSOD. Specifically, more constraints on the gradient search strategy are imposed from which a new gradient update rule is derived, allowing for better knowledge exchange between base and novel classes. To evaluate our method, we conduct extensive experiments on MS-COCO and PASCAL-VOC datasets. Our method outperforms current FSOD and G-FSOD approaches on the novel task with minor degeneration on the base task. Moreover, CFA is orthogonal to FSOD approaches and operates as a plug-and-play module without increasing the model capacity or inference time.", "paper_url": "http://arxiv.org/abs/2204.05220v1", "pdf_url": "http://arxiv.org/pdf/2204.05220v1", "repo_url": null}, "2204.04799": {"publish_time": "2022-04-10", "title": "DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning", "author": "Zifeng Wang et.al.", "abstract": "Continual learning aims to enable a single model to learn a sequence of tasks without catastrophic forgetting. Top-performing methods usually require a rehearsal buffer to store past pristine examples for experience replay, which, however, limits their practical value due to privacy and memory constraints. In this work, we present a simple yet effective framework, DualPrompt, which learns a tiny set of parameters, called prompts, to properly instruct a pre-trained model to learn tasks arriving sequentially without buffering past examples. DualPrompt presents a novel approach to attach complementary prompts to the pre-trained backbone, and then formulates the objective as learning task-invariant and task-specific \"instructions\". With extensive experimental validation, DualPrompt consistently sets state-of-the-art performance under the challenging class-incremental setting. In particular, DualPrompt outperforms recent advanced continual learning methods with relatively large buffer sizes. We also introduce a more challenging benchmark, Split ImageNet-R, to help generalize rehearsal-free continual learning research. Source code is available at https://github.com/google-research/l2p.", "paper_url": "http://arxiv.org/abs/2204.04799v1", "pdf_url": "http://arxiv.org/pdf/2204.04799v1", "repo_url": "https://github.com/google-research/l2p"}, "2204.04795": {"publish_time": "2022-04-10", "title": "Edge Continual Learning for Dynamic Digital Twins over Wireless Networks", "author": "Omar Hashash et.al.", "abstract": "Digital twins (DTs) constitute a critical link between the real-world and the metaverse. To guarantee a robust connection between these two worlds, DTs should maintain accurate representations of the physical applications, while preserving synchronization between real and digital entities. In this paper, a novel edge continual learning framework is proposed to accurately model the evolving affinity between a physical twin (PT) and its corresponding cyber twin (CT) while maintaining their utmost synchronization. In particular, a CT is simulated as a deep neural network (DNN) at the wireless network edge to model an autonomous vehicle traversing an episodically dynamic environment. As the vehicular PT updates its driving policy in each episode, the CT is required to concurrently adapt its DNN model to the PT, which gives rise to a de-synchronization gap. Considering the history-aware nature of DTs, the model update process is posed a dual objective optimization problem whose goal is to jointly minimize the loss function over all encountered episodes and the corresponding de-synchronization time. As the de-synchronization time continues to increase over sequential episodes, an elastic weight consolidation (EWC) technique that regularizes the DT history is proposed to limit de-synchronization time. Furthermore, to address the plasticity-stability tradeoff accompanying the progressive growth of the EWC regularization terms, a modified EWC method that considers fair execution between the historical episodes of the DTs is adopted. Ultimately, the proposed framework achieves a simultaneously accurate and synchronous CT model that is robust to catastrophic forgetting. Simulation results show that the proposed solution can achieve an accuracy of 90 % while guaranteeing a minimal desynchronization time.", "paper_url": "http://arxiv.org/abs/2204.04795v1", "pdf_url": "http://arxiv.org/pdf/2204.04795v1", "repo_url": null}, "2204.04763": {"publish_time": "2022-04-10", "title": "Information-theoretic Online Memory Selection for Continual Learning", "author": "Shengyang Sun et.al.", "abstract": "A challenging problem in task-free continual learning is the online selection of a representative replay memory from data streams. In this work, we investigate the online memory selection problem from an information-theoretic perspective. To gather the most information, we propose the \\textit{surprise} and the \\textit{learnability} criteria to pick informative points and to avoid outliers. We present a Bayesian model to compute the criteria efficiently by exploiting rank-one matrix structures. We demonstrate that these criteria encourage selecting informative points in a greedy algorithm for online memory selection. Furthermore, by identifying the importance of \\textit{the timing to update the memory}, we introduce a stochastic information-theoretic reservoir sampler (InfoRS), which conducts sampling among selective points with high information. Compared to reservoir sampling, InfoRS demonstrates improved robustness against data imbalance. Finally, empirical performances over continual learning benchmarks manifest its efficiency and efficacy.", "paper_url": "http://arxiv.org/abs/2204.04763v1", "pdf_url": "http://arxiv.org/pdf/2204.04763v1", "repo_url": null}, "2204.04297": {"publish_time": "2022-04-08", "title": "Learning to modulate random weights can induce task-specific contexts for economical meta and continual learning", "author": "Jinyung Hong et.al.", "abstract": "Neural networks are vulnerable to catastrophic forgetting when data distributions are non-stationary during continual online learning; learning of a later task often leads to forgetting of an earlier task. One solution approach is model-agnostic continual meta-learning, whereby both task-specific and meta parameters are trained. Here, we depart from this view and introduce a novel neural-network architecture inspired by neuromodulation in biological nervous systems. Neuromodulation is the biological mechanism that dynamically controls and fine-tunes synaptic dynamics to complement the behavioral context in real-time, which has received limited attention in machine learning. We introduce a single-hidden-layer network that learns only a relatively small context vector per task (task-specific parameters) that neuromodulates unchanging, randomized weights (meta parameters) that transform the input. We show that when task boundaries are available, this approach can eliminate catastrophic forgetting entirely while also drastically reducing the number of learnable parameters relative to other context-vector-based approaches. Furthermore, by combining this model with a simple meta-learning approach for inferring task identity, we demonstrate that the model can be generalized into a framework to perform continual learning without knowledge of task boundaries. Finally, we showcase the framework in a supervised continual online learning scenario and discuss the implications of the proposed formalism.", "paper_url": "http://arxiv.org/abs/2204.04297v1", "pdf_url": "http://arxiv.org/pdf/2204.04297v1", "repo_url": null}, "2204.05928": {"publish_time": "2022-04-12", "title": "Dynamic Dialogue Policy Transformer for Continual Reinforcement Learning", "author": "Christian Geishauser et.al.", "abstract": "Continual learning is one of the key components of human learning and a necessary requirement of artificial intelligence. As dialogue can potentially span infinitely many topics and tasks, a task-oriented dialogue system must have the capability to continually learn, dynamically adapting to new challenges while preserving the knowledge it already acquired. Despite the importance, continual reinforcement learning of the dialogue policy has remained largely unaddressed. The lack of a framework with training protocols, baseline models and suitable metrics, has so far hindered research in this direction. In this work we fill precisely this gap, enabling research in dialogue policy optimisation to go from static to dynamic learning. We provide a continual learning algorithm, baseline architectures and metrics for assessing continual learning models. Moreover, we propose the dynamic dialogue policy transformer (DDPT), a novel dynamic architecture that can integrate new knowledge seamlessly, is capable of handling large state spaces and obtains significant zero-shot performance when being exposed to unseen domains, without any growth in network parameter size.", "paper_url": "http://arxiv.org/abs/2204.05928v1", "pdf_url": "http://arxiv.org/pdf/2204.05928v1", "repo_url": null}, "2204.05893": {"publish_time": "2022-04-12", "title": "Offline Distillation for Robot Lifelong Learning with Imbalanced Experience", "author": "Wenxuan Zhou et.al.", "abstract": "Robots will experience non-stationary environment dynamics throughout their lifetime: the robot dynamics can change due to wear and tear, or its surroundings may change over time. Eventually, the robots should perform well in all of the environment variations it has encountered. At the same time, it should still be able to learn fast in a new environment. We investigate two challenges in such a lifelong learning setting: first, existing off-policy algorithms struggle with the trade-off between being conservative to maintain good performance in the old environment and learning efficiently in the new environment. We propose the Offline Distillation Pipeline to break this trade-off by separating the training procedure into interleaved phases of online interaction and offline distillation. Second, training with the combined datasets from multiple environments across the lifetime might create a significant performance drop compared to training on the datasets individually. Our hypothesis is that both the imbalanced quality and size of the datasets exacerbate the extrapolation error of the Q-function during offline training over the \"weaker\" dataset. We propose a simple fix to the issue by keeping the policy closer to the dataset during the distillation phase. In the experiments, we demonstrate these challenges and the proposed solutions with a simulated bipedal robot walking task across various environment changes. We show that the Offline Distillation Pipeline achieves better performance across all the encountered environments without affecting data collection. We also provide a comprehensive empirical study to support our hypothesis on the data imbalance issue.", "paper_url": "http://arxiv.org/abs/2204.05893v1", "pdf_url": "http://arxiv.org/pdf/2204.05893v1", "repo_url": null}, "2204.05842": {"publish_time": "2022-04-12", "title": "Generative Negative Replay for Continual Learning", "author": "Gabriele Graffieti et.al.", "abstract": "Learning continually is a key aspect of intelligence and a necessary ability to solve many real-life problems. One of the most effective strategies to control catastrophic forgetting, the Achilles' heel of continual learning, is storing part of the old data and replaying them interleaved with new experiences (also known as the replay approach). Generative replay, which is using generative models to provide replay patterns on demand, is particularly intriguing, however, it was shown to be effective mainly under simplified assumptions, such as simple scenarios and low-dimensional data. In this paper, we show that, while the generated data are usually not able to improve the classification accuracy for the old classes, they can be effective as negative examples (or antagonists) to better learn the new classes, especially when the learning experiences are small and contain examples of just one or few classes. The proposed approach is validated on complex class-incremental and data-incremental continual learning scenarios (CORe50 and ImageNet-1000) composed of high-dimensional data and a large number of training experiences: a setup where existing generative replay approaches usually fail.", "paper_url": "http://arxiv.org/abs/2204.05842v1", "pdf_url": "http://arxiv.org/pdf/2204.05842v1", "repo_url": null}, "2204.05737": {"publish_time": "2022-04-12", "title": "LifeLonger: A Benchmark for Continual Disease Classification", "author": "Mohammad Mahdi Derakhshani et.al.", "abstract": "Deep learning models have shown a great effectiveness in recognition of findings in medical images. However, they cannot handle the ever-changing clinical environment, bringing newly annotated medical data from different sources. To exploit the incoming streams of data, these models would benefit largely from sequentially learning from new samples, without forgetting the previously obtained knowledge. In this paper we introduce LifeLonger, a benchmark for continual disease classification on the MedMNIST collection, by applying existing state-of-the-art continual learning methods. In particular, we consider three continual learning scenarios, namely, task and class incremental learning and the newly defined cross-domain incremental learning. Task and class incremental learning of diseases address the issue of classifying new samples without re-training the models from scratch, while cross-domain incremental learning addresses the issue of dealing with datasets originating from different institutions while retaining the previously obtained knowledge. We perform a thorough analysis of the performance and examine how the well-known challenges of continual learning, such as the catastrophic forgetting exhibit themselves in this setting. The encouraging results demonstrate that continual learning has a major potential to advance disease classification and to produce a more robust and efficient learning framework for clinical settings. The code repository, data partitions and baseline results for the complete benchmark will be made publicly available.", "paper_url": "http://arxiv.org/abs/2204.05737v1", "pdf_url": "http://arxiv.org/pdf/2204.05737v1", "repo_url": null}, "2204.05624": {"publish_time": "2022-04-12", "title": "Continual Predictive Learning from Videos", "author": "Geng Chen et.al.", "abstract": "Predictive learning ideally builds the world model of physical processes in one or more given environments. Typical setups assume that we can collect data from all environments at all times. In practice, however, different prediction tasks may arrive sequentially so that the environments may change persistently throughout the training procedure. Can we develop predictive learning algorithms that can deal with more realistic, non-stationary physical environments? In this paper, we study a new continual learning problem in the context of video prediction, and observe that most existing methods suffer from severe catastrophic forgetting in this setup. To tackle this problem, we propose the continual predictive learning (CPL) approach, which learns a mixture world model via predictive experience replay and performs test-time adaptation with non-parametric task inference. We construct two new benchmarks based on RoboNet and KTH, in which different tasks correspond to different physical robotic environments or human actions. Our approach is shown to effectively mitigate forgetting and remarkably outperform the na\\\"ive combinations of previous art in video prediction and continual learning.", "paper_url": "http://arxiv.org/abs/2204.05624v1", "pdf_url": "http://arxiv.org/pdf/2204.05624v1", "repo_url": "https://github.com/jc043/cpl"}}, "Meta Learning": {"2202.12888": {"publish_time": "2022-02-25", "title": "Meta-Learning for Simple Regret Minimization", "author": "Mohammadjavad Azizi et.al.", "abstract": "We develop a meta-learning framework for simple regret minimization in bandits. In this framework, a learning agent interacts with a sequence of bandit tasks, which are sampled i.i.d.\\ from an unknown prior distribution, and learns its meta-parameters to perform better on future tasks. We propose the first Bayesian and frequentist algorithms for this meta-learning problem. The Bayesian algorithm has access to a prior distribution over the meta-parameters and its meta simple regret over $m$ bandit tasks with horizon $n$ is mere $\\tilde{O}(m / \\sqrt{n})$. This is while we show that the meta simple regret of the frequentist algorithm is $\\tilde{O}(\\sqrt{m} n + m/ \\sqrt{n})$, and thus, worse. However, the algorithm is more general, because it does not need a prior distribution over the meta-parameters, and is easier to implement for various distributions. We instantiate our algorithms for several classes of bandit problems. Our algorithms are general and we complement our theory by evaluating them empirically in several environments.", "paper_url": "http://arxiv.org/abs/2202.12888v1", "pdf_url": "http://arxiv.org/pdf/2202.12888v1", "repo_url": "https://github.com/Azizimj/Meta-SRM"}, "2202.12450": {"publish_time": "2022-02-25", "title": "MetaVA: Curriculum Meta-learning and Pre-fine-tuning of Deep Neural Networks for Detecting Ventricular Arrhythmias based on ECGs", "author": "Wenrui Zhang et.al.", "abstract": "Ventricular arrhythmias (VA) are the main causes of sudden cardiac death. Developing machine learning methods for detecting VA based on electrocardiograms (ECGs) can help save people's lives. However, developing such machine learning models for ECGs is challenging because of the following: 1) group-level diversity from different subjects and 2) individual-level diversity from different moments of a single subject. In this study, we aim to solve these problems in the pre-training and fine-tuning stages. For the pre-training stage, we propose a novel model agnostic meta-learning (MAML) with curriculum learning (CL) method to solve group-level diversity. MAML is expected to better transfer the knowledge from a large dataset and use only a few recordings to quickly adapt the model to a new person. CL is supposed to further improve MAML by meta-learning from easy to difficult tasks. For the fine-tuning stage, we propose improved pre-fine-tuning to solve individual-level diversity. We conduct experiments using a combination of three publicly available ECG datasets. The results show that our method outperforms the compared methods in terms of all evaluation metrics. Ablation studies show that MAML and CL could help perform more evenly, and pre-fine-tuning could better fit the model to training data.", "paper_url": "http://arxiv.org/abs/2202.12450v1", "pdf_url": "http://arxiv.org/pdf/2202.12450v1", "repo_url": null}, "2202.12396": {"publish_time": "2022-02-24", "title": "Finite-Sum Compositional Stochastic Optimization: Theory and Applications", "author": "Bokun Wang et.al.", "abstract": "This paper studies stochastic optimization for a sum of compositional functions, where the inner-level function of each summand is coupled with the corresponding summation index. We refer to this family of problems as finite-sum coupled compositional optimization (FCCO). It has broad applications in machine learning for optimizing non-convex or convex compositional measures/objectives such as average precision (AP), $p$-norm push, listwise ranking losses, neighborhood component analysis (NCA), deep survival analysis, deep latent variable models, softmax functions, and model agnostic meta-learning, which deserves finer analysis. Yet, existing algorithms and analysis are restricted in one or other aspects. The contribution of this paper is to provide a comprehensive analysis of a simple stochastic algorithm for both non-convex and convex objectives. The key results are {\\bf improved oracle complexities with the parallel speed-up} by the moving-average based stochastic estimator with mini-batching. Our theoretical analysis also exhibits new insights for improving the practical implementation by sampling the batches of equal size for the outer and inner levels. Numerical experiments on AP maximization and $p$-norm push optimization corroborate some aspects of the theory.", "paper_url": "http://arxiv.org/abs/2202.12396v1", "pdf_url": "http://arxiv.org/pdf/2202.12396v1", "repo_url": null}, "2202.12326": {"publish_time": "2022-02-24", "title": "Towards Better Meta-Initialization with Task Augmentation for Kindergarten-aged Speech Recognition", "author": "Yunzheng Zhu et.al.", "abstract": "Children's automatic speech recognition (ASR) is always difficult due to, in part, the data scarcity problem, especially for kindergarten-aged kids. When data are scarce, the model might overfit to the training data, and hence good starting points for training are essential. Recently, meta-learning was proposed to learn model initialization (MI) for ASR tasks of different languages. This method leads to good performance when the model is adapted to an unseen language. However, MI is vulnerable to overfitting on training tasks (learner overfitting). It is also unknown whether MI generalizes to other low-resource tasks. In this paper, we validate the effectiveness of MI in children's ASR and attempt to alleviate the problem of learner overfitting. To achieve model-agnostic meta-learning (MAML), we regard children's speech at each age as a different task. In terms of learner overfitting, we propose a task-level augmentation method by simulating new ages using frequency warping techniques. Detailed experiments are conducted to show the impact of task augmentation on each age for kindergarten-aged speech. As a result, our approach achieves a relative word error rate (WER) improvement of 51% over the baseline system with no augmentation or initialization.", "paper_url": "http://arxiv.org/abs/2202.12326v1", "pdf_url": "http://arxiv.org/pdf/2202.12326v1", "repo_url": null}, "2202.11490": {"publish_time": "2022-02-23", "title": "Towards Tailored Models on Private AIoT Devices: Federated Direct Neural Architecture Search", "author": "Chunhui Zhang et.al.", "abstract": "Neural networks often encounter various stringent resource constraints while deploying on edge devices. To tackle these problems with less human efforts, automated machine learning becomes popular in finding various neural architectures that fit diverse Artificial Intelligence of Things (AIoT) scenarios. Recently, to prevent the leakage of private information while enable automated machine intelligence, there is an emerging trend to integrate federated learning and neural architecture search (NAS). Although promising as it may seem, the coupling of difficulties from both tenets makes the algorithm development quite challenging. In particular, how to efficiently search the optimal neural architecture directly from massive non-independent and identically distributed (non-IID) data among AIoT devices in a federated manner is a hard nut to crack. In this paper, to tackle this challenge, by leveraging the advances in ProxylessNAS, we propose a Federated Direct Neural Architecture Search (FDNAS) framework that allows for hardware-friendly NAS from non- IID data across devices. To further adapt to both various data distributions and different types of devices with heterogeneous embedded hardware platforms, inspired by meta-learning, a Cluster Federated Direct Neural Architecture Search (CFDNAS) framework is proposed to achieve device-aware NAS, in the sense that each device can learn a tailored deep learning model for its particular data distribution and hardware constraint. Extensive experiments on non-IID datasets have shown the state-of-the-art accuracy-efficiency trade-offs achieved by the proposed solution in the presence of both data and device heterogeneity.", "paper_url": "http://arxiv.org/abs/2202.11490v1", "pdf_url": "http://arxiv.org/pdf/2202.11490v1", "repo_url": null}, "2202.13611": {"publish_time": "2022-02-28", "title": "Prepare for Trouble and Make it Double. Supervised and Unsupervised Stacking for AnomalyBased Intrusion Detection", "author": "Tommaso Zoppi et.al.", "abstract": "In the last decades, researchers, practitioners and companies struggled in devising mechanisms to detect malicious activities originating security threats. Amongst the many solutions, network intrusion detection emerged as one of the most popular to analyze network traffic and detect ongoing intrusions based on rules or by means of Machine Learners (MLs), which process such traffic and learn a model to suspect intrusions. Supervised MLs are very effective in detecting known threats, but struggle in identifying zero-day attacks (unknown during learning phase), which instead can be detected through unsupervised MLs. Unfortunately, there are no definitive answers on the combined use of both approaches for network intrusion detection. In this paper we first expand the problem of zero-day attacks and motivate the need to combine supervised and unsupervised algorithms. We propose the adoption of meta-learning, in the form of a two-layer Stacker, to create a mixed approach that detects both known and unknown threats. Then we implement and empirically evaluate our Stacker through an experimental campaign that allows i) debating on meta-features crafted through unsupervised base-level learners, ii) electing the most promising supervised meta-level classifiers, and iii) benchmarking classification scores of the Stacker with respect to supervised and unsupervised classifiers. Last, we compare our solution with existing works from the recent literature. Overall, our Stacker reduces misclassifications with respect to (un)supervised ML algorithms in all the 7 public datasets we considered, and outperforms existing studies in 6 out of those 7 datasets. In particular, it turns out to be more effective in detecting zero-day attacks than supervised algorithms, limiting their main weakness but still maintaining adequate capabilities in detecting known attacks.", "paper_url": "http://arxiv.org/abs/2202.13611v1", "pdf_url": "http://arxiv.org/pdf/2202.13611v1", "repo_url": null}, "2202.13474": {"publish_time": "2022-02-27", "title": "Interpretable Concept-based Prototypical Networks for Few-Shot Learning", "author": "Mohammad Reza Zarei et.al.", "abstract": "Few-shot learning aims at recognizing new instances from classes with limited samples. This challenging task is usually alleviated by performing meta-learning on similar tasks. However, the resulting models are black-boxes. There has been growing concerns about deploying black-box machine learning models and FSL is not an exception in this regard. In this paper, we propose a method for FSL based on a set of human-interpretable concepts. It constructs a set of metric spaces associated with the concepts and classifies samples of novel classes by aggregating concept-specific decisions. The proposed method does not require concept annotations for query samples. This interpretable method achieved results on a par with six previously state-of-the-art black-box FSL methods on the CUB fine-grained bird classification dataset.", "paper_url": "http://arxiv.org/abs/2202.13474v1", "pdf_url": "http://arxiv.org/pdf/2202.13474v1", "repo_url": null}, "2202.13227": {"publish_time": "2022-02-26", "title": "Towards Scalable and Robust Structured Bandits: A Meta-Learning Framework", "author": "Runzhe Wan et.al.", "abstract": "Online learning in large-scale structured bandits is known to be challenging due to the curse of dimensionality. In this paper, we propose a unified meta-learning framework for a general class of structured bandit problems where the parameter space can be factorized to item-level. The novel bandit algorithm is general to be applied to many popular problems,scalable to the huge parameter and action spaces, and robust to the specification of the generalization model. At the core of this framework is a Bayesian hierarchical model that allows information sharing among items via their features, upon which we design a meta Thompson sampling algorithm. Three representative examples are discussed thoroughly. Both theoretical analysis and numerical results support the usefulness of the proposed method.", "paper_url": "http://arxiv.org/abs/2202.13227v1", "pdf_url": "http://arxiv.org/pdf/2202.13227v1", "repo_url": null}, "2202.13117": {"publish_time": "2022-02-26", "title": "An Unsupervised Cross-Modal Hashing Method Robust to Noisy Training Image-Text Correspondences in Remote Sensing", "author": "Georgii Mikriukov et.al.", "abstract": "The development of accurate and scalable cross-modal image-text retrieval methods, where queries from one modality (e.g., text) can be matched to archive entries from another (e.g., remote sensing image) has attracted great attention in remote sensing (RS). Most of the existing methods assume that a reliable multi-modal training set with accurately matched text-image pairs is existing. However, this assumption may not always hold since the multi-modal training sets may include noisy pairs (i.e., textual descriptions/captions associated to training images can be noisy), distorting the learning process of the retrieval methods. To address this problem, we propose a novel unsupervised cross-modal hashing method robust to the noisy image-text correspondences (CHNR). CHNR consists of three modules: 1) feature extraction module, which extracts feature representations of image-text pairs; 2) noise detection module, which detects potential noisy correspondences; and 3) hashing module that generates cross-modal binary hash codes. The proposed CHNR includes two training phases: i) meta-learning phase that uses a small portion of clean (i.e., reliable) data to train the noise detection module in an adversarial fashion; and ii) the main training phase for which the trained noise detection module is used to identify noisy correspondences while the hashing module is trained on the noisy multi-modal training set. Experimental results show that the proposed CHNR outperforms state-of-the-art methods. Our code is publicly available at https://git.tu-berlin.de/rsim/chnr", "paper_url": "http://arxiv.org/abs/2202.13117v1", "pdf_url": "http://arxiv.org/pdf/2202.13117v1", "repo_url": "https://git.tu-berlin.de/rsim/chnr"}, "2202.13001": {"publish_time": "2022-02-25", "title": "Non-stationary Bandits and Meta-Learning with a Small Set of Optimal Arms", "author": "MohammadJavad Azizi et.al.", "abstract": "We study a sequential decision problem where the learner faces a sequence of $K$-armed stochastic bandit tasks. The tasks may be designed by an adversary, but the adversary is constrained to choose the optimal arm of each task in a smaller (but unknown) subset of $M$ arms. The task boundaries might be known (the bandit meta-learning setting), or unknown (the non-stationary bandit setting), and the number of tasks $N$ as well as the total number of rounds $T$ are known ($N$ could be unknown in the meta-learning setting). We design an algorithm based on a reduction to bandit submodular maximization, and show that its regret in both settings is smaller than the simple baseline of $\\tilde{O}(\\sqrt{KNT})$ that can be obtained by using standard algorithms designed for non-stationary bandit problems. For the bandit meta-learning problem with fixed task length $\\tau$, we show that the regret of the algorithm is bounded as $\\tilde{O}(N\\sqrt{M \\tau}+N^{2/3})$. Under additional assumptions on the identifiability of the optimal arms in each task, we show a bandit meta-learning algorithm with an improved $\\tilde{O}(N\\sqrt{M \\tau}+N^{1/2})$ regret.", "paper_url": "http://arxiv.org/abs/2202.13001v1", "pdf_url": "http://arxiv.org/pdf/2202.13001v1", "repo_url": "https://github.com/duongnhatthang/meta-bandit"}, "2203.00089": {"publish_time": "2022-02-28", "title": "Amortized Proximal Optimization", "author": "Juhan Bae et.al.", "abstract": "We propose a framework for online meta-optimization of parameters that govern optimization, called Amortized Proximal Optimization (APO). We first interpret various existing neural network optimizers as approximate stochastic proximal point methods which trade off the current-batch loss with proximity terms in both function space and weight space. The idea behind APO is to amortize the minimization of the proximal point objective by meta-learning the parameters of an update rule. We show how APO can be used to adapt a learning rate or a structured preconditioning matrix. Under appropriate assumptions, APO can recover existing optimizers such as natural gradient descent and KFAC. It enjoys low computational overhead and avoids expensive and numerically sensitive operations required by some second-order optimizers, such as matrix inverses. We empirically test APO for online adaptation of learning rates and structured preconditioning matrices for regression, image reconstruction, image classification, and natural language translation tasks. Empirically, the learning rate schedules found by APO generally outperform optimal fixed learning rates and are competitive with manually tuned decay schedules. Using APO to adapt a structured preconditioning matrix generally results in optimization performance competitive with second-order methods. Moreover, the absence of matrix inversion provides numerical stability, making it effective for low precision training.", "paper_url": "http://arxiv.org/abs/2203.00089v1", "pdf_url": "http://arxiv.org/pdf/2203.00089v1", "repo_url": null}, "2203.01123": {"publish_time": "2022-03-01", "title": "A Constrained Optimization Approach to Bilevel Optimization with Multiple Inner Minima", "author": "Daouda Sow et.al.", "abstract": "Bilevel optimization has found extensive applications in modern machine learning problems such as hyperparameter optimization, neural architecture search, meta-learning, etc. While bilevel problems with a unique inner minimal point (e.g., where the inner function is strongly convex) are well understood, bilevel problems with multiple inner minimal points remains to be a challenging and open problem. Existing algorithms designed for such a problem were applicable to restricted situations and do not come with the full guarantee of convergence. In this paper, we propose a new approach, which convert the bilevel problem to an equivalent constrained optimization, and then the primal-dual algorithm can be used to solve the problem. Such an approach enjoys a few advantages including (a) addresses the multiple inner minima challenge; (b) features fully first-order efficiency without involving second-order Hessian and Jacobian computations, as opposed to most existing gradient-based bilevel algorithms; (c) admits the convergence guarantee via constrained nonconvex optimization. Our experiments further demonstrate the desired performance of the proposed approach.", "paper_url": "http://arxiv.org/abs/2203.01123v1", "pdf_url": "http://arxiv.org/pdf/2203.01123v1", "repo_url": null}, "2203.01924": {"publish_time": "2022-03-03", "title": "Min-Max Bilevel Multi-objective Optimization with Applications in Machine Learning", "author": "Alex Gu et.al.", "abstract": "This paper is the first to propose a generic min-max bilevel multi-objective optimization framework, highlighting applications in representation learning and hyperparameter optimization. In many machine learning applications such as meta-learning, multi-task learning, and representation learning, a subset of the parameters are shared by all the tasks, while each specific task has its own set of additional parameters. By leveraging the recent advances of nonconvex min-max optimization, we propose a gradient descent-ascent bilevel optimization (MORBiT) algorithm which is able to extract a set of shared parameters that is robust over all tasks and further overcomes the distributional shift between training and testing tasks. Theoretical analyses show that MORBiT converges to the first-order stationary point at a rate of $\\mathcal{O}(\\sqrt{n}K^{-2/5})$ for a class of nonconvex problems, where $K$ denotes the total number of iterations and $n$ denotes the number of tasks. Overall, we formulate a min-max bilevel multi-objective optimization problem, provide a single loop two-timescale algorithm with convergence rate guarantees, and show theoretical bounds on the generalization abilities of the optimizer. Experimental results on sinusoid regression and representation learning showcase the superiority of MORBiT over state-of-the-art methods, validating our convergence and generalization results.", "paper_url": "http://arxiv.org/abs/2203.01924v1", "pdf_url": "http://arxiv.org/pdf/2203.01924v1", "repo_url": null}, "2203.01482": {"publish_time": "2022-03-03", "title": "MetaDT: Meta Decision Tree for Interpretable Few-Shot Learning", "author": "Baoquan Zhang et.al.", "abstract": "Few-Shot Learning (FSL) is a challenging task, which aims to recognize novel classes with few examples. Recently, lots of methods have been proposed from the perspective of meta-learning and representation learning for improving FSL performance. However, few works focus on the interpretability of FSL decision process. In this paper, we take a step towards the interpretable FSL by proposing a novel decision tree-based meta-learning framework, namely, MetaDT. Our insight is replacing the last black-box FSL classifier of the existing representation learning methods by an interpretable decision tree with meta-learning. The key challenge is how to effectively learn the decision tree (i.e., the tree structure and the parameters of each node) in the FSL setting. To address the challenge, we introduce a tree-like class hierarchy as our prior: 1) the hierarchy is directly employed as the tree structure; 2) by regarding the class hierarchy as an undirected graph, a graph convolution-based decision tree inference network is designed as our meta-learner to learn to infer the parameters of each node. At last, a two-loop optimization mechanism is incorporated into our framework for a fast adaptation of the decision tree with few examples. Extensive experiments on performance comparison and interpretability analysis show the effectiveness and superiority of our MetaDT. Our code will be publicly available upon acceptance.", "paper_url": "http://arxiv.org/abs/2203.01482v1", "pdf_url": "http://arxiv.org/pdf/2203.01482v1", "repo_url": null}, "2203.01443": {"publish_time": "2022-03-02", "title": "Continuous-Time Meta-Learning with Forward Mode Differentiation", "author": "Tristan Deleu et.al.", "abstract": "Drawing inspiration from gradient-based meta-learning methods with infinitely small gradient steps, we introduce Continuous-Time Meta-Learning (COMLN), a meta-learning algorithm where adaptation follows the dynamics of a gradient vector field. Specifically, representations of the inputs are meta-learned such that a task-specific linear classifier is obtained as a solution of an ordinary differential equation (ODE). Treating the learning process as an ODE offers the notable advantage that the length of the trajectory is now continuous, as opposed to a fixed and discrete number of gradient steps. As a consequence, we can optimize the amount of adaptation necessary to solve a new task using stochastic gradient descent, in addition to learning the initial conditions as is standard practice in gradient-based meta-learning. Importantly, in order to compute the exact meta-gradients required for the outer-loop updates, we devise an efficient algorithm based on forward mode differentiation, whose memory requirements do not scale with the length of the learning trajectory, thus allowing longer adaptation in constant memory. We provide analytical guarantees for the stability of COMLN, we show empirically its efficiency in terms of runtime and memory usage, and we illustrate its effectiveness on a range of few-shot image classification problems.", "paper_url": "http://arxiv.org/abs/2203.01443v1", "pdf_url": "http://arxiv.org/pdf/2203.01443v1", "repo_url": null}, "2203.02113": {"publish_time": "2022-03-04", "title": "FS-COCO: Towards Understanding of Freehand Sketches of Common Objects in Context", "author": "Pinaki Nath Chowdhury et.al.", "abstract": "We advance sketch research to scenes with the first dataset of freehand scene sketches, FS-COCO. With practical applications in mind, we collect sketches that convey well scene content but can be sketched within a few minutes by a person with any sketching skills. Our dataset comprises 10,000 freehand scene vector sketches with per point space-time information by 100 non-expert individuals, offering both object- and scene-level abstraction. Each sketch is augmented with its text description. Using our dataset, we study for the first time the problem of the fine-grained image retrieval from freehand scene sketches and sketch captions. We draw insights on (i) Scene salience encoded in sketches with strokes temporal order; (ii) The retrieval performance accuracy from scene sketches against image captions; (iii) Complementarity of information in sketches and image captions, as well as the potential benefit of combining the two modalities. In addition, we propose new solutions enabled by our dataset (i) We adopt meta-learning to show how the retrieval model can be fine-tuned to a new user style given just a small set of sketches, (ii) We extend a popular vector sketch LSTM-based encoder to handle sketches with larger complexity than was supported by previous work. Namely, we propose a hierarchical sketch decoder, which we leverage at a sketch-specific \"pretext\" task. Our dataset enables for the first time research on freehand scene sketch understanding and its practical applications.", "paper_url": "http://arxiv.org/abs/2203.02113v1", "pdf_url": "http://arxiv.org/pdf/2203.02113v1", "repo_url": null}, "2203.03328": {"publish_time": "2022-03-07", "title": "Automated Few-Shot Time Series Forecasting based on Bi-level Programming", "author": "Jiangjiao Xu et.al.", "abstract": "New micro-grid design with renewable energy sources and battery storage systems can help improve greenhouse gas emissions and reduce the operational cost. To provide an effective short-/long-term forecasting of both energy generation and load demand, time series predictive modeling has been one of the key tools to guide the optimal decision-making for planning and operation. One of the critical challenges of time series renewable energy forecasting is the lack of historical data to train an adequate predictive model. Moreover, the performance of a machine learning model is sensitive to the choice of its corresponding hyperparameters. Bearing these considerations in mind, this paper develops a BiLO-Auto-TSF/ML framework that automates the optimal design of a few-shot learning pipeline from a bi-level programming perspective. Specifically, the lower-level meta-learning helps boost the base-learner to mitigate the small data challenge while the hyperparameter optimization at the upper level proactively searches for the optimal hyperparameter configurations for both base- and meta-learners. Note that the proposed framework is so general that any off-the-shelf machine learning method can be used in a plug-in manner. Comprehensive experiments fully demonstrate the effectiveness of our proposed BiLO-Auto-TSF/ML framework to search for a high-performance few-shot learning pipeline for various energy sources.", "paper_url": "http://arxiv.org/abs/2203.03328v1", "pdf_url": "http://arxiv.org/pdf/2203.03328v1", "repo_url": null}, "2203.03279": {"publish_time": "2022-03-07", "title": "Evaluating State of the Art, Forecasting Ensembles- and Meta-learning Strategies for Model Fusion", "author": "Pieter Cawood et.al.", "abstract": "Techniques of hybridisation and ensemble learning are popular model fusion techniques for improving the predictive power of forecasting methods. With limited research that instigates combining these two promising approaches, this paper focuses on the utility of the Exponential-Smoothing-Recurrent Neural Network (ES-RNN) in the pool of base models for different ensembles. We compare against some state of the art ensembling techniques and arithmetic model averaging as a benchmark. We experiment with the M4 forecasting data set of 100,000 time-series, and the results show that the Feature-based Forecast Model Averaging (FFORMA), on average, is the best technique for late data fusion with the ES-RNN. However, considering the M4's Daily subset of data, stacking was the only successful ensemble at dealing with the case where all base model performances are similar. Our experimental results indicate that we attain state of the art forecasting results compared to N-BEATS as a benchmark. We conclude that model averaging is a more robust ensemble than model selection and stacking strategies. Further, the results show that gradient boosting is superior for implementing ensemble learning strategies.", "paper_url": "http://arxiv.org/abs/2203.03279v1", "pdf_url": "http://arxiv.org/pdf/2203.03279v1", "repo_url": null}, "2203.03191": {"publish_time": "2022-03-07", "title": "Language-Agnostic Meta-Learning for Low-Resource Text-to-Speech with Articulatory Features", "author": "Florian Lux et.al.", "abstract": "While neural text-to-speech systems perform remarkably well in high-resource scenarios, they cannot be applied to the majority of the over 6,000 spoken languages in the world due to a lack of appropriate training data. In this work, we use embeddings derived from articulatory vectors rather than embeddings derived from phoneme identities to learn phoneme representations that hold across languages. In conjunction with language agnostic meta learning, this enables us to fine-tune a high-quality text-to-speech model on just 30 minutes of data in a previously unseen language spoken by a previously unseen speaker.", "paper_url": "http://arxiv.org/abs/2203.03191v1", "pdf_url": "http://arxiv.org/pdf/2203.03191v1", "repo_url": "https://github.com/digitalphonetics/ims-toucan"}, "2203.03059": {"publish_time": "2022-03-06", "title": "Is Bayesian Model-Agnostic Meta Learning Better than Model-Agnostic Meta Learning, Provably?", "author": "Lisha Chen et.al.", "abstract": "Meta learning aims at learning a model that can quickly adapt to unseen tasks. Widely used meta learning methods include model agnostic meta learning (MAML), implicit MAML, Bayesian MAML. Thanks to its ability of modeling uncertainty, Bayesian MAML often has advantageous empirical performance. However, the theoretical understanding of Bayesian MAML is still limited, especially on questions such as if and when Bayesian MAML has provably better performance than MAML. In this paper, we aim to provide theoretical justifications for Bayesian MAML's advantageous performance by comparing the meta test risks of MAML and Bayesian MAML. In the meta linear regression, under both the distribution agnostic and linear centroid cases, we have established that Bayesian MAML indeed has provably lower meta test risks than MAML. We verify our theoretical results through experiments.", "paper_url": "http://arxiv.org/abs/2203.03059v1", "pdf_url": "http://arxiv.org/pdf/2203.03059v1", "repo_url": "https://github.com/lisha-chen/Bayesian-MAML-vs-MAML"}, "2203.02711": {"publish_time": "2022-03-05", "title": "Meta Mirror Descent: Optimiser Learning for Fast Convergence", "author": "Boyan Gao et.al.", "abstract": "Optimisers are an essential component for training machine learning models, and their design influences learning speed and generalisation. Several studies have attempted to learn more effective gradient-descent optimisers via solving a bi-level optimisation problem where generalisation error is minimised with respect to optimiser parameters. However, most existing optimiser learning methods are intuitively motivated, without clear theoretical support. We take a different perspective starting from mirror descent rather than gradient descent, and meta-learning the corresponding Bregman divergence. Within this paradigm, we formalise a novel meta-learning objective of minimising the regret bound of learning. The resulting framework, termed Meta Mirror Descent (MetaMD), learns to accelerate optimisation speed. Unlike many meta-learned optimisers, it also supports convergence and generalisation guarantees and uniquely does so without requiring validation data. We evaluate our framework on a variety of tasks and architectures in terms of convergence rate and generalisation error and demonstrate strong performance.", "paper_url": "http://arxiv.org/abs/2203.02711v1", "pdf_url": "http://arxiv.org/pdf/2203.02711v1", "repo_url": null}, "2203.03978": {"publish_time": "2022-03-08", "title": "Contrastive Conditional Neural Processes", "author": "Zesheng Ye et.al.", "abstract": "Conditional Neural Processes~(CNPs) bridge neural networks with probabilistic inference to approximate functions of Stochastic Processes under meta-learning settings. Given a batch of non-{\\it i.i.d} function instantiations, CNPs are jointly optimized for in-instantiation observation prediction and cross-instantiation meta-representation adaptation within a generative reconstruction pipeline. There can be a challenge in tying together such two targets when the distribution of function observations scales to high-dimensional and noisy spaces. Instead, noise contrastive estimation might be able to provide more robust representations by learning distributional matching objectives to combat such inherent limitation of generative models. In light of this, we propose to equip CNPs by 1) aligning prediction with encoded ground-truth observation, and 2) decoupling meta-representation adaptation from generative reconstruction. Specifically, two auxiliary contrastive branches are set up hierarchically, namely in-instantiation temporal contrastive learning~({\\tt TCL}) and cross-instantiation function contrastive learning~({\\tt FCL}), to facilitate local predictive alignment and global function consistency, respectively. We empirically show that {\\tt TCL} captures high-level abstraction of observations, whereas {\\tt FCL} helps identify underlying functions, which in turn provides more efficient representations. Our model outperforms other CNPs variants when evaluating function distribution reconstruction and parameter identification across 1D, 2D and high-dimensional time-series.", "paper_url": "http://arxiv.org/abs/2203.03978v1", "pdf_url": "http://arxiv.org/pdf/2203.03978v1", "repo_url": null}, "2203.04905": {"publish_time": "2022-03-09", "title": "What Matters For Meta-Learning Vision Regression Tasks?", "author": "Ning Gao et.al.", "abstract": "Meta-learning is widely used in few-shot classification and function regression due to its ability to quickly adapt to unseen tasks. However, it has not yet been well explored on regression tasks with high dimensional inputs such as images. This paper makes two main contributions that help understand this barely explored area. \\emph{First}, we design two new types of cross-category level vision regression tasks, namely object discovery and pose estimation of unprecedented complexity in the meta-learning domain for computer vision. To this end, we (i) exhaustively evaluate common meta-learning techniques on these tasks, and (ii) quantitatively analyze the effect of various deep learning techniques commonly used in recent meta-learning algorithms in order to strengthen the generalization capability: data augmentation, domain randomization, task augmentation and meta-regularization. Finally, we (iii) provide some insights and practical recommendations for training meta-learning algorithms on vision regression tasks. \\emph{Second}, we propose the addition of functional contrastive learning (FCL) over the task representations in Conditional Neural Processes (CNPs) and train in an end-to-end fashion. The experimental results show that the results of prior work are misleading as a consequence of a poor choice of the loss function as well as too small meta-training sets. Specifically, we find that CNPs outperform MAML on most tasks without fine-tuning. Furthermore, we observe that naive task augmentation without a tailored design results in underfitting.", "paper_url": "http://arxiv.org/abs/2203.04905v1", "pdf_url": "http://arxiv.org/pdf/2203.04905v1", "repo_url": null}, "2203.04904": {"publish_time": "2022-03-09", "title": "Model-Agnostic Multitask Fine-tuning for Few-shot Vision-Language Transfer Learning", "author": "Zhenhailong Wang et.al.", "abstract": "Despite achieving state-of-the-art zero-shot performance, existing vision-language models, e.g., CLIP, still fall short of domain-specific classification tasks, e.g., Fungi Classification. In the context of few-shot transfer learning, traditional fine-tuning fails to prevent highly expressive model from exploiting spurious correlations in the training data. On the other hand, although model-agnostic meta-learning (MAML) presents as a natural alternative for transfer learning, the expensive computation due to implicit second-order optimization limits its use in large-scale models and datasets. In this work we aim to further improve the generalization of existing vision-language models on unseen tasks via a simple yet efficient fine-tuning strategy based on uniform task sampling. We term our method as Model-Agnostic Multitask Fine-tuning (MAMF). Compared with MAML, MAMF discards the bi-level optimization and uses only first-order gradients, which makes it easily scalable and computationally efficient. Due to the uniform task sampling procedure, MAMF consistently outperforms the classical fine-tuning method for few-shot transfer learning on five benchmark datasets. Empirically, we further discover that the effectiveness of first-order MAML is highly dependent on the zero-shot performance of the pretrained model, and our simple algorithm can outperform first-order MAML on more challenging datasets with low zero-shot performance.", "paper_url": "http://arxiv.org/abs/2203.04904v1", "pdf_url": "http://arxiv.org/pdf/2203.04904v1", "repo_url": null}, "2203.04540": {"publish_time": "2022-03-09", "title": "MetaCon: Unified Predictive Segments System with Trillion Concept Meta-Learning", "author": "Keqian Li et.al.", "abstract": "Accurate understanding of users in terms of predicative segments play an essential role in the day to day operation of modern internet enterprises. Nevertheless, there are significant challenges that limit the quality of data, especially on long tail predictive tasks. In this work, we present MetaCon, our unified predicative segments system with scalable, trillion concepts meta learning that addresses these challenges. It builds on top of a flat concept representation that summarizes entities' heterogeneous digital footprint, jointly considers the entire spectrum of predicative tasks as a single learning task, and leverages principled meta learning approach with efficient first order meta-optimization procedure under a provable performance guarantee in order to solve the learning task. Experiments on both proprietary production datasets and public structured learning tasks demonstrate that MetaCon can lead to substantial improvements over state of the art recommendation and ranking approaches.", "paper_url": "http://arxiv.org/abs/2203.04540v1", "pdf_url": "http://arxiv.org/pdf/2203.04540v1", "repo_url": null}, "2203.04291": {"publish_time": "2022-03-07", "title": "Learning from Few Examples: A Summary of Approaches to Few-Shot Learning", "author": "Archit Parnami et.al.", "abstract": "Few-Shot Learning refers to the problem of learning the underlying pattern in the data just from a few training samples. Requiring a large number of data samples, many deep learning solutions suffer from data hunger and extensively high computation time and resources. Furthermore, data is often not available due to not only the nature of the problem or privacy concerns but also the cost of data preparation. Data collection, preprocessing, and labeling are strenuous human tasks. Therefore, few-shot learning that could drastically reduce the turnaround time of building machine learning applications emerges as a low-cost solution. This survey paper comprises a representative list of recently proposed few-shot learning algorithms. Given the learning dynamics and characteristics, the approaches to few-shot learning problems are discussed in the perspectives of meta-learning, transfer learning, and hybrid approaches (i.e., different variations of the few-shot learning problem).", "paper_url": "http://arxiv.org/abs/2203.04291v1", "pdf_url": "http://arxiv.org/pdf/2203.04291v1", "repo_url": null}, "2203.05119": {"publish_time": "2022-03-10", "title": "MetAug: Contrastive Learning via Meta Feature Augmentation", "author": "Jiangmeng Li et.al.", "abstract": "What matters for contrastive learning? We argue that contrastive learning heavily relies on informative features, or \"hard\" (positive or negative) features. Early works include more informative features by applying complex data augmentations and large batch size or memory bank, and recent works design elaborate sampling approaches to explore informative features. The key challenge toward exploring such features is that the source multi-view data is generated by applying random data augmentations, making it infeasible to always add useful information in the augmented data. Consequently, the informativeness of features learned from such augmented data is limited. In response, we propose to directly augment the features in latent space, thereby learning discriminative representations without a large amount of input data. We perform a meta learning technique to build the augmentation generator that updates its network parameters by considering the performance of the encoder. However, insufficient input data may lead the encoder to learn collapsed features and therefore malfunction the augmentation generator. A new margin-injected regularization is further added in the objective function to avoid the encoder learning a degenerate mapping. To contrast all features in one gradient back-propagation step, we adopt the proposed optimization-driven unified contrastive loss instead of the conventional contrastive loss. Empirically, our method achieves state-of-the-art results on several benchmark datasets.", "paper_url": "http://arxiv.org/abs/2203.05119v1", "pdf_url": "http://arxiv.org/pdf/2203.05119v1", "repo_url": null}, "2203.05882": {"publish_time": "2022-03-11", "title": "Improving the transferability of speech separation by meta-learning", "author": "Kuan-Po Huang et.al.", "abstract": "Speech separation aims to separate multiple speech sources from a speech mixture. Although speech separation is well-solved on some existing English speech separation benchmarks, it is worthy of more investigation on the generalizability of speech separation models on the accents or languages unseen during training. This paper adopts meta-learning based methods to improve the transferability of speech separation models. With the meta-learning based methods, we discovered that only using speech data with one accent, the native English accent, as our training data, the models still can be adapted to new unseen accents on the Speech Accent Archive. We compared the results with a human-rated native-likeness of accents, showing that the transferability of MAML methods has less relation to the similarity of data between the training and testing phase compared to the typical transfer learning methods. Furthermore, we found that models can deal with different language data from the CommonVoice corpus during the testing phase. Most of all, the MAML methods outperform typical transfer learning methods when it comes to new accents, new speakers, new languages, and noisy environments.", "paper_url": "http://arxiv.org/abs/2203.05882v1", "pdf_url": "http://arxiv.org/pdf/2203.05882v1", "repo_url": "https://github.com/nobel861017/mtss"}, "2203.07725": {"publish_time": "2022-03-15", "title": "Meta Ordinal Regression Forest for Medical Image Classification with Ordinal Labels", "author": "Yiming Lei et.al.", "abstract": "The performance of medical image classification has been enhanced by deep convolutional neural networks (CNNs), which are typically trained with cross-entropy (CE) loss. However, when the label presents an intrinsic ordinal property in nature, e.g., the development from benign to malignant tumor, CE loss cannot take into account such ordinal information to allow for better generalization. To improve model generalization with ordinal information, we propose a novel meta ordinal regression forest (MORF) method for medical image classification with ordinal labels, which learns the ordinal relationship through the combination of convolutional neural network and differential forest in a meta-learning framework. The merits of the proposed MORF come from the following two components: a tree-wise weighting net (TWW-Net) and a grouped feature selection (GFS) module. First, the TWW-Net assigns each tree in the forest with a specific weight that is mapped from the classification loss of the corresponding tree. Hence, all the trees possess varying weights, which is helpful for alleviating the tree-wise prediction variance. Second, the GFS module enables a dynamic forest rather than a fixed one that was previously used, allowing for random feature perturbation. During training, we alternatively optimize the parameters of the CNN backbone and TWW-Net in the meta-learning framework through calculating the Hessian matrix. Experimental results on two medical image classification datasets with ordinal labels, i.e., LIDC-IDRI and Breast Ultrasound Dataset, demonstrate the superior performances of our MORF method over existing state-of-the-art methods.", "paper_url": "http://arxiv.org/abs/2203.07725v1", "pdf_url": "http://arxiv.org/pdf/2203.07725v1", "repo_url": null}, "2203.07615": {"publish_time": "2022-03-15", "title": "Learning What Not to Segment: A New Perspective on Few-Shot Segmentation", "author": "Chunbo Lang et.al.", "abstract": "Recently few-shot segmentation (FSS) has been extensively developed. Most previous works strive to achieve generalization through the meta-learning framework derived from classification tasks; however, the trained models are biased towards the seen classes instead of being ideally class-agnostic, thus hindering the recognition of new concepts. This paper proposes a fresh and straightforward insight to alleviate the problem. Specifically, we apply an additional branch (base learner) to the conventional FSS model (meta learner) to explicitly identify the targets of base classes, i.e., the regions that do not need to be segmented. Then, the coarse results output by these two learners in parallel are adaptively integrated to yield precise segmentation prediction. Considering the sensitivity of meta learner, we further introduce an adjustment factor to estimate the scene differences between the input image pairs for facilitating the model ensemble forecasting. The substantial performance gains on PASCAL-5i and COCO-20i verify the effectiveness, and surprisingly, our versatile scheme sets a new state-of-the-art even with two plain learners. Moreover, in light of the unique nature of the proposed approach, we also extend it to a more realistic but challenging setting, i.e., generalized FSS, where the pixels of both base and novel classes are required to be determined. The source code is available at github.com/chunbolang/BAM.", "paper_url": "http://arxiv.org/abs/2203.07615v1", "pdf_url": "http://arxiv.org/pdf/2203.07615v1", "repo_url": "https://github.com/chunbolang/BAM"}, "2203.07910": {"publish_time": "2022-03-14", "title": "Deep Transfer Learning with Graph Neural Network for Sensor-Based Human Activity Recognition", "author": "Yan Yan et.al.", "abstract": "The sensor-based human activity recognition (HAR) in mobile application scenarios is often confronted with sensor modalities variation and annotated data deficiency. Given this observation, we devised a graph-inspired deep learning approach toward the sensor-based HAR tasks, which was further used to build a deep transfer learning model toward giving a tentative solution for these two challenging problems. Specifically, we present a multi-layer residual structure involved graph convolutional neural network (ResGCNN) toward the sensor-based HAR tasks, namely the HAR-ResGCNN approach. Experimental results on the PAMAP2 and mHealth data sets demonstrate that our ResGCNN is effective at capturing the characteristics of actions with comparable results compared to other sensor-based HAR models (with an average accuracy of 98.18% and 99.07%, respectively). More importantly, the deep transfer learning experiments using the ResGCNN model show excellent transferability and few-shot learning performance. The graph-based framework shows good meta-learning ability and is supposed to be a promising solution in sensor-based HAR tasks.", "paper_url": "http://arxiv.org/abs/2203.07910v1", "pdf_url": "http://arxiv.org/pdf/2203.07910v1", "repo_url": null}, "2203.08775": {"publish_time": "2022-03-16", "title": "Practical Conditional Neural Processes Via Tractable Dependent Predictions", "author": "Stratis Markou et.al.", "abstract": "Conditional Neural Processes (CNPs; Garnelo et al., 2018a) are meta-learning models which leverage the flexibility of deep learning to produce well-calibrated predictions and naturally handle off-the-grid and missing data. CNPs scale to large datasets and train with ease. Due to these features, CNPs appear well-suited to tasks from environmental sciences or healthcare. Unfortunately, CNPs do not produce correlated predictions, making them fundamentally inappropriate for many estimation and decision making tasks. Predicting heat waves or floods, for example, requires modelling dependencies in temperature or precipitation over time and space. Existing approaches which model output dependencies, such as Neural Processes (NPs; Garnelo et al., 2018b) or the FullConvGNP (Bruinsma et al., 2021), are either complicated to train or prohibitively expensive. What is needed is an approach which provides dependent predictions, but is simple to train and computationally tractable. In this work, we present a new class of Neural Process models that make correlated predictions and support exact maximum likelihood training that is simple and scalable. We extend the proposed models by using invertible output transformations, to capture non-Gaussian output distributions. Our models can be used in downstream estimation tasks which require dependent function samples. By accounting for output dependencies, our models show improved predictive performance on a range of experiments with synthetic and real data.", "paper_url": "http://arxiv.org/abs/2203.08775v1", "pdf_url": "http://arxiv.org/pdf/2203.08775v1", "repo_url": null}, "2203.09137": {"publish_time": "2022-03-17", "title": "Global Convergence of MAML and Theory-Inspired Neural Architecture Search for Few-Shot Learning", "author": "Haoxiang Wang et.al.", "abstract": "Model-agnostic meta-learning (MAML) and its variants have become popular approaches for few-shot learning. However, due to the non-convexity of deep neural nets (DNNs) and the bi-level formulation of MAML, the theoretical properties of MAML with DNNs remain largely unknown. In this paper, we first prove that MAML with over-parameterized DNNs is guaranteed to converge to global optima at a linear rate. Our convergence analysis indicates that MAML with over-parameterized DNNs is equivalent to kernel regression with a novel class of kernels, which we name as Meta Neural Tangent Kernels (MetaNTK). Then, we propose MetaNTK-NAS, a new training-free neural architecture search (NAS) method for few-shot learning that uses MetaNTK to rank and select architectures. Empirically, we compare our MetaNTK-NAS with previous NAS methods on two popular few-shot learning benchmarks, miniImageNet, and tieredImageNet. We show that the performance of MetaNTK-NAS is comparable or better than the state-of-the-art NAS method designed for few-shot learning while enjoying more than 100x speedup. We believe the efficiency of MetaNTK-NAS makes itself more practical for many real-world tasks.", "paper_url": "http://arxiv.org/abs/2203.09137v1", "pdf_url": "http://arxiv.org/pdf/2203.09137v1", "repo_url": "https://github.com/yitewang/metantk-nas"}, "2203.08951": {"publish_time": "2022-03-16", "title": "Meta-Learning of NAS for Few-shot Learning in Medical Image Applications", "author": "Viet-Khoa Vo-Ho et.al.", "abstract": "Deep learning methods have been successful in solving tasks in machine learning and have made breakthroughs in many sectors owing to their ability to automatically extract features from unstructured data. However, their performance relies on manual trial-and-error processes for selecting an appropriate network architecture, hyperparameters for training, and pre-/post-procedures. Even though it has been shown that network architecture plays a critical role in learning feature representation feature from data and the final performance, searching for the best network architecture is computationally intensive and heavily relies on researchers' experience. Automated machine learning (AutoML) and its advanced techniques i.e. Neural Architecture Search (NAS) have been promoted to address those limitations. Not only in general computer vision tasks, but NAS has also motivated various applications in multiple areas including medical imaging. In medical imaging, NAS has significant progress in improving the accuracy of image classification, segmentation, reconstruction, and more. However, NAS requires the availability of large annotated data, considerable computation resources, and pre-defined tasks. To address such limitations, meta-learning has been adopted in the scenarios of few-shot learning and multiple tasks. In this book chapter, we first present a brief review of NAS by discussing well-known approaches in search space, search strategy, and evaluation strategy. We then introduce various NAS approaches in medical imaging with different applications such as classification, segmentation, detection, reconstruction, etc. Meta-learning in NAS for few-shot learning and multiple tasks is then explained. Finally, we describe several open problems in NAS.", "paper_url": "http://arxiv.org/abs/2203.08951v1", "pdf_url": "http://arxiv.org/pdf/2203.08951v1", "repo_url": null}, "2203.09661": {"publish_time": "2022-03-17", "title": "Meta Reinforcement Learning for Adaptive Control: An Offline Approach", "author": "Daniel G. McClement et.al.", "abstract": "Meta-learning is a branch of machine learning which trains neural network models to synthesize a wide variety of data in order to rapidly solve new problems. In process control, many systems have similar and well-understood dynamics, which suggests it is feasible to create a generalizable controller through meta-learning. In this work, we formulate a meta reinforcement learning (meta-RL) control strategy that takes advantage of known, offline information for training, such as the system gain or time constant, yet efficiently controls novel systems in a completely model-free fashion. Our meta-RL agent has a recurrent structure that accumulates \"context\" for its current dynamics through a hidden state variable. This end-to-end architecture enables the agent to automatically adapt to changes in the process dynamics. Moreover, the same agent can be deployed on systems with previously unseen nonlinearities and timescales. In tests reported here, the meta-RL agent was trained entirely offline, yet produced excellent results in novel settings. A key design element is the ability to leverage model-based information offline during training, while maintaining a model-free policy structure for interacting with novel environments. To illustrate the approach, we take the actions proposed by the meta-RL agent to be changes to gains of a proportional-integral controller, resulting in a generalized, adaptive, closed-loop tuning strategy. Meta-learning is a promising approach for constructing sample-efficient intelligent controllers.", "paper_url": "http://arxiv.org/abs/2203.09661v1", "pdf_url": "http://arxiv.org/pdf/2203.09661v1", "repo_url": null}, "2203.11074": {"publish_time": "2022-03-21", "title": "Distributed Stochastic Compositional Optimization Problems over Directed Networks", "author": "Shengchao Zhao et.al.", "abstract": "We study the distributed stochastic compositional optimization problems over directed communication networks in which agents privately own a stochastic compositional objective function and collaborate to minimize the sum of all objective functions. We propose a distributed stochastic compositional gradient descent method, where the gradient tracking and the stochastic correction techniques are employed to adapt to the networks' directed structure and increase the accuracy of inner function estimation. When the objective function is smooth, the proposed method achieves the convergence rate $\\mathcal{O}\\left(k^{-1/2}\\right)$ and sample complexity $\\mathcal{O}\\left(\\frac{1}{\\epsilon^2}\\right)$ for finding the ($\\epsilon$)-stationary point. When the objective function is strongly convex, the convergence rate is improved to $\\mathcal{O}\\left(k^{-1}\\right)$. Moreover, the asymptotic normality of Polyak-Ruppert averaged iterates of the proposed method is also presented. We demonstrate the empirical performance of the proposed method on model-agnostic meta-learning problem and logistic regression problem.", "paper_url": "http://arxiv.org/abs/2203.11074v1", "pdf_url": "http://arxiv.org/pdf/2203.11074v1", "repo_url": null}, "2203.10395": {"publish_time": "2022-03-19", "title": "Towards Robust Semantic Segmentation of Accident Scenes via Multi-Source Mixed Sampling and Meta-Learning", "author": "Xinyu Luo et.al.", "abstract": "Autonomous vehicles utilize urban scene segmentation to understand the real world like a human and react accordingly. Semantic segmentation of normal scenes has experienced a remarkable rise in accuracy on conventional benchmarks. However, a significant portion of real-life accidents features abnormal scenes, such as those with object deformations, overturns, and unexpected traffic behaviors. Since even small mis-segmentation of driving scenes can lead to serious threats to human lives, the robustness of such models in accident scenarios is an extremely important factor in ensuring safety of intelligent transportation systems.   In this paper, we propose a Multi-source Meta-learning Unsupervised Domain Adaptation (MMUDA) framework, to improve the generalization of segmentation transformers to extreme accident scenes. In MMUDA, we make use of Multi-Domain Mixed Sampling to augment the images of multiple-source domains (normal scenes) with the target data appearances (abnormal scenes). To train our model, we intertwine and study a meta-learning strategy in the multi-source setting for robustifying the segmentation results. We further enhance the segmentation backbone (SegFormer) with a HybridASPP decoder design, featuring large window attention spatial pyramid pooling and strip pooling, to efficiently aggregate long-range contextual dependencies. Our approach achieves a mIoU score of 46.97% on the DADA-seg benchmark, surpassing the previous state-of-the-art model by more than 7.50%. Code will be made publicly available at https://github.com/xinyu-laura/MMUDA.", "paper_url": "http://arxiv.org/abs/2203.10395v1", "pdf_url": "http://arxiv.org/pdf/2203.10395v1", "repo_url": "https://github.com/xinyu-laura/mmuda"}, "2203.10354": {"publish_time": "2022-03-19", "title": "Meta-Learning for Online Update of Recommender Systems", "author": "Minseok Kim et.al.", "abstract": "Online recommender systems should be always aligned with users' current interest to accurately suggest items that each user would like. Since user interest usually evolves over time, the update strategy should be flexible to quickly catch users' current interest from continuously generated new user-item interactions. Existing update strategies focus either on the importance of each user-item interaction or the learning rate for each recommender parameter, but such one-directional flexibility is insufficient to adapt to varying relationships between interactions and parameters. In this paper, we propose MeLON, a meta-learning based novel online recommender update strategy that supports two-directional flexibility. It is featured with an adaptive learning rate for each parameter-interaction pair for inducing a recommender to quickly learn users' up-to-date interest. The procedure of MeLON is optimized following a meta-learning approach: it learns how a recommender learns to generate the optimal learning rates for future updates. Specifically, MeLON first enriches the meaning of each interaction based on previous interactions and identifies the role of each parameter for the interaction; and then combines these two pieces of information to generate an adaptive learning rate. Theoretical analysis and extensive evaluation on three real-world online recommender datasets validate the effectiveness of MeLON.", "paper_url": "http://arxiv.org/abs/2203.10354v1", "pdf_url": "http://arxiv.org/pdf/2203.10354v1", "repo_url": "https://github.com/kaist-dmlab/MeLON"}, "2203.10250": {"publish_time": "2022-03-19", "title": "Meta-X$_{NLG}$: A Meta-Learning Approach Based on Language Clustering for Zero-Shot Cross-Lingual Transfer and Generation", "author": "Kaushal Kumar Maurya et.al.", "abstract": "Recently, the NLP community has witnessed a rapid advancement in multilingual and cross-lingual transfer research where the supervision is transferred from high-resource languages (HRLs) to low-resource languages (LRLs). However, the cross-lingual transfer is not uniform across languages, particularly in the zero-shot setting. Towards this goal, one promising research direction is to learn shareable structures across multiple tasks with limited annotated data. The downstream multilingual applications may benefit from such a learning setup as most of the languages across the globe are low-resource and share some structures with other languages. In this paper, we propose a novel meta-learning framework (called Meta-X$_{NLG}$) to learn shareable structures from typologically diverse languages based on meta-learning and language clustering. This is a step towards uniform cross-lingual transfer for unseen languages. We first cluster the languages based on language representations and identify the centroid language of each cluster. Then, a meta-learning algorithm is trained with all centroid languages and evaluated on the other languages in the zero-shot setting. We demonstrate the effectiveness of this modeling on two NLG tasks (Abstractive Text Summarization and Question Generation), 5 popular datasets and 30 typologically diverse languages. Consistent improvements over strong baselines demonstrate the efficacy of the proposed framework. The careful design of the model makes this end-to-end NLG setup less vulnerable to the accidental translation problem, which is a prominent concern in zero-shot cross-lingual NLG tasks.", "paper_url": "http://arxiv.org/abs/2203.10250v1", "pdf_url": "http://arxiv.org/pdf/2203.10250v1", "repo_url": null}, "2203.10185": {"publish_time": "2022-03-18", "title": "Negative Inner-Loop Learning Rates Learn Universal Features", "author": "Tom Starshak et.al.", "abstract": "Model Agnostic Meta-Learning (MAML) consists of two optimization loops: the outer loop learns a meta-initialization of model parameters that is shared across tasks, and the inner loop task-specific adaptation step. A variant of MAML, Meta-SGD, uses the same two loop structure, but also learns the learning-rate for the adaptation step. Little attention has been paid to how the learned learning-rate of Meta-SGD affects feature reuse. In this paper, we study the effect that a learned learning-rate has on the per-task feature representations in Meta-SGD. The learned learning-rate of Meta-SGD often contains negative values. During the adaptation phase, these negative learning rates push features away from task-specific features and towards task-agnostic features.   We performed several experiments on the Mini-Imagenet dataset. Two neural networks were trained, one with MAML, and one with Meta-SGD. The feature quality for both models was tested as follows: strip away the linear classification layer, pass labeled and unlabeled samples through this encoder, classify the unlabeled samples according to their nearest neighbor. This process was performed: 1) after training and using the meta-initialization parameters; 2) after adaptation, and validated on that task; and 3) after adaptation, and validated on a different task. The MAML trained model improved on the task it was adapted to, but had worse performance on other tasks. The Meta-SGD trained model was the opposite; it had worse performance on the task it was adapted to, but improved on other tasks. This confirms the hypothesis that Meta-SGD's negative learning rates cause the model to learn task-agnostic features rather than simply adapt to task specific features.", "paper_url": "http://arxiv.org/abs/2203.10185v1", "pdf_url": "http://arxiv.org/pdf/2203.10185v1", "repo_url": null}, "2203.11670": {"publish_time": "2022-03-22", "title": "Improving Meta-learning for Low-resource Text Classification and Generation via Memory Imitation", "author": "Yingxiu Zhao et.al.", "abstract": "Building models of natural language processing (NLP) is challenging in low-resource scenarios where only limited data are available. Optimization-based meta-learning algorithms achieve promising results in low-resource scenarios by adapting a well-generalized model initialization to handle new tasks. Nonetheless, these approaches suffer from the memorization overfitting issue, where the model tends to memorize the meta-training tasks while ignoring support sets when adapting to new tasks. To address this issue, we propose a memory imitation meta-learning (MemIML) method that enhances the model's reliance on support sets for task adaptation. Specifically, we introduce a task-specific memory module to store support set information and construct an imitation module to force query sets to imitate the behaviors of some representative support-set samples stored in the memory. A theoretical analysis is provided to prove the effectiveness of our method, and empirical results also demonstrate that our method outperforms competitive baselines on both text classification and generation tasks.", "paper_url": "http://arxiv.org/abs/2203.11670v1", "pdf_url": "http://arxiv.org/pdf/2203.11670v1", "repo_url": null}, "2203.12304": {"publish_time": "2022-03-23", "title": "Domain-Generalized Textured Surface Anomaly Detection", "author": "Shang-Fu Chen et.al.", "abstract": "Anomaly detection aims to identify abnormal data that deviates from the normal ones, while typically requiring a sufficient amount of normal data to train the model for performing this task. Despite the success of recent anomaly detection methods, performing anomaly detection in an unseen domain remain a challenging task. In this paper, we address the task of domain-generalized textured surface anomaly detection. By observing normal and abnormal surface data across multiple source domains, our model is expected to be generalized to an unseen textured surface of interest, in which only a small number of normal data can be observed during testing. Although with only image-level labels observed in the training data, our patch-based meta-learning model exhibits promising generalization ability: not only can it generalize to unseen image domains, but it can also localize abnormal regions in the query image. Our experiments verify that our model performs favorably against state-of-the-art anomaly detection and domain generalization approaches in various settings.", "paper_url": "http://arxiv.org/abs/2203.12304v1", "pdf_url": "http://arxiv.org/pdf/2203.12304v1", "repo_url": null}, "2203.12274": {"publish_time": "2022-03-23", "title": "Pre-training to Match for Unified Low-shot Relation Extraction", "author": "Fangchao Liu et.al.", "abstract": "Low-shot relation extraction~(RE) aims to recognize novel relations with very few or even no samples, which is critical in real scenario application. Few-shot and zero-shot RE are two representative low-shot RE tasks, which seem to be with similar target but require totally different underlying abilities. In this paper, we propose Multi-Choice Matching Networks to unify low-shot relation extraction. To fill in the gap between zero-shot and few-shot RE, we propose the triplet-paraphrase meta-training, which leverages triplet paraphrase to pre-train zero-shot label matching ability and uses meta-learning paradigm to learn few-shot instance summarizing ability. Experimental results on three different low-shot RE tasks show that the proposed method outperforms strong baselines by a large margin, and achieve the best performance on few-shot RE leaderboard.", "paper_url": "http://arxiv.org/abs/2203.12274v1", "pdf_url": "http://arxiv.org/pdf/2203.12274v1", "repo_url": "https://github.com/fc-liu/mcmn"}, "2203.12768": {"publish_time": "2022-03-23", "title": "Multidimensional Belief Quantification for Label-Efficient Meta-Learning", "author": "Deep Pandey et.al.", "abstract": "Optimization-based meta-learning offers a promising direction for few-shot learning that is essential for many real-world computer vision applications. However, learning from few samples introduces uncertainty, and quantifying model confidence for few-shot predictions is essential for many critical domains. Furthermore, few-shot tasks used in meta training are usually sampled randomly from a task distribution for an iterative model update, leading to high labeling costs and computational overhead in meta-training. We propose a novel uncertainty-aware task selection model for label efficient meta-learning. The proposed model formulates a multidimensional belief measure, which can quantify the known uncertainty and lower bound the unknown uncertainty of any given task. Our theoretical result establishes an important relationship between the conflicting belief and the incorrect belief. The theoretical result allows us to estimate the total uncertainty of a task, which provides a principled criterion for task selection. A novel multi-query task formulation is further developed to improve both the computational and labeling efficiency of meta-learning. Experiments conducted over multiple real-world few-shot image classification tasks demonstrate the effectiveness of the proposed model.", "paper_url": "http://arxiv.org/abs/2203.12768v1", "pdf_url": "http://arxiv.org/pdf/2203.12768v1", "repo_url": null}, "2203.12715": {"publish_time": "2022-03-23", "title": "Predicting Multi-Antenna Frequency-Selective Channels via Meta-Learned Linear Filters based on Long-Short Term Channel Decomposition", "author": "Sangwoo Park et.al.", "abstract": "An efficient data-driven prediction strategy for multi-antenna frequency-selective channels must operate based on a small number of pilot symbols. This paper proposes novel channel prediction algorithms that address this goal by integrating transfer and meta-learning with a reduced-rank parametrization of the channel. The proposed methods optimize linear predictors by utilizing data from previous frames, which are generally characterized by distinct propagation characteristics, in order to enable fast training on the time slots of the current frame. The proposed predictors rely on a novel long-short-term decomposition (LSTD) of the linear prediction model that leverages the disaggregation of the channel into long-term space-time signatures and fading amplitudes. We first develop predictors for single-antenna frequency-flat channels based on transfer/meta-learned quadratic regularization. Then, we introduce transfer and meta-learning algorithms for LSTD-based prediction models that build on equilibrium propagation (EP) and alternating least squares (ALS). Numerical results under the 3GPP 5G standard channel model demonstrate the impact of transfer and meta-learning on reducing the number of pilots for channel prediction, as well as the merits of the proposed LSTD parametrization.", "paper_url": "http://arxiv.org/abs/2203.12715v1", "pdf_url": "http://arxiv.org/pdf/2203.12715v1", "repo_url": null}, "2203.12653": {"publish_time": "2022-03-23", "title": "Iterative Implicit Gradients for Nonconvex Optimization with Variational Inequality Constraints", "author": "Harshal D. Kaushik et.al.", "abstract": "We propose an implicit gradient based scheme for a constrained optimization problem with nonconvex loss function, which can be potentially used to analyze a variety of applications in machine learning, including meta-learning, hyperparameter optimization, and reinforcement learning. The proposed algorithm is based on the iterative differentiation (ITD) strategy. We extend the convergence and rate analysis of the current literature of bilevel optimization to a constrained bilevel structure with the motivation of learning under constraints. For addressing bilevel optimization using any first-order scheme, it involves the gradient of the inner-level optimal solution with respect to the outer variable (implicit gradient). In this paper, taking into account of a possible large-scale structure, we propose an efficient way of obtaining the implicit gradient. We further provide error bounds with respect to the true gradients. Further, we provide nonasymptotic rate results.", "paper_url": "http://arxiv.org/abs/2203.12653v1", "pdf_url": "http://arxiv.org/pdf/2203.12653v1", "repo_url": null}, "2203.13610": {"publish_time": "2022-03-25", "title": "Learning to Adapt to Unseen Abnormal Activities under Weak Supervision", "author": "Jaeyoo Park et.al.", "abstract": "We present a meta-learning framework for weakly supervised anomaly detection in videos, where the detector learns to adapt to unseen types of abnormal activities effectively when only video-level annotations of binary labels are available. Our work is motivated by the fact that existing methods suffer from poor generalization to diverse unseen examples. We claim that an anomaly detector equipped with a meta-learning scheme alleviates the limitation by leading the model to an initialization point for better optimization. We evaluate the performance of our framework on two challenging datasets, UCF-Crime and ShanghaiTech. The experimental results demonstrate that our algorithm boosts the capability to localize unseen abnormal events in a weakly supervised setting. Besides the technical contributions, we perform the annotation of missing labels in the UCF-Crime dataset and make our task evaluated effectively.", "paper_url": "http://arxiv.org/abs/2203.13610v1", "pdf_url": "http://arxiv.org/pdf/2203.13610v1", "repo_url": null}, "2203.13465": {"publish_time": "2022-03-25", "title": "CAD: Co-Adapting Discriminative Features for Improved Few-Shot Classification", "author": "Philip Chikontwe et.al.", "abstract": "Few-shot classification is a challenging problem that aims to learn a model that can adapt to unseen classes given a few labeled samples. Recent approaches pre-train a feature extractor, and then fine-tune for episodic meta-learning. Other methods leverage spatial features to learn pixel-level correspondence while jointly training a classifier. However, results using such approaches show marginal improvements. In this paper, inspired by the transformer style self-attention mechanism, we propose a strategy to cross-attend and re-weight discriminative features for few-shot classification. Given a base representation of support and query images after global pooling, we introduce a single shared module that projects features and cross-attends in two aspects: (i) query to support, and (ii) support to query. The module computes attention scores between features to produce an attention pooled representation of features in the same class that is later added to the original representation followed by a projection head. This effectively re-weights features in both aspects (i & ii) to produce features that better facilitate improved metric-based meta-learning. Extensive experiments on public benchmarks show our approach outperforms state-of-the-art methods by 3%~5%.", "paper_url": "http://arxiv.org/abs/2203.13465v1", "pdf_url": "http://arxiv.org/pdf/2203.13465v1", "repo_url": null}, "2203.14855": {"publish_time": "2022-03-28", "title": "Modular Adaptive Policy Selection for Multi-Task Imitation Learning through Task Division", "author": "Dafni Antotsiou et.al.", "abstract": "Deep imitation learning requires many expert demonstrations, which can be hard to obtain, especially when many tasks are involved. However, different tasks often share similarities, so learning them jointly can greatly benefit them and alleviate the need for many demonstrations. But, joint multi-task learning often suffers from negative transfer, sharing information that should be task-specific. In this work, we introduce a method to perform multi-task imitation while allowing for task-specific features. This is done by using proto-policies as modules to divide the tasks into simple sub-behaviours that can be shared. The proto-policies operate in parallel and are adaptively chosen by a selector mechanism that is jointly trained with the modules. Experiments on different sets of tasks show that our method improves upon the accuracy of single agents, task-conditioned and multi-headed multi-task agents, as well as state-of-the-art meta learning agents. We also demonstrate its ability to autonomously divide the tasks into both shared and task-specific sub-behaviours.", "paper_url": "http://arxiv.org/abs/2203.14855v1", "pdf_url": "http://arxiv.org/pdf/2203.14855v1", "repo_url": null}, "2203.14840": {"publish_time": "2022-03-28", "title": "A Framework of Meta Functional Learning for Regularising Knowledge Transfer", "author": "Pan Li et.al.", "abstract": "Machine learning classifiers' capability is largely dependent on the scale of available training data and limited by the model overfitting in data-scarce learning tasks. To address this problem, this work proposes a novel framework of Meta Functional Learning (MFL) by meta-learning a generalisable functional model from data-rich tasks whilst simultaneously regularising knowledge transfer to data-scarce tasks. The MFL computes meta-knowledge on functional regularisation generalisable to different learning tasks by which functional training on limited labelled data promotes more discriminative functions to be learned. Based on this framework, we formulate three variants of MFL: MFL with Prototypes (MFL-P) which learns a functional by auxiliary prototypes, Composite MFL (ComMFL) that transfers knowledge from both functional space and representational space, and MFL with Iterative Updates (MFL-IU) which improves knowledge transfer regularisation from MFL by progressively learning the functional regularisation in knowledge transfer. Moreover, we generalise these variants for knowledge transfer regularisation from binary classifiers to multi-class classifiers. Extensive experiments on two few-shot learning scenarios, Few-Shot Learning (FSL) and Cross-Domain Few-Shot Learning (CD-FSL), show that meta functional learning for knowledge transfer regularisation can improve FSL classifiers.", "paper_url": "http://arxiv.org/abs/2203.14840v1", "pdf_url": "http://arxiv.org/pdf/2203.14840v1", "repo_url": null}, "2203.14691": {"publish_time": "2022-03-28", "title": "Sketch3T: Test-Time Training for Zero-Shot SBIR", "author": "Aneeshan Sain et.al.", "abstract": "Zero-shot sketch-based image retrieval typically asks for a trained model to be applied as is to unseen categories. In this paper, we question to argue that this setup by definition is not compatible with the inherent abstract and subjective nature of sketches, i.e., the model might transfer well to new categories, but will not understand sketches existing in different test-time distribution as a result. We thus extend ZS-SBIR asking it to transfer to both categories and sketch distributions. Our key contribution is a test-time training paradigm that can adapt using just one sketch. Since there is no paired photo, we make use of a sketch raster-vector reconstruction module as a self-supervised auxiliary task. To maintain the fidelity of the trained cross-modal joint embedding during test-time update, we design a novel meta-learning based training paradigm to learn a separation between model updates incurred by this auxiliary task from those off the primary objective of discriminative learning. Extensive experiments show our model to outperform state of-the-arts, thanks to the proposed test-time adaption that not only transfers to new categories but also accommodates to new sketching styles.", "paper_url": "http://arxiv.org/abs/2203.14691v1", "pdf_url": "http://arxiv.org/pdf/2203.14691v1", "repo_url": null}, "2203.14607": {"publish_time": "2022-03-28", "title": "Boosting Black-Box Adversarial Attacks with Meta Learning", "author": "Junjie Fu et.al.", "abstract": "Deep neural networks (DNNs) have achieved remarkable success in diverse fields. However, it has been demonstrated that DNNs are very vulnerable to adversarial examples even in black-box settings. A large number of black-box attack methods have been proposed to in the literature. However, those methods usually suffer from low success rates and large query counts, which cannot fully satisfy practical purposes. In this paper, we propose a hybrid attack method which trains meta adversarial perturbations (MAPs) on surrogate models and performs black-box attacks by estimating gradients of the models. Our method uses the meta adversarial perturbation as an initialization and subsequently trains any black-box attack method for several epochs. Furthermore, the MAPs enjoy favorable transferability and universality, in the sense that they can be employed to boost performance of other black-box adversarial attack methods. Extensive experiments demonstrate that our method can not only improve the attack success rates, but also reduces the number of queries compared to other methods.", "paper_url": "http://arxiv.org/abs/2203.14607v1", "pdf_url": "http://arxiv.org/pdf/2203.14607v1", "repo_url": null}, "2203.14565": {"publish_time": "2022-03-28", "title": "Style-Guided Domain Adaptation for Face Presentation Attack Detection", "author": "Young-Eun Kim et.al.", "abstract": "Domain adaptation (DA) or domain generalization (DG) for face presentation attack detection (PAD) has attracted attention recently with its robustness against unseen attack scenarios. Existing DA/DG-based PAD methods, however, have not yet fully explored the domain-specific style information that can provide knowledge regarding attack styles (e.g., materials, background, illumination and resolution). In this paper, we introduce a novel Style-Guided Domain Adaptation (SGDA) framework for inference-time adaptive PAD. Specifically, Style-Selective Normalization (SSN) is proposed to explore the domain-specific style information within the high-order feature statistics. The proposed SSN enables the adaptation of the model to the target domain by reducing the style difference between the target and the source domains. Moreover, we carefully design Style-Aware Meta-Learning (SAML) to boost the adaptation ability, which simulates the inference-time adaptation with style selection process on virtual test domain. In contrast to previous domain adaptation approaches, our method does not require either additional auxiliary models (e.g., domain adaptors) or the unlabeled target domain during training, which makes our method more practical to PAD task. To verify our experiments, we utilize the public datasets: MSU-MFSD, CASIA-FASD, OULU-NPU and Idiap REPLAYATTACK. In most assessments, the result demonstrates a notable gap of performance compared to the conventional DA/DG-based PAD methods.", "paper_url": "http://arxiv.org/abs/2203.14565v1", "pdf_url": "http://arxiv.org/pdf/2203.14565v1", "repo_url": null}, "2203.15674": {"publish_time": "2022-03-29", "title": "Exploring Frequency Adversarial Attacks for Face Forgery Detection", "author": "Shuai Jia et.al.", "abstract": "Various facial manipulation techniques have drawn serious public concerns in morality, security, and privacy. Although existing face forgery classifiers achieve promising performance on detecting fake images, these methods are vulnerable to adversarial examples with injected imperceptible perturbations on the pixels. Meanwhile, many face forgery detectors always utilize the frequency diversity between real and fake faces as a crucial clue. In this paper, instead of injecting adversarial perturbations into the spatial domain, we propose a frequency adversarial attack method against face forgery detectors. Concretely, we apply discrete cosine transform (DCT) on the input images and introduce a fusion module to capture the salient region of adversary in the frequency domain. Compared with existing adversarial attacks (e.g. FGSM, PGD) in the spatial domain, our method is more imperceptible to human observers and does not degrade the visual quality of the original images. Moreover, inspired by the idea of meta-learning, we also propose a hybrid adversarial attack that performs attacks in both the spatial and frequency domains. Extensive experiments indicate that the proposed method fools not only the spatial-based detectors but also the state-of-the-art frequency-based detectors effectively. In addition, the proposed frequency attack enhances the transferability across face forgery detectors as black-box attacks.", "paper_url": "http://arxiv.org/abs/2203.15674v1", "pdf_url": "http://arxiv.org/pdf/2203.15674v1", "repo_url": null}, "2203.15297": {"publish_time": "2022-03-29", "title": "Kernel Modulation: A Parameter-Efficient Method for Training Convolutional Neural Networks", "author": "Yuhuang Hu et.al.", "abstract": "Deep Neural Networks, particularly Convolutional Neural Networks (ConvNets), have achieved incredible success in many vision tasks, but they usually require millions of parameters for good accuracy performance. With increasing applications that use ConvNets, updating hundreds of networks for multiple tasks on an embedded device can be costly in terms of memory, bandwidth, and energy. Approaches to reduce this cost include model compression and parameter-efficient models that adapt a subset of network layers for each new task. This work proposes a novel parameter-efficient kernel modulation (KM) method that adapts all parameters of a base network instead of a subset of layers. KM uses lightweight task-specialized kernel modulators that require only an additional 1.4% of the base network parameters. With multiple tasks, only the task-specialized KM weights are communicated and stored on the end-user device. We applied this method in training ConvNets for Transfer Learning and Meta-Learning scenarios. Our results show that KM delivers up to 9% higher accuracy than other parameter-efficient methods on the Transfer Learning benchmark.", "paper_url": "http://arxiv.org/abs/2203.15297v1", "pdf_url": "http://arxiv.org/pdf/2203.15297v1", "repo_url": null}, "2203.15972": {"publish_time": "2022-03-30", "title": "Higher-Order Generalization Bounds: Learning Deep Probabilistic Programs via PAC-Bayes Objectives", "author": "Jonathan Warrell et.al.", "abstract": "Deep Probabilistic Programming (DPP) allows powerful models based on recursive computation to be learned using efficient deep-learning optimization techniques. Additionally, DPP offers a unified perspective, where inference and learning algorithms are treated on a par with models as stochastic programs. Here, we offer a framework for representing and learning flexible PAC-Bayes bounds as stochastic programs using DPP-based methods. In particular, we show that DPP techniques may be leveraged to derive generalization bounds that draw on the compositionality of DPP representations. In turn, the bounds we introduce offer principled training objectives for higher-order probabilistic programs. We offer a definition of a higher-order generalization bound, which naturally encompasses single- and multi-task generalization perspectives (including transfer- and meta-learning) and a novel class of bound based on a learned measure of model complexity. Further, we show how modified forms of all higher-order bounds can be efficiently optimized as objectives for DPP training, using variational techniques. We test our framework using single- and multi-task generalization settings on synthetic and biological data, showing improved performance and generalization prediction using flexible DPP model representations and learned complexity measures.", "paper_url": "http://arxiv.org/abs/2203.15972v1", "pdf_url": "http://arxiv.org/pdf/2203.15972v1", "repo_url": null}, "2203.15936": {"publish_time": "2022-03-29", "title": "A Simple Yet Effective Pretraining Strategy for Graph Few-shot Learning", "author": "Zhen Tan et.al.", "abstract": "Recently, increasing attention has been devoted to the graph few-shot learning problem, where the target novel classes only contain a few labeled nodes. Among many existing endeavors, episodic meta-learning has become the most prevailing paradigm, and its episodic emulation of the test environment is believed to equip the graph neural network models with adaptability to novel node classes. However, in the image domain, recent results have shown that feature reuse is more likely to be the key of meta-learning to few-shot extrapolation. Based on such observation, in this work, we propose a simple transductive fine-tuning based framework as a new paradigm for graph few-shot learning. In the proposed paradigm, a graph encoder backbone is pretrained with base classes, and a simple linear classifier is fine-tuned by the few labeled samples and is tasked to classify the unlabeled ones. For pretraining, we propose a supervised contrastive learning framework with data augmentation strategies specific for few-shot node classification to improve the extrapolation of a GNN encoder. Finally, extensive experiments conducted on three benchmark datasets demonstrate the superior advantage of our framework over the state-of-the-art methods.", "paper_url": "http://arxiv.org/abs/2203.15936v1", "pdf_url": "http://arxiv.org/pdf/2203.15936v1", "repo_url": null}, "2203.16309": {"publish_time": "2022-03-29", "title": "Zero-shot meta-learning for small-scale data from human subjects", "author": "Julie Jiang et.al.", "abstract": "While developments in machine learning led to impressive performance gains on big data, many human subjects data are, in actuality, small and sparsely labeled. Existing methods applied to such data often do not easily generalize to out-of-sample subjects. Instead, models must make predictions on test data that may be drawn from a different distribution, a problem known as \\textit{zero-shot learning}. To address this challenge, we develop an end-to-end framework using a meta-learning approach, which enables the model to rapidly adapt to a new prediction task with limited training data for out-of-sample test data. We use three real-world small-scale human subjects datasets (two randomized control studies and one observational study), for which we predict treatment outcomes for held-out treatment groups. Our model learns the latent treatment effects of each intervention and, by design, can naturally handle multi-task predictions. We show that our model performs the best holistically for each held-out group and especially when the test group is distinctly different from the training group. Our model has implications for improved generalization of small-size human studies to the wider population.", "paper_url": "http://arxiv.org/abs/2203.16309v1", "pdf_url": "http://arxiv.org/pdf/2203.16309v1", "repo_url": null}, "2203.17218": {"publish_time": "2022-03-31", "title": "Improved Relation Networks for End-to-End Speaker Verification and Identification", "author": "Ashutosh Chaubey et.al.", "abstract": "Speaker identification systems in a real-world scenario are tasked to identify a speaker amongst a set of enrolled speakers given just a few samples for each enrolled speaker. This paper demonstrates the effectiveness of meta-learning and relation networks for this use case. We propose improved relation networks for speaker verification and few-shot (unseen) speaker identification. The use of relation networks facilitates joint training of the frontend speaker encoder and the backend model. Inspired by the use of prototypical networks in speaker verification and to increase the discriminability of the speaker embeddings, we train the model to classify samples in the current episode amongst all speakers present in the training set. Furthermore, we propose a new training regime for faster model convergence by extracting more information from a given meta-learning episode with negligible extra computation. We evaluate the proposed techniques on VoxCeleb, SITW and VCTK datasets on the tasks of speaker verification and unseen speaker identification. The proposed approach outperforms the existing approaches consistently on both tasks.", "paper_url": "http://arxiv.org/abs/2203.17218v1", "pdf_url": "http://arxiv.org/pdf/2203.17218v1", "repo_url": null}, "2203.17089": {"publish_time": "2022-03-31", "title": "Quantum-Aided Meta-Learning for Bayesian Binary Neural Networks via Born Machines", "author": "Ivana Nikoloska et.al.", "abstract": "Near-term noisy intermediate-scale quantum circuits can efficiently implement implicit probabilistic models in discrete spaces, supporting distributions that are practically infeasible to sample from using classical means. One of the possible applications of such models, also known as Born machines, is probabilistic inference, which is at the core of Bayesian methods. This paper studies the use of Born machines for the problem of training binary Bayesian neural networks. In the proposed approach, a Born machine is used to model the variational distribution of the binary weights of the neural network, and data from multiple tasks is used to reduce training data requirements on new tasks. The method combines gradient-based meta-learning and variational inference via Born machines, and is shown in a prototypical regression problem to outperform conventional joint learning strategies.", "paper_url": "http://arxiv.org/abs/2203.17089v1", "pdf_url": "http://arxiv.org/pdf/2203.17089v1", "repo_url": null}, "2203.17030": {"publish_time": "2022-03-31", "title": "Few-Shot Class-Incremental Learning by Sampling Multi-Phase Tasks", "author": "Da-Wei Zhou et.al.", "abstract": "New classes arise frequently in our ever-changing world, e.g., emerging topics in social media and new types of products in e-commerce. A model should recognize new classes and meanwhile maintain discriminability over old classes. Under severe circumstances, only limited novel instances are available to incrementally update the model. The task of recognizing few-shot new classes without forgetting old classes is called few-shot class-incremental learning (FSCIL). In this work, we propose a new paradigm for FSCIL based on meta-learning by LearnIng Multi-phase Incremental Tasks (LIMIT), which synthesizes fake FSCIL tasks from the base dataset. The data format of fake tasks is consistent with the `real' incremental tasks, and we can build a generalizable feature space for the unseen tasks through meta-learning. Besides, LIMIT also constructs a calibration module based on transformer, which calibrates the old class classifiers and new class prototypes into the same scale and fills in the semantic gap. The calibration module also adaptively contextualizes the instance-specific embedding with a set-to-set function. LIMIT efficiently adapts to new classes and meanwhile resists forgetting over old classes. Experiments on three benchmark datasets (CIFAR100, miniImageNet, and CUB200) and large-scale dataset, i.e., ImageNet ILSVRC2012 validate that LIMIT achieves state-of-the-art performance.", "paper_url": "http://arxiv.org/abs/2203.17030v1", "pdf_url": "http://arxiv.org/pdf/2203.17030v1", "repo_url": null}, "2203.16639": {"publish_time": "2022-03-30", "title": "FALCON: Fast Visual Concept Learning by Integrating Images, Linguistic descriptions, and Conceptual Relations", "author": "Lingjie Mei et.al.", "abstract": "We present a meta-learning framework for learning new visual concepts quickly, from just one or a few examples, guided by multiple naturally occurring data streams: simultaneously looking at images, reading sentences that describe the objects in the scene, and interpreting supplemental sentences that relate the novel concept with other concepts. The learned concepts support downstream applications, such as answering questions by reasoning about unseen images. Our model, namely FALCON, represents individual visual concepts, such as colors and shapes, as axis-aligned boxes in a high-dimensional space (the \"box embedding space\"). Given an input image and its paired sentence, our model first resolves the referential expression in the sentence and associates the novel concept with particular objects in the scene. Next, our model interprets supplemental sentences to relate the novel concept with other known concepts, such as \"X has property Y\" or \"X is a kind of Y\". Finally, it infers an optimal box embedding for the novel concept that jointly 1) maximizes the likelihood of the observed instances in the image, and 2) satisfies the relationships between the novel concepts and the known ones. We demonstrate the effectiveness of our model on both synthetic and real-world datasets.", "paper_url": "http://arxiv.org/abs/2203.16639v1", "pdf_url": "http://arxiv.org/pdf/2203.16639v1", "repo_url": null}, "2203.16588": {"publish_time": "2022-03-30", "title": "Constrained Few-shot Class-incremental Learning", "author": "Michael Hersche et.al.", "abstract": "Continually learning new classes from fresh data without forgetting previous knowledge of old classes is a very challenging research problem. Moreover, it is imperative that such learning must respect certain memory and computational constraints such as (i) training samples are limited to only a few per class, (ii) the computational cost of learning a novel class remains constant, and (iii) the memory footprint of the model grows at most linearly with the number of classes observed. To meet the above constraints, we propose C-FSCIL, which is architecturally composed of a frozen meta-learned feature extractor, a trainable fixed-size fully connected layer, and a rewritable dynamically growing memory that stores as many vectors as the number of encountered classes. C-FSCIL provides three update modes that offer a trade-off between accuracy and compute-memory cost of learning novel classes. C-FSCIL exploits hyperdimensional embedding that allows to continually express many more classes than the fixed dimensions in the vector space, with minimal interference. The quality of class vector representations is further improved by aligning them quasi-orthogonally to each other by means of novel loss functions. Experiments on the CIFAR100, miniImageNet, and Omniglot datasets show that C-FSCIL outperforms the baselines with remarkable accuracy and compression. It also scales up to the largest problem size ever tried in this few-shot setting by learning 423 novel classes on top of 1200 base classes with less than 1.6% accuracy drop. Our code is available at https://github.com/IBM/constrained-FSCIL.", "paper_url": "http://arxiv.org/abs/2203.16588v1", "pdf_url": "http://arxiv.org/pdf/2203.16588v1", "repo_url": "https://github.com/ibm/constrained-fscil"}, "2204.00352": {"publish_time": "2022-04-01", "title": "On the Efficiency of Integrating Self-supervised Learning and Meta-learning for User-defined Few-shot Keyword Spotting", "author": "Wei-Tsung Kao et.al.", "abstract": "User-defined keyword spotting is a task to detect new spoken terms defined by users. This can be viewed as a few-shot learning problem since it is unreasonable for users to define their desired keywords by providing many examples. To solve this problem, previous works try to incorporate self-supervised learning models or apply meta-learning algorithms. But it is unclear whether self-supervised learning and meta-learning are complementary and which combination of the two types of approaches is most effective for few-shot keyword discovery. In this work, we systematically study these questions by utilizing various self-supervised learning models and combining them with a wide variety of meta-learning algorithms. Our result shows that HuBERT combined with Matching network achieves the best result and is robust to the changes of few-shot examples.", "paper_url": "http://arxiv.org/abs/2204.00352v1", "pdf_url": "http://arxiv.org/pdf/2204.00352v1", "repo_url": null}, "2204.00327": {"publish_time": "2022-04-01", "title": "Diverse Preference Augmentation with Multiple Domains for Cold-start Recommendations", "author": "Yan Zhang et.al.", "abstract": "Cold-start issues have been more and more challenging for providing accurate recommendations with the fast increase of users and items. Most existing approaches attempt to solve the intractable problems via content-aware recommendations based on auxiliary information and/or cross-domain recommendations with transfer learning. Their performances are often constrained by the extremely sparse user-item interactions, unavailable side information, or very limited domain-shared users. Recently, meta-learners with meta-augmentation by adding noises to labels have been proven to be effective to avoid overfitting and shown good performance on new tasks. Motivated by the idea of meta-augmentation, in this paper, by treating a user's preference over items as a task, we propose a so-called Diverse Preference Augmentation framework with multiple source domains based on meta-learning (referred to as MetaDPA) to i) generate diverse ratings in a new domain of interest (known as target domain) to handle overfitting on the case of sparse interactions, and to ii) learn a preference model in the target domain via a meta-learning scheme to alleviate cold-start issues. Specifically, we first conduct multi-source domain adaptation by dual conditional variational autoencoders and impose a Multi-domain InfoMax (MDI) constraint on the latent representations to learn domain-shared and domain-specific preference properties. To avoid overfitting, we add a Mutually-Exclusive (ME) constraint on the output of decoders to generate diverse ratings given content data. Finally, these generated diverse ratings and the original ratings are introduced into the meta-training procedure to learn a preference meta-learner, which produces good generalization ability on cold-start recommendation tasks. Experiments on real-world datasets show our proposed MetaDPA clearly outperforms the current state-of-the-art baselines.", "paper_url": "http://arxiv.org/abs/2204.00327v1", "pdf_url": "http://arxiv.org/pdf/2204.00327v1", "repo_url": null}, "2204.01513": {"publish_time": "2022-04-04", "title": "Context-aware Visual Tracking with Joint Meta-updating", "author": "Qiuhong Shen et.al.", "abstract": "Visual object tracking acts as a pivotal component in various emerging video applications. Despite the numerous developments in visual tracking, existing deep trackers are still likely to fail when tracking against objects with dramatic variation. These deep trackers usually do not perform online update or update single sub-branch of the tracking model, for which they cannot adapt to the appearance variation of objects. Efficient updating methods are therefore crucial for tracking while previous meta-updater optimizes trackers directly over parameter space, which is prone to over-fit even collapse on longer sequences. To address these issues, we propose a context-aware tracking model to optimize the tracker over the representation space, which jointly meta-update both branches by exploiting information along the whole sequence, such that it can avoid the over-fitting problem. First, we note that the embedded features of the localization branch and the box-estimation branch, focusing on the local and global information of the target, are effective complements to each other. Based on this insight, we devise a context-aggregation module to fuse information in historical frames, followed by a context-aware module to learn affinity vectors for both branches of the tracker. Besides, we develop a dedicated meta-learning scheme, on account of fast and stable updating with limited training samples. The proposed tracking method achieves an EAO score of 0.514 on VOT2018 with the speed of 40FPS, demonstrating its capability of improving the accuracy and robustness of the underlying tracker with little speed drop.", "paper_url": "http://arxiv.org/abs/2204.01513v1", "pdf_url": "http://arxiv.org/pdf/2204.01513v1", "repo_url": null}, "2204.01437": {"publish_time": "2022-04-04", "title": "Disentangling Abstraction from Statistical Pattern Matching in Human and Machine Learning", "author": "Sreejan Kumar et.al.", "abstract": "The ability to acquire abstract knowledge is a hallmark of human intelligence and is believed by many to be one of the core differences between humans and neural network models. Agents can be endowed with an inductive bias towards abstraction through meta-learning, where they are trained on a distribution of tasks that share some abstract structure that can be learned and applied. However, because neural networks are hard to interpret, it can be difficult to tell whether agents have learned the underlying abstraction, or alternatively statistical patterns that are characteristic of that abstraction. In this work, we compare the performance of humans and agents in a meta-reinforcement learning paradigm in which tasks are generated from abstract rules. We define a novel methodology for building \"task metamers\" that closely match the statistics of the abstract tasks but use a different underlying generative process, and evaluate performance on both abstract and metamer tasks. In our first set of experiments, we found that humans perform better at abstract tasks than metamer tasks whereas a widely-used meta-reinforcement learning agent performs worse on the abstract tasks than the matched metamers. In a second set of experiments, we base the tasks on abstractions derived directly from empirically identified human priors. We utilize the same procedure to generate corresponding metamer tasks, and see the same double dissociation between humans and agents. This work provides a foundation for characterizing differences between humans and machine learning that can be used in future work towards developing machines with human-like behavior.", "paper_url": "http://arxiv.org/abs/2204.01437v1", "pdf_url": "http://arxiv.org/pdf/2204.01437v1", "repo_url": "https://github.com/sreejank/Abstract_Neural_Metamers"}, "2204.00970": {"publish_time": "2022-04-03", "title": "A Dynamic Meta-Learning Model for Time-Sensitive Cold-Start Recommendations", "author": "Krishna Prasad Neupane et.al.", "abstract": "We present a novel dynamic recommendation model that focuses on users who have interactions in the past but turn relatively inactive recently. Making effective recommendations to these time-sensitive cold-start users is critical to maintain the user base of a recommender system. Due to the sparse recent interactions, it is challenging to capture these users' current preferences precisely. Solely relying on their historical interactions may also lead to outdated recommendations misaligned with their recent interests. The proposed model leverages historical and current user-item interactions and dynamically factorizes a user's (latent) preference into time-specific and time-evolving representations that jointly affect user behaviors. These latent factors further interact with an optimized item embedding to achieve accurate and timely recommendations. Experiments over real-world data help demonstrate the effectiveness of the proposed time-sensitive cold-start recommendation model.", "paper_url": "http://arxiv.org/abs/2204.00970v1", "pdf_url": "http://arxiv.org/pdf/2204.00970v1", "repo_url": null}, "2204.00929": {"publish_time": "2022-04-02", "title": "AutoProtoNet: Interpretability for Prototypical Networks", "author": "Pedro Sandoval-Segura et.al.", "abstract": "In meta-learning approaches, it is difficult for a practitioner to make sense of what kind of representations the model employs. Without this ability, it can be difficult to both understand what the model knows as well as to make meaningful corrections. To address these challenges, we introduce AutoProtoNet, which builds interpretability into Prototypical Networks by training an embedding space suitable for reconstructing inputs, while remaining convenient for few-shot learning. We demonstrate how points in this embedding space can be visualized and used to understand class representations. We also devise a prototype refinement method, which allows a human to debug inadequate classification parameters. We use this debugging technique on a custom classification task and find that it leads to accuracy improvements on a validation set consisting of in-the-wild images. We advocate for interpretability in meta-learning approaches and show that there are interactive ways for a human to enhance meta-learning algorithms.", "paper_url": "http://arxiv.org/abs/2204.00929v1", "pdf_url": "http://arxiv.org/pdf/2204.00929v1", "repo_url": null}, "2204.02210": {"publish_time": "2022-04-05", "title": "Model Based Meta Learning of Critics for Policy Gradients", "author": "Sarah Bechtle et.al.", "abstract": "Being able to seamlessly generalize across different tasks is fundamental for robots to act in our world. However, learning representations that generalize quickly to new scenarios is still an open research problem in reinforcement learning. In this paper we present a framework to meta-learn the critic for gradient-based policy learning. Concretely, we propose a model-based bi-level optimization algorithm that updates the critics parameters such that the policy that is learned with the updated critic gets closer to solving the meta-training tasks. We illustrate that our algorithm leads to learned critics that resemble the ground truth Q function for a given task. Finally, after meta-training, the learned critic can be used to learn new policies for new unseen task and environment settings via model-free policy gradient optimization, without requiring a model. We present results that show the generalization capabilities of our learned critic to new tasks and dynamics when used to learn a new policy in a new scenario.", "paper_url": "http://arxiv.org/abs/2204.02210v1", "pdf_url": "http://arxiv.org/pdf/2204.02210v1", "repo_url": null}, "2204.02121": {"publish_time": "2022-04-05", "title": "MetaAudio: A Few-Shot Audio Classification Benchmark", "author": "Calum Heggan et.al.", "abstract": "Currently available benchmarks for few-shot learning (machine learning with few training examples) are limited in the domains they cover, primarily focusing on image classification. This work aims to alleviate this reliance on image-based benchmarks by offering the first comprehensive, public and fully reproducible audio based alternative, covering a variety of sound domains and experimental settings. We compare the few-shot classification performance of a variety of techniques on seven audio datasets (spanning environmental sounds to human-speech). Extending this, we carry out in-depth analyses of joint training (where all datasets are used during training) and cross-dataset adaptation protocols, establishing the possibility of a generalised audio few-shot classification algorithm. Our experimentation shows gradient-based meta-learning methods such as MAML and Meta-Curvature consistently outperform both metric and baseline methods. We also demonstrate that the joint training routine helps overall generalisation for the environmental sound databases included, as well as being a somewhat-effective method of tackling the cross-dataset/domain setting.", "paper_url": "http://arxiv.org/abs/2204.02121v1", "pdf_url": "http://arxiv.org/pdf/2204.02121v1", "repo_url": "https://github.com/cheggan/metaaudio-a-few-shot-audio-classification-benchmark"}, "2204.01905": {"publish_time": "2022-04-05", "title": "Learning to Adapt to Domain Shifts with Few-shot Samples in Anomalous Sound Detection", "author": "Bingqing Chen et.al.", "abstract": "Anomaly detection has many important applications, such as monitoring industrial equipment. Despite recent advances in anomaly detection with deep-learning methods, it is unclear how existing solutions would perform under out-of-distribution scenarios, e.g., due to shifts in machine load or environmental noise. Grounded in the application of machine health monitoring, we propose a framework that adapts to new conditions with few-shot samples. Building upon prior work, we adopt a classification-based approach for anomaly detection and show its equivalence to mixture density estimation of the normal samples. We incorporate an episodic training procedure to match the few-shot setting during inference. We define multiple auxiliary classification tasks based on meta-information and leverage gradient-based meta-learning to improve generalization to different shifts. We evaluate our proposed method on a recently-released dataset of audio measurements from different machine types. It improved upon two baselines by around 10% and is on par with best-performing model reported on the dataset.", "paper_url": "http://arxiv.org/abs/2204.01905v1", "pdf_url": "http://arxiv.org/pdf/2204.01905v1", "repo_url": null}, "2204.03609": {"publish_time": "2022-04-07", "title": "Pin the Memory: Learning to Generalize Semantic Segmentation", "author": "Jin Kim et.al.", "abstract": "The rise of deep neural networks has led to several breakthroughs for semantic segmentation. In spite of this, a model trained on source domain often fails to work properly in new challenging domains, that is directly concerned with the generalization capability of the model. In this paper, we present a novel memory-guided domain generalization method for semantic segmentation based on meta-learning framework. Especially, our method abstracts the conceptual knowledge of semantic classes into categorical memory which is constant beyond the domains. Upon the meta-learning concept, we repeatedly train memory-guided networks and simulate virtual test to 1) learn how to memorize a domain-agnostic and distinct information of classes and 2) offer an externally settled memory as a class-guidance to reduce the ambiguity of representation in the test data of arbitrary unseen domain. To this end, we also propose memory divergence and feature cohesion losses, which encourage to learn memory reading and update processes for category-aware domain generalization. Extensive experiments for semantic segmentation demonstrate the superior generalization capability of our method over state-of-the-art works on various benchmarks.", "paper_url": "http://arxiv.org/abs/2204.03609v1", "pdf_url": "http://arxiv.org/pdf/2204.03609v1", "repo_url": null}, "2204.03511": {"publish_time": "2022-04-07", "title": "Interval Bound Propagation$\\unicode{x2013}$aided Few$\\unicode{x002d}$shot Learning", "author": "Shounak Datta et.al.", "abstract": "Few-shot learning aims to transfer the knowledge acquired from training on a diverse set of tasks, from a given task distribution, to generalize to unseen tasks, from the same distribution, with a limited amount of labeled data. The underlying requirement for effective few-shot generalization is to learn a good representation of the task manifold. One way to encourage this is to preserve local neighborhoods in the feature space learned by the few-shot learner. To this end, we introduce the notion of interval bounds from the provably robust training literature to few-shot learning. The interval bounds are used to characterize neighborhoods around the training tasks. These neighborhoods can then be preserved by minimizing the distance between a task and its respective bounds. We further introduce a novel strategy to artificially form new tasks for training by interpolating between the available tasks and their respective interval bounds, to aid in cases with a scarcity of tasks. We apply our framework to both model-agnostic meta-learning as well as prototype-based metric-learning paradigms. The efficacy of our proposed approach is evident from the improved performance on several datasets from diverse domains in comparison to a sizable number of recent competitors.", "paper_url": "http://arxiv.org/abs/2204.03511v2", "pdf_url": "http://arxiv.org/pdf/2204.03511v2", "repo_url": null}, "2204.04952": {"publish_time": "2022-04-11", "title": "MGIMN: Multi-Grained Interactive Matching Network for Few-shot Text Classification", "author": "Jianhai Zhang et.al.", "abstract": "Text classification struggles to generalize to unseen classes with very few labeled text instances per class. In such a few-shot learning (FSL) setting, metric-based meta-learning approaches have shown promising results. Previous studies mainly aim to derive a prototype representation for each class. However, they neglect that it is challenging-yet-unnecessary to construct a compact representation which expresses the entire meaning for each class. They also ignore the importance to capture the inter-dependency between query and the support set for few-shot text classification. To deal with these issues, we propose a meta-learning based method MGIMN which performs instance-wise comparison followed by aggregation to generate class-wise matching vectors instead of prototype learning. The key of instance-wise comparison is the interactive matching within the class-specific context and episode-specific context. Extensive experiments demonstrate that the proposed method significantly outperforms the existing state-of-the-art approaches, under both the standard FSL and generalized FSL settings.", "paper_url": "http://arxiv.org/abs/2204.04952v1", "pdf_url": "http://arxiv.org/pdf/2204.04952v1", "repo_url": null}, "2204.04348": {"publish_time": "2022-04-09", "title": "Neural networks embrace learned diversity", "author": "Anshul Choudhary et.al.", "abstract": "Diversity conveys advantages in nature, yet homogeneous neurons typically comprise the layers of artificial neural networks. Here we construct neural networks from neurons that learn their own activation functions, quickly diversify, and subsequently outperform their homogeneous counterparts. Sub-networks instantiate the neurons, which meta-learn especially efficient sets of nonlinear responses. Such learned diversity provides examples of dynamical systems selecting diversity over uniformity and elucidates the role of diversity in natural and artificial systems.", "paper_url": "http://arxiv.org/abs/2204.04348v1", "pdf_url": "http://arxiv.org/pdf/2204.04348v1", "repo_url": "https://github.com/nonlinearartificialintelligencelab/diversityNN"}, "2204.04297": {"publish_time": "2022-04-08", "title": "Learning to modulate random weights can induce task-specific contexts for economical meta and continual learning", "author": "Jinyung Hong et.al.", "abstract": "Neural networks are vulnerable to catastrophic forgetting when data distributions are non-stationary during continual online learning; learning of a later task often leads to forgetting of an earlier task. One solution approach is model-agnostic continual meta-learning, whereby both task-specific and meta parameters are trained. Here, we depart from this view and introduce a novel neural-network architecture inspired by neuromodulation in biological nervous systems. Neuromodulation is the biological mechanism that dynamically controls and fine-tunes synaptic dynamics to complement the behavioral context in real-time, which has received limited attention in machine learning. We introduce a single-hidden-layer network that learns only a relatively small context vector per task (task-specific parameters) that neuromodulates unchanging, randomized weights (meta parameters) that transform the input. We show that when task boundaries are available, this approach can eliminate catastrophic forgetting entirely while also drastically reducing the number of learnable parameters relative to other context-vector-based approaches. Furthermore, by combining this model with a simple meta-learning approach for inferring task identity, we demonstrate that the model can be generalized into a framework to perform continual learning without knowledge of task boundaries. Finally, we showcase the framework in a supervised continual online learning scenario and discuss the implications of the proposed formalism.", "paper_url": "http://arxiv.org/abs/2204.04297v1", "pdf_url": "http://arxiv.org/pdf/2204.04297v1", "repo_url": null}, "2204.05751": {"publish_time": "2022-04-12", "title": "Decomposed Meta-Learning for Few-Shot Named Entity Recognition", "author": "Tingting Ma et.al.", "abstract": "Few-shot named entity recognition (NER) systems aim at recognizing novel-class named entities based on only a few labeled examples. In this paper, we present a decomposed meta-learning approach which addresses the problem of few-shot NER by sequentially tackling few-shot span detection and few-shot entity typing using meta-learning. In particular, we take the few-shot span detection as a sequence labeling problem and train the span detector by introducing the model-agnostic meta-learning (MAML) algorithm to find a good model parameter initialization that could fast adapt to new entity classes. For few-shot entity typing, we propose MAML-ProtoNet, i.e., MAML-enhanced prototypical networks to find a good embedding space that can better distinguish text span representations from different entity classes. Extensive experiments on various benchmarks show that our approach achieves superior performance over prior methods.", "paper_url": "http://arxiv.org/abs/2204.05751v1", "pdf_url": "http://arxiv.org/pdf/2204.05751v1", "repo_url": "https://github.com/microsoft/vert-papers"}, "2204.05547": {"publish_time": "2022-04-12", "title": "DistPro: Searching A Fast Knowledge Distillation Process via Meta Optimization", "author": "Xueqing Deng et.al.", "abstract": "Recent Knowledge distillation (KD) studies show that different manually designed schemes impact the learned results significantly. Yet, in KD, automatically searching an optimal distillation scheme has not yet been well explored. In this paper, we propose DistPro, a novel framework which searches for an optimal KD process via differentiable meta-learning. Specifically, given a pair of student and teacher networks, DistPro first sets up a rich set of KD connection from the transmitting layers of the teacher to the receiving layers of the student, and in the meanwhile, various transforms are also proposed for comparing feature maps along its pathway for the distillation. Then, each combination of a connection and a transform choice (pathway) is associated with a stochastic weighting process which indicates its importance at every step during the distillation. In the searching stage, the process can be effectively learned through our proposed bi-level meta-optimization strategy. In the distillation stage, DistPro adopts the learned processes for knowledge distillation, which significantly improves the student accuracy especially when faster training is required. Lastly, we find the learned processes can be generalized between similar tasks and networks. In our experiments, DistPro produces state-of-the-art (SoTA) accuracy under varying number of learning epochs on popular datasets, i.e. CIFAR100 and ImageNet, which demonstrate the effectiveness of our framework.", "paper_url": "http://arxiv.org/abs/2204.05547v1", "pdf_url": "http://arxiv.org/pdf/2204.05547v1", "repo_url": null}, "2204.05449": {"publish_time": "2022-04-11", "title": "Neural Processes with Stochastic Attention: Paying more attention to the context dataset", "author": "Mingyu Kim et.al.", "abstract": "Neural processes (NPs) aim to stochastically complete unseen data points based on a given context dataset. NPs essentially leverage a given dataset as a context representation to derive a suitable identifier for a novel task. To improve the prediction accuracy, many variants of NPs have investigated context embedding approaches that generally design novel network architectures and aggregation functions satisfying permutation invariant. In this work, we propose a stochastic attention mechanism for NPs to capture appropriate context information. From the perspective of information theory, we demonstrate that the proposed method encourages context embedding to be differentiated from a target dataset, allowing NPs to consider features in a target dataset and context embedding independently. We observe that the proposed method can appropriately capture context embedding even under noisy data sets and restricted task distributions, where typical NPs suffer from a lack of context embeddings. We empirically show that our approach substantially outperforms conventional NPs in various domains through 1D regression, predator-prey model, and image completion. Moreover, the proposed method is also validated by MovieLens-10k dataset, a real-world problem.", "paper_url": "http://arxiv.org/abs/2204.05449v1", "pdf_url": "http://arxiv.org/pdf/2204.05449v1", "repo_url": null}, "2204.05432": {"publish_time": "2022-04-11", "title": "A Simple Approach to Adversarial Robustness in Few-shot Image Classification", "author": "Akshayvarun Subramanya et.al.", "abstract": "Few-shot image classification, where the goal is to generalize to tasks with limited labeled data, has seen great progress over the years. However, the classifiers are vulnerable to adversarial examples, posing a question regarding their generalization capabilities. Recent works have tried to combine meta-learning approaches with adversarial training to improve the robustness of few-shot classifiers. We show that a simple transfer-learning based approach can be used to train adversarially robust few-shot classifiers. We also present a method for novel classification task based on calibrating the centroid of the few-shot category towards the base classes. We show that standard adversarial training on base categories along with calibrated centroid-based classifier in the novel categories, outperforms or is on-par with state-of-the-art advanced methods on standard benchmarks for few-shot learning. Our method is simple, easy to scale, and with little effort can lead to robust few-shot classifiers. Code is available here: \\url{https://github.com/UCDvision/Simple_few_shot.git}", "paper_url": "http://arxiv.org/abs/2204.05432v1", "pdf_url": "http://arxiv.org/pdf/2204.05432v1", "repo_url": null}}, "Transfer Learning": {"2202.12814": {"publish_time": "2022-02-25", "title": "The Reality of Multi-Lingual Machine Translation", "author": "Tom Kocmi et.al.", "abstract": "Our book \"The Reality of Multi-Lingual Machine Translation\" discusses the benefits and perils of using more than two languages in machine translation systems. While focused on the particular task of sequence-to-sequence processing and multi-task learning, the book targets somewhat beyond the area of natural language processing. Machine translation is for us a prime example of deep learning applications where human skills and learning capabilities are taken as a benchmark that many try to match and surpass. We document that some of the gains observed in multi-lingual translation may result from simpler effects than the assumed cross-lingual transfer of knowledge.   In the first, rather general part, the book will lead you through the motivation for multi-linguality, the versatility of deep neural networks especially in sequence-to-sequence tasks to complications of this learning. We conclude the general part with warnings against too optimistic and unjustified explanations of the gains that neural networks demonstrate.   In the second part, we fully delve into multi-lingual models, with a particularly careful examination of transfer learning as one of the more straightforward approaches utilizing additional languages. The recent multi-lingual techniques, including massive models, are surveyed and practical aspects of deploying systems for many languages are discussed. The conclusion highlights the open problem of machine understanding and reminds of two ethical aspects of building large-scale models: the inclusivity of research and its ecological trace.", "paper_url": "http://arxiv.org/abs/2202.12814v1", "pdf_url": "http://arxiv.org/pdf/2202.12814v1", "repo_url": null}, "2202.12576": {"publish_time": "2022-02-25", "title": "A Survey of Multilingual Models for Automatic Speech Recognition", "author": "Hemant Yadav et.al.", "abstract": "Although Automatic Speech Recognition (ASR) systems have achieved human-like performance for a few languages, the majority of the world's languages do not have usable systems due to the lack of large speech datasets to train these models. Cross-lingual transfer is an attractive solution to this problem, because low-resource languages can potentially benefit from higher-resource languages either through transfer learning, or being jointly trained in the same multilingual model. The problem of cross-lingual transfer has been well studied in ASR, however, recent advances in Self Supervised Learning are opening up avenues for unlabeled speech data to be used in multilingual ASR models, which can pave the way for improved performance on low-resource languages. In this paper, we survey the state of the art in multilingual ASR models that are built with cross-lingual transfer in mind. We present best practices for building multilingual models from research across diverse languages and techniques, discuss open questions and provide recommendations for future work.", "paper_url": "http://arxiv.org/abs/2202.12576v1", "pdf_url": "http://arxiv.org/pdf/2202.12576v1", "repo_url": null}, "2202.12505": {"publish_time": "2022-02-25", "title": "A Deep Learning Approach for Network-wide Dynamic Traffic Prediction during Hurricane Evacuation", "author": "Rezaur Rahman et.al.", "abstract": "Proactive evacuation traffic management largely depends on real-time monitoring and prediction of traffic flow at a high spatiotemporal resolution. However, evacuation traffic prediction is challenging due to the uncertainties caused by sudden changes in projected hurricane paths and consequently household evacuation behavior. Moreover, modeling spatiotemporal traffic flow patterns requires extensive data over a longer time period, whereas evacuations typically last for 2 to 5 days. In this paper, we present a novel data-driven approach for predicting evacuation traffic at a network scale. We develop a dynamic graph convolution LSTM (DGCN-LSTM) model to learn the network dynamics of hurricane evacuation. We first train the model for non-evacuation period traffic data showing that the model outperforms existing deep learning models for predicting non-evacuation period traffic with an RMSE value of 226.84. However, when we apply the model for evacuation period, the RMSE value increased to 1440.99. We overcome this issue by adopting a transfer learning approach with additional features related to evacuation traffic demand such as distance from the evacuation zone, time to landfall, and other zonal level features to control the transfer of information (network dynamics) from non-evacuation periods to evacuation periods. The final transfer learned DGCN-LSTM model performs well to predict evacuation traffic flow (RMSE=399.69). The implemented model can be applied to predict evacuation traffic over a longer forecasting horizon (6 hour). It will assist transportation agencies to activate appropriate traffic management strategies to reduce delays for evacuating traffic.", "paper_url": "http://arxiv.org/abs/2202.12505v1", "pdf_url": "http://arxiv.org/pdf/2202.12505v1", "repo_url": null}, "2202.12174": {"publish_time": "2022-02-24", "title": "Collaborative Training of Heterogeneous Reinforcement Learning Agents in Environments with Sparse Rewards: What and When to Share?", "author": "Alain Andres et.al.", "abstract": "In the early stages of human life, babies develop their skills by exploring different scenarios motivated by their inherent satisfaction rather than by extrinsic rewards from the environment. This behavior, referred to as intrinsic motivation, has emerged as one solution to address the exploration challenge derived from reinforcement learning environments with sparse rewards. Diverse exploration approaches have been proposed to accelerate the learning process over single- and multi-agent problems with homogeneous agents. However, scarce studies have elaborated on collaborative learning frameworks between heterogeneous agents deployed into the same environment, but interacting with different instances of the latter without any prior knowledge. Beyond the heterogeneity, each agent's characteristics grant access only to a subset of the full state space, which may hide different exploration strategies and optimal solutions. In this work we combine ideas from intrinsic motivation and transfer learning. Specifically, we focus on sharing parameters in actor-critic model architectures and on combining information obtained through intrinsic motivation with the aim of having a more efficient exploration and faster learning. We test our strategies through experiments performed over a modified ViZDooM's My Way Home scenario, which is more challenging than its original version and allows evaluating the heterogeneity between agents. Our results reveal different ways in which a collaborative framework with little additional computational cost can outperform an independent learning process without knowledge sharing. Additionally, we depict the need for modulating correctly the importance between the extrinsic and intrinsic rewards to avoid undesired agent behaviors.", "paper_url": "http://arxiv.org/abs/2202.12174v1", "pdf_url": "http://arxiv.org/pdf/2202.12174v1", "repo_url": null}, "2202.11685": {"publish_time": "2022-02-23", "title": "A Class of Geometric Structures in Transfer Learning: Minimax Bounds and Optimality", "author": "Xuhui Zhang et.al.", "abstract": "We study the problem of transfer learning, observing that previous efforts to understand its information-theoretic limits do not fully exploit the geometric structure of the source and target domains. In contrast, our study first illustrates the benefits of incorporating a natural geometric structure within a linear regression model, which corresponds to the generalized eigenvalue problem formed by the Gram matrices of both domains. We next establish a finite-sample minimax lower bound, propose a refined model interpolation estimator that enjoys a matching upper bound, and then extend our framework to multiple source domains and generalized linear models. Surprisingly, as long as information is available on the distance between the source and target parameters, negative-transfer does not occur. Simulation studies show that our proposed interpolation estimator outperforms state-of-the-art transfer learning methods in both moderate- and high-dimensional settings.", "paper_url": "http://arxiv.org/abs/2202.11685v1", "pdf_url": "http://arxiv.org/pdf/2202.11685v1", "repo_url": null}, "2202.13626": {"publish_time": "2022-02-28", "title": "Improving Response Time of Home IoT Services in Federated Learning", "author": "Dongjun Hwang et.al.", "abstract": "For intelligent home IoT services with sensors and machine learning, we need to upload IoT data to the cloud server which cannot share private data for training. A recent machine learning approach, called federated learning, keeps user data on the device in the distributed computing environment. Though federated learning is useful for protecting privacy, it experiences poor performance in terms of the end-to-end response time in home IoT services, because IoT devices are usually controlled by remote servers in the cloud. In addition, it is difficult to achieve the high accuracy of federated learning models due to insufficient data problems and model inversion attacks. In this paper, we propose a local IoT control method for a federated learning home service that recognizes the user behavior in the home network quickly and accurately. We present a federated learning client with transfer learning and differential privacy to solve data scarcity and data model inversion attack problems. From experiments, we show that the local control of home IoT devices for user authentication and control message transmission by the federated learning clients improves the response time to less than 1 second. Moreover, we demonstrate that federated learning with transfer learning achieves 97% of accuracy under 9,000 samples, which is only 2% of the difference from centralized learning.", "paper_url": "http://arxiv.org/abs/2202.13626v1", "pdf_url": "http://arxiv.org/pdf/2202.13626v1", "repo_url": "https://github.com/hwangdongjun/federated_learning_using_websockets"}, "2202.13403": {"publish_time": "2022-02-27", "title": "A Multimodal German Dataset for Automatic Lip Reading Systems and Transfer Learning", "author": "Gerald Schwiebert et.al.", "abstract": "Large datasets as required for deep learning of lip reading do not exist in many languages. In this paper we present the dataset GLips (German Lips) consisting of 250,000 publicly available videos of the faces of speakers of the Hessian Parliament, which was processed for word-level lip reading using an automatic pipeline. The format is similar to that of the English language LRW (Lip Reading in the Wild) dataset, with each video encoding one word of interest in a context of 1.16 seconds duration, which yields compatibility for studying transfer learning between both datasets. By training a deep neural network, we investigate whether lip reading has language-independent features, so that datasets of different languages can be used to improve lip reading models. We demonstrate learning from scratch and show that transfer learning from LRW to GLips and vice versa improves learning speed and performance, in particular for the validation set.", "paper_url": "http://arxiv.org/abs/2202.13403v1", "pdf_url": "http://arxiv.org/pdf/2202.13403v1", "repo_url": null}, "2202.13174": {"publish_time": "2022-02-26", "title": "BioADAPT-MRC: Adversarial Learning-based Domain Adaptation Improves Biomedical Machine Reading Comprehension Task", "author": "Maria Mahbub et.al.", "abstract": "Motivation: Biomedical machine reading comprehension (biomedical-MRC) aims to comprehend complex biomedical narratives and assist healthcare professionals in retrieving information from them. The high performance of modern neural network-based MRC systems depends on high-quality, large-scale, human-annotated training datasets. In the biomedical domain, a crucial challenge in creating such datasets is the requirement for domain knowledge, inducing the scarcity of labeled data and the need for transfer learning from the labeled general-purpose (source) domain to the biomedical (target) domain. However, there is a discrepancy in marginal distributions between the general-purpose and biomedical domains due to the variances in topics. Therefore, direct-transferring of learned representations from a model trained on a general-purpose domain to the biomedical domain can hurt the model's performance.   Results: We present an adversarial learning-based domain adaptation framework for the biomedical machine reading comprehension task (BioADAPT-MRC), a neural network-based method to address the discrepancies in the marginal distributions between the general and biomedical domain datasets. BioADAPT-MRC relaxes the need for generating pseudo labels for training a well-performing biomedical-MRC model. We extensively evaluate the performance of BioADAPT-MRC by comparing it with the best existing methods on three widely used benchmark biomedical-MRC datasets -- BioASQ-7b, BioASQ-8b, and BioASQ-9b. Our results suggest that without using any synthetic or human-annotated data from the biomedical domain, BioADAPT-MRC can achieve state-of-the-art performance on these datasets.   Availability: BioADAPT-MRC is freely available as an open-source project at\\\\https://github.com/mmahbub/BioADAPT-MRC", "paper_url": "http://arxiv.org/abs/2202.13174v1", "pdf_url": "http://arxiv.org/pdf/2202.13174v1", "repo_url": null}, "2203.00585": {"publish_time": "2022-03-01", "title": "Self-Supervised Vision Transformers Learn Visual Concepts in Histopathology", "author": "Richard J. Chen et.al.", "abstract": "Tissue phenotyping is a fundamental task in learning objective characterizations of histopathologic biomarkers within the tumor-immune microenvironment in cancer pathology. However, whole-slide imaging (WSI) is a complex computer vision in which: 1) WSIs have enormous image resolutions with precludes large-scale pixel-level efforts in data curation, and 2) diversity of morphological phenotypes results in inter- and intra-observer variability in tissue labeling. To address these limitations, current efforts have proposed using pretrained image encoders (transfer learning from ImageNet, self-supervised pretraining) in extracting morphological features from pathology, but have not been extensively validated. In this work, we conduct a search for good representations in pathology by training a variety of self-supervised models with validation on a variety of weakly-supervised and patch-level tasks. Our key finding is in discovering that Vision Transformers using DINO-based knowledge distillation are able to learn data-efficient and interpretable features in histology images wherein the different attention heads learn distinct morphological phenotypes. We make evaluation code and pretrained weights publicly-available at: https://github.com/Richarizardd/Self-Supervised-ViT-Path.", "paper_url": "http://arxiv.org/abs/2203.00585v1", "pdf_url": "http://arxiv.org/pdf/2203.00585v1", "repo_url": "https://github.com/richarizardd/self-supervised-vit-path"}, "2203.00355": {"publish_time": "2022-03-01", "title": "Tempera: Spatial Transformer Feature Pyramid Network for Cardiac MRI Segmentation", "author": "Christoforos Galazis et.al.", "abstract": "Assessing the structure and function of the right ventricle (RV) is important in the diagnosis of several cardiac pathologies. However, it remains more challenging to segment the RV than the left ventricle (LV). In this paper, we focus on segmenting the RV in both short (SA) and long-axis (LA) cardiac MR images simultaneously. For this task, we propose a new multi-input/output architecture, hybrid 2D/3D geometric spatial TransformEr Multi-Pass fEature pyRAmid (Tempera). Our feature pyramid extends current designs by allowing not only a multi-scale feature output but multi-scale SA and LA input images as well. Tempera transfers learned features between SA and LA images via layer weight sharing and incorporates a geometric target transformer to map the predicted SA segmentation to LA space. Our model achieves an average Dice score of 0.836 and 0.798 for the SA and LA, respectively, and 26.31 mm and 31.19 mm Hausdorff distances. This opens up the potential for the incorporation of RV segmentation models into clinical workflows.", "paper_url": "http://arxiv.org/abs/2203.00355v1", "pdf_url": "http://arxiv.org/pdf/2203.00355v1", "repo_url": "https://github.com/cgalaz01/mnms2_challenge"}, "2203.00251": {"publish_time": "2022-03-01", "title": "FIRL: Fast Imitation and Policy Reuse Learning", "author": "Yiwen Chen et.al.", "abstract": "Intelligent robotics policies have been widely researched for challenging applications such as opening doors, washing dishes, and table organization. We refer to a \"Policy Pool\", containing skills that be easily accessed and reused. There are researches to leverage the pool, such as policy reuse, modular learning, assembly learning, transfer learning, hierarchical reinforcement learning (HRL), etc. However, most methods generally do not perform well in learning efficiency and require large datasets for training. This work focuses on enabling fast learning based on the policy pool. It should learn fast enough in one-shot or few-shot by avoiding learning from scratch. We also allow it to interact and learn from humans, but the training period should be within minutes. We propose FIRL, Fast (one-shot) Imitation, and Policy Reuse Learning. Instead of learning a new skill from scratch, it performs the one-shot imitation learning on the higher layer under a 2-layer hierarchical mechanism. Our method reduces a complex task learning to a simple regression problem that it could solve in a few offline iterations. The agent could have a good command of a new task given a one-shot demonstration. We demonstrate this method on the OpenDoors mini-grid environment, and the code is available on http://www.github.com/yiwc/firl.", "paper_url": "http://arxiv.org/abs/2203.00251v1", "pdf_url": "http://arxiv.org/pdf/2203.00251v1", "repo_url": "https://github.com/yiwc/firl"}, "2203.01311": {"publish_time": "2022-03-02", "title": "HighMMT: Towards Modality and Task Generalization for High-Modality Representation Learning", "author": "Paul Pu Liang et.al.", "abstract": "Learning multimodal representations involves discovering correspondences and integrating information from multiple heterogeneous sources of data. While recent research has begun to explore the design of more general-purpose multimodal models (contrary to prior focus on domain and modality-specific architectures), these methods are still largely focused on a small set of modalities in the language, vision, and audio space. In order to accelerate generalization towards diverse and understudied modalities, we investigate methods for high-modality (a large set of diverse modalities) and partially-observable (each task only defined on a small subset of modalities) scenarios. To tackle these challenges, we design a general multimodal model that enables multitask and transfer learning: multitask learning with shared parameters enables stable parameter counts (addressing scalability), and cross-modal transfer learning enables information sharing across modalities and tasks (addressing partial observability). Our resulting model generalizes across text, image, video, audio, time-series, sensors, tables, and set modalities from different research areas, improves the tradeoff between performance and efficiency, transfers to new modalities and tasks, and reveals surprising insights on the nature of information sharing in multitask models. We release our code and benchmarks which we hope will present a unified platform for subsequent theoretical and empirical analysis: https://github.com/pliang279/HighMMT.", "paper_url": "http://arxiv.org/abs/2203.01311v2", "pdf_url": "http://arxiv.org/pdf/2203.01311v2", "repo_url": "https://github.com/pliang279/highmmt"}, "2203.01265": {"publish_time": "2022-03-02", "title": "Self-supervised Transformer for Deepfake Detection", "author": "Hanqing Zhao et.al.", "abstract": "The fast evolution and widespread of deepfake techniques in real-world scenarios require stronger generalization abilities of face forgery detectors. Some works capture the features that are unrelated to method-specific artifacts, such as clues of blending boundary, accumulated up-sampling, to strengthen the generalization ability. However, the effectiveness of these methods can be easily corrupted by post-processing operations such as compression. Inspired by transfer learning, neural networks pre-trained on other large-scale face-related tasks may provide useful features for deepfake detection. For example, lip movement has been proved to be a kind of robust and good-transferring highlevel semantic feature, which can be learned from the lipreading task. However, the existing method pre-trains the lip feature extraction model in a supervised manner, which requires plenty of human resources in data annotation and increases the difficulty of obtaining training data. In this paper, we propose a self-supervised transformer based audio-visual contrastive learning method. The proposed method learns mouth motion representations by encouraging the paired video and audio representations to be close while unpaired ones to be diverse. After pre-training with our method, the model will then be partially fine-tuned for deepfake detection task. Extensive experiments show that our self-supervised method performs comparably or even better than the supervised pre-training counterpart.", "paper_url": "http://arxiv.org/abs/2203.01265v1", "pdf_url": "http://arxiv.org/pdf/2203.01265v1", "repo_url": null}, "2203.01187": {"publish_time": "2022-03-02", "title": "Visual Feature Encoding for GNNs on Road Networks", "author": "Oliver Stromann et.al.", "abstract": "In this work, we present a novel approach to learning an encoding of visual features into graph neural networks with the application on road network data. We propose an architecture that combines state-of-the-art vision backbone networks with graph neural networks. More specifically, we perform a road type classification task on an Open Street Map road network through encoding of satellite imagery using various ResNet architectures. Our architecture further enables fine-tuning and a transfer-learning approach is evaluated by pretraining on the NWPU-RESISC45 image classification dataset for remote sensing and comparing them to purely ImageNet-pretrained ResNet models as visual feature encoders. The results show not only that the visual feature encoders are superior to low-level visual features, but also that the fine-tuning of the visual feature encoder to a general remote sensing dataset such as NWPU-RESISC45 can further improve the performance of a GNN on a machine learning task like road type classification.", "paper_url": "http://arxiv.org/abs/2203.01187v1", "pdf_url": "http://arxiv.org/pdf/2203.01187v1", "repo_url": null}, "2203.01111": {"publish_time": "2022-03-02", "title": "Large-Scale Hate Speech Detection with Cross-Domain Transfer", "author": "Cagri Toraman et.al.", "abstract": "The performance of hate speech detection models relies on the datasets on which the models are trained. Existing datasets are mostly prepared with a limited number of instances or hate domains that define hate topics. This hinders large-scale analysis and transfer learning with respect to hate domains. In this study, we construct large-scale tweet datasets for hate speech detection in English and a low-resource language, Turkish, consisting of human-labeled 100k tweets per each. Our datasets are designed to have equal number of tweets distributed over five domains. The experimental results supported by statistical tests show that Transformer-based language models outperform conventional bag-of-words and neural models by at least 5% in English and 10% in Turkish for large-scale hate speech detection. The performance is also scalable to different training sizes, such that 98% of performance in English, and 97% in Turkish, are recovered when 20% of training instances are used. We further examine the generalization ability of cross-domain transfer among hate domains. We show that 96% of the performance of a target domain in average is recovered by other domains for English, and 92% for Turkish. Gender and religion are more successful to generalize to other domains, while sports fail most.", "paper_url": "http://arxiv.org/abs/2203.01111v1", "pdf_url": "http://arxiv.org/pdf/2203.01111v1", "repo_url": "https://github.com/avaapm/hatespeech"}, "2203.00853": {"publish_time": "2022-03-02", "title": "Transfer Learning of High-Fidelity Opacity Spectra in Autoencoders and Surrogate Models", "author": "Michael D. Vander Wal et.al.", "abstract": "Simulations of high energy density physics are expensive, largely in part for the need to produce non-local thermodynamic equilibrium opacities. High-fidelity spectra may reveal new physics in the simulations not seen with low-fidelity spectra, but the cost of these simulations also scale with the level of fidelity of the opacities being used. Neural networks are capable of reproducing these spectra, but neural networks need data to to train them which limits the level of fidelity of the training data. This paper demonstrates that it is possible to reproduce high-fidelity spectra with median errors in the realm of 3\\% to 4\\% using as few as 50 samples of high-fidelity Krypton data by performing transfer learning on a neural network trained on many times more low-fidelity data.", "paper_url": "http://arxiv.org/abs/2203.00853v1", "pdf_url": "http://arxiv.org/pdf/2203.00853v1", "repo_url": null}, "2203.01655": {"publish_time": "2022-03-03", "title": "On partitioning of an SHM problem and parallels with transfer learning", "author": "G. Tsialiamanis et.al.", "abstract": "In the current work, a problem-splitting approach and a scheme motivated by transfer learning is applied to a structural health monitoring problem. The specific problem in this case is that of localising damage on an aircraft wing. The original experiment is described, together with the initial approach, in which a neural network was trained to localise damage. The results were not ideal, partly because of a scarcity of training data, and partly because of the difficulty in resolving two of the damage cases. In the current paper, the problem is split into two sub-problems and an increase in classification accuracy is obtained. The sub-problems are obtained by separating out the most difficult-to-classify damage cases. A second approach to the problem is considered by adopting ideas from transfer learning (usually applied in much deeper) networks to see if a network trained on the simpler damage cases can help with feature extraction in the more difficult cases. The transfer of a fixed trained batch of layers between the networks is found to improve classification by making the classes more separable in the feature space and to speed up convergence.", "paper_url": "http://arxiv.org/abs/2203.01655v1", "pdf_url": "http://arxiv.org/pdf/2203.01655v1", "repo_url": null}, "2203.03443": {"publish_time": "2022-03-07", "title": "Generalization Through The Lens Of Leave-One-Out Error", "author": "Gregor Bachmann et.al.", "abstract": "Despite the tremendous empirical success of deep learning models to solve various learning tasks, our theoretical understanding of their generalization ability is very limited. Classical generalization bounds based on tools such as the VC dimension or Rademacher complexity, are so far unsuitable for deep models and it is doubtful that these techniques can yield tight bounds even in the most idealistic settings (Nagarajan & Kolter, 2019). In this work, we instead revisit the concept of leave-one-out (LOO) error to measure the generalization ability of deep models in the so-called kernel regime. While popular in statistics, the LOO error has been largely overlooked in the context of deep learning. By building upon the recently established connection between neural networks and kernel learning, we leverage the closed-form expression for the leave-one-out error, giving us access to an efficient proxy for the test error. We show both theoretically and empirically that the leave-one-out error is capable of capturing various phenomena in generalization theory, such as double descent, random labels or transfer learning. Our work therefore demonstrates that the leave-one-out error provides a tractable way to estimate the generalization ability of deep neural networks in the kernel regime, opening the door to potential, new research directions in the field of generalization.", "paper_url": "http://arxiv.org/abs/2203.03443v1", "pdf_url": "http://arxiv.org/pdf/2203.03443v1", "repo_url": "https://github.com/gregorbachmann/leaveoneout"}, "2203.03227": {"publish_time": "2022-03-07", "title": "Knowledge Transfer in Deep Reinforcement Learning for Slice-Aware Mobility Robustness Optimization", "author": "Qi Liao et.al.", "abstract": "The legacy mobility robustness optimization (MRO) in self-organizing networks aims at improving handover performance by optimizing cell-specific handover parameters. However, such solutions cannot satisfy the needs of next-generation network with network slicing, because it only guarantees the received signal strength but not the per-slice service quality. To provide the truly seamless mobility service, we propose a deep reinforcement learning-based slice-aware mobility robustness optimization (SAMRO) approach, which improves handover performance with per-slice service assurance by optimizing slice-specific handover parameters. Moreover, to allow safe and sample efficient online training, we develop a two-step transfer learning scheme: 1) regularized offline reinforcement learning, and 2) effective online fine-tuning with mixed experience replay. System-level simulations show that compared against the legacy MRO algorithms, SAMRO significantly improves slice-aware service continuation while optimizing the handover performance.", "paper_url": "http://arxiv.org/abs/2203.03227v1", "pdf_url": "http://arxiv.org/pdf/2203.03227v1", "repo_url": null}, "2203.04287": {"publish_time": "2022-03-08", "title": "A Simple Multi-Modality Transfer Learning Baseline for Sign Language Translation", "author": "Yutong Chen et.al.", "abstract": "This paper proposes a simple transfer learning baseline for sign language translation. Existing sign language datasets (e.g. PHOENIX-2014T, CSL-Daily) contain only about 10K-20K pairs of sign videos, gloss annotations and texts, which are an order of magnitude smaller than typical parallel data for training spoken language translation models. Data is thus a bottleneck for training effective sign language translation models. To mitigate this problem, we propose to progressively pretrain the model from general-domain datasets that include a large amount of external supervision to within-domain datasets. Concretely, we pretrain the sign-to-gloss visual network on the general domain of human actions and the within-domain of a sign-to-gloss dataset, and pretrain the gloss-to-text translation network on the general domain of a multilingual corpus and the within-domain of a gloss-to-text corpus. The joint model is fine-tuned with an additional module named the visual-language mapper that connects the two networks. This simple baseline surpasses the previous state-of-the-art results on two sign language translation benchmarks, demonstrating the effectiveness of transfer learning. With its simplicity and strong performance, this approach can serve as a solid baseline for future research.", "paper_url": "http://arxiv.org/abs/2203.04287v1", "pdf_url": "http://arxiv.org/pdf/2203.04287v1", "repo_url": null}, "2203.04027": {"publish_time": "2022-03-08", "title": "Data augmentation with mixtures of max-entropy transformations for filling-level classification", "author": "Apostolos Modas et.al.", "abstract": "We address the problem of distribution shifts in test-time data with a principled data augmentation scheme for the task of content-level classification. In such a task, properties such as shape or transparency of test-time containers (cup or drinking glass) may differ from those represented in the training data. Dealing with such distribution shifts using standard augmentation schemes is challenging and transforming the training images to cover the properties of the test-time instances requires sophisticated image manipulations. We therefore generate diverse augmentations using a family of max-entropy transformations that create samples with new shapes, colors and spectral characteristics. We show that such a principled augmentation scheme, alone, can replace current approaches that use transfer learning or can be used in combination with transfer learning to improve its performance.", "paper_url": "http://arxiv.org/abs/2203.04027v1", "pdf_url": "http://arxiv.org/pdf/2203.04027v1", "repo_url": null}, "2203.03878": {"publish_time": "2022-03-08", "title": "HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks", "author": "Zhengkun Zhang et.al.", "abstract": "The workflow of pretraining and fine-tuning has emerged as a popular paradigm for solving various NLP and V&L (Vision-and-Language) downstream tasks. With the capacity of pretrained models growing rapidly, how to perform parameter-efficient fine-tuning has become fairly important for quick transfer learning and deployment. In this paper, we design a novel unified parameter-efficient transfer learning framework that works effectively on both pure language and V&L tasks. In particular, we use a shared hypernetwork that takes trainable hyper-embeddings as input, and outputs weights for fine-tuning different small modules in a pretrained language model, such as tuning the parameters inserted into multi-head attention blocks (i.e., prefix-tuning) and feed-forward blocks (i.e., adapter-tuning). We define a set of embeddings (e.g., layer, block, task and visual embeddings) as the key components to calculate hyper-embeddings, which thus can support both pure language and V&L tasks. Our proposed framework adds fewer trainable parameters in multi-task learning while achieving superior performances and transfer ability compared to state-of-the-art methods. Empirical results on the GLUE benchmark and multiple V&L tasks confirm the effectiveness of our framework on both textual and visual modalities.", "paper_url": "http://arxiv.org/abs/2203.03878v1", "pdf_url": "http://arxiv.org/pdf/2203.03878v1", "repo_url": null}, "2203.03871": {"publish_time": "2022-03-08", "title": "Discriminability-Transferability Trade-Off: An Information-Theoretic Perspective", "author": "Quan Cui et.al.", "abstract": "This work simultaneously considers the discriminability and transferability properties of deep representations in the typical supervised learning task, i.e., image classification. By a comprehensive temporal analysis, we observe a trade-off between these two properties. The discriminability keeps increasing with the training progressing while the transferability intensely diminishes in the later training period.   From the perspective of information-bottleneck theory, we reveal that the incompatibility between discriminability and transferability is attributed to the over-compression of input information. More importantly, we investigate why and how the InfoNCE loss can alleviate the over-compression, and further present a learning framework, named contrastive temporal coding~(CTC), to counteract the over-compression and alleviate the incompatibility. Extensive experiments validate that CTC successfully mitigates the incompatibility, yielding discriminative and transferable representations. Noticeable improvements are achieved on the image classification task and challenging transfer learning tasks. We hope that this work will raise the significance of the transferability property in the conventional supervised learning setting. Code will be publicly available.", "paper_url": "http://arxiv.org/abs/2203.03871v1", "pdf_url": "http://arxiv.org/pdf/2203.03871v1", "repo_url": null}, "2203.04904": {"publish_time": "2022-03-09", "title": "Model-Agnostic Multitask Fine-tuning for Few-shot Vision-Language Transfer Learning", "author": "Zhenhailong Wang et.al.", "abstract": "Despite achieving state-of-the-art zero-shot performance, existing vision-language models, e.g., CLIP, still fall short of domain-specific classification tasks, e.g., Fungi Classification. In the context of few-shot transfer learning, traditional fine-tuning fails to prevent highly expressive model from exploiting spurious correlations in the training data. On the other hand, although model-agnostic meta-learning (MAML) presents as a natural alternative for transfer learning, the expensive computation due to implicit second-order optimization limits its use in large-scale models and datasets. In this work we aim to further improve the generalization of existing vision-language models on unseen tasks via a simple yet efficient fine-tuning strategy based on uniform task sampling. We term our method as Model-Agnostic Multitask Fine-tuning (MAMF). Compared with MAML, MAMF discards the bi-level optimization and uses only first-order gradients, which makes it easily scalable and computationally efficient. Due to the uniform task sampling procedure, MAMF consistently outperforms the classical fine-tuning method for few-shot transfer learning on five benchmark datasets. Empirically, we further discover that the effectiveness of first-order MAML is highly dependent on the zero-shot performance of the pretrained model, and our simple algorithm can outperform first-order MAML on more challenging datasets with low zero-shot performance.", "paper_url": "http://arxiv.org/abs/2203.04904v1", "pdf_url": "http://arxiv.org/pdf/2203.04904v1", "repo_url": null}, "2203.04863": {"publish_time": "2022-03-09", "title": "Unsupervised Alignment of Distributional Word Embeddings", "author": "Aissatou Diallo et.al.", "abstract": "Cross-domain alignment play a key roles in tasks ranging from machine translation to transfer learning. Recently, purely unsupervised methods operating on monolingual embeddings have successfully been used to infer a bilingual lexicon without relying on supervision. However, current state-of-the art methods only focus on point vectors although distributional embeddings have proven to embed richer semantic information when representing words. In this paper, we propose stochastic optimization approach for aligning probabilistic embeddings. Finally, we evaluate our method on the problem of unsupervised word translation, by aligning word embeddings trained on monolingual data. We show that the proposed approach achieves good performance on the bilingual lexicon induction task across several language pairs and performs better than the point-vector based approach.", "paper_url": "http://arxiv.org/abs/2203.04863v1", "pdf_url": "http://arxiv.org/pdf/2203.04863v1", "repo_url": null}, "2203.04729": {"publish_time": "2022-03-09", "title": "Pretrained Domain-Specific Language Model for General Information Retrieval Tasks in the AEC Domain", "author": "Zhe Zheng et.al.", "abstract": "As an essential task for the architecture, engineering, and construction (AEC) industry, information retrieval (IR) from unstructured textual data based on natural language processing (NLP) is gaining increasing attention. Although various deep learning (DL) models for IR tasks have been investigated in the AEC domain, it is still unclear how domain corpora and domain-specific pretrained DL models can improve performance in various IR tasks. To this end, this work systematically explores the impacts of domain corpora and various transfer learning techniques on the performance of DL models for IR tasks and proposes a pretrained domain-specific language model for the AEC domain. First, both in-domain and close-domain corpora are developed. Then, two types of pretrained models, including traditional wording embedding models and BERT-based models, are pretrained based on various domain corpora and transfer learning strategies. Finally, several widely used DL models for IR tasks are further trained and tested based on various configurations and pretrained models. The result shows that domain corpora have opposite effects on traditional word embedding models for text classification and named entity recognition tasks but can further improve the performance of BERT-based models in all tasks. Meanwhile, BERT-based models dramatically outperform traditional methods in all IR tasks, with maximum improvements of 5.4% and 10.1% in the F1 score, respectively. This research contributes to the body of knowledge in two ways: 1) demonstrating the advantages of domain corpora and pretrained DL models and 2) opening the first domain-specific dataset and pretrained language model for the AEC domain, to the best of our knowledge. Thus, this work sheds light on the adoption and application of pretrained models in the AEC domain.", "paper_url": "http://arxiv.org/abs/2203.04729v1", "pdf_url": "http://arxiv.org/pdf/2203.04729v1", "repo_url": "https://github.com/skydustz/aec-domain-corpora"}, "2203.04482": {"publish_time": "2022-03-09", "title": "Multi-Agent Policy Transfer via Task Relationship Modeling", "author": "Rongjun Qin et.al.", "abstract": "Team adaptation to new cooperative tasks is a hallmark of human intelligence, which has yet to be fully realized in learning agents. Previous work on multi-agent transfer learning accommodate teams of different sizes, heavily relying on the generalization ability of neural networks for adapting to unseen tasks. We believe that the relationship among tasks provides the key information for policy adaptation. In this paper, we try to discover and exploit common structures among tasks for more efficient transfer, and propose to learn effect-based task representations as a common space of tasks, using an alternatively fixed training scheme. We demonstrate that the task representation can capture the relationship among tasks, and can generalize to unseen tasks. As a result, the proposed method can help transfer learned cooperation knowledge to new tasks after training on a few source tasks. We also find that fine-tuning the transferred policies help solve tasks that are hard to learn from scratch.", "paper_url": "http://arxiv.org/abs/2203.04482v1", "pdf_url": "http://arxiv.org/pdf/2203.04482v1", "repo_url": null}, "2203.05208": {"publish_time": "2022-03-10", "title": "Transferring Dual Stochastic Graph Convolutional Network for Facial Micro-expression Recognition", "author": "Hui Tang et.al.", "abstract": "Micro-expression recognition has drawn increasing attention due to its wide application in lie detection, criminal detection and psychological consultation. To improve the recognition performance of the small micro-expression data, this paper presents a transferring dual stochastic Graph Convolutional Network (TDSGCN) model. We propose a stochastic graph construction method and dual graph convolutional network to extract more discriminative features from the micro-expression images. We use transfer learning to pre-train SGCNs from macro expression data. Optical flow algorithm is also integrated to extract their temporal features. We fuse both spatial and temporal features to improve the recognition performance. To the best of our knowledge, this is the first attempt to utilize the transferring learning and graph convolutional network in micro-expression recognition task. In addition, to handle the class imbalance problem of dataset, we focus on the design of focal loss function. Through extensive evaluation, our proposed method achieves state-of-the-art performance on SAMM and recently released MMEW benchmarks. Our code will be publicly available accompanying this paper.", "paper_url": "http://arxiv.org/abs/2203.05208v1", "pdf_url": "http://arxiv.org/pdf/2203.05208v1", "repo_url": null}, "2203.05126": {"publish_time": "2022-03-10", "title": "PACTran: PAC-Bayesian Metrics for Estimating the Transferability of Pretrained Models to Classification Tasks", "author": "Nan Ding et.al.", "abstract": "With the increasing abundance of pretrained models in recent years, the problem of selecting the best pretrained checkpoint for a particular downstream classification task has been gaining increased attention. Although several methods have recently been proposed to tackle the selection problem (e.g. LEEP, H-score), these methods resort to applying heuristics that are not well motivated by learning theory. In this paper we present PACTran, a theoretically grounded family of metrics for pretrained model selection and transferability measurement. We first show how to derive PACTran metrics from the optimal PAC-Bayesian bound under the transfer learning setting. We then empirically evaluate three metric instantiations of PACTran on a number of vision tasks (VTAB) as well as a language-and-vision (OKVQA) task. An analysis of the results shows PACTran is a more consistent and effective transferability measure compared to existing selection methods.", "paper_url": "http://arxiv.org/abs/2203.05126v1", "pdf_url": "http://arxiv.org/pdf/2203.05126v1", "repo_url": null}, "2203.05026": {"publish_time": "2022-03-09", "title": "Transfer Learning as an Essential Tool for Digital Twins in Renewable Energy Systems", "author": "Chandana Priya Nivarthi et.al.", "abstract": "Transfer learning (TL), the next frontier in machine learning (ML), has gained much popularity in recent years, due to the various challenges faced in ML, like the requirement of vast amounts of training data, expensive and time-consuming labelling processes for data samples, and long training duration for models. TL is useful in tackling these problems, as it focuses on transferring knowledge from previously solved tasks to new tasks. Digital twins and other intelligent systems need to utilise TL to use the previously gained knowledge and solve new tasks in a more self-reliant way, and to incrementally increase their knowledge base. Therefore, in this article, the critical challenges in power forecasting and anomaly detection in the context of renewable energy systems are identified, and a potential TL framework to meet these challenges is proposed. This article also proposes a feature embedding approach to handle the missing sensors data. The proposed TL methods help to make a system more autonomous in the context of organic computing.", "paper_url": "http://arxiv.org/abs/2203.05026v1", "pdf_url": "http://arxiv.org/pdf/2203.05026v1", "repo_url": null}, "2203.06107": {"publish_time": "2022-03-11", "title": "REX: Reasoning-aware and Grounded Explanation", "author": "Shi Chen et.al.", "abstract": "Effectiveness and interpretability are two essential properties for trustworthy AI systems. Most recent studies in visual reasoning are dedicated to improving the accuracy of predicted answers, and less attention is paid to explaining the rationales behind the decisions. As a result, they commonly take advantage of spurious biases instead of actually reasoning on the visual-textual data, and have yet developed the capability to explain their decision making by considering key information from both modalities. This paper aims to close the gap from three distinct perspectives: first, we define a new type of multi-modal explanations that explain the decisions by progressively traversing the reasoning process and grounding keywords in the images. We develop a functional program to sequentially execute different reasoning steps and construct a new dataset with 1,040,830 multi-modal explanations. Second, we identify the critical need to tightly couple important components across the visual and textual modalities for explaining the decisions, and propose a novel explanation generation method that explicitly models the pairwise correspondence between words and regions of interest. It improves the visual grounding capability by a considerable margin, resulting in enhanced interpretability and reasoning performance. Finally, with our new data and method, we perform extensive analyses to study the effectiveness of our explanation under different settings, including multi-task learning and transfer learning. Our code and data are available at https://github.com/szzexpoi/rex.", "paper_url": "http://arxiv.org/abs/2203.06107v1", "pdf_url": "http://arxiv.org/pdf/2203.06107v1", "repo_url": "https://github.com/szzexpoi/rex"}, "2203.05908": {"publish_time": "2022-03-11", "title": "BabyNet: Reconstructing 3D faces of babies from uncalibrated photographs", "author": "Araceli Morales et.al.", "abstract": "We present a 3D face reconstruction system that aims at recovering the 3D facial geometry of babies from uncalibrated photographs, BabyNet. Since the 3D facial geometry of babies differs substantially from that of adults, baby-specific facial reconstruction systems are needed. BabyNet consists of two stages: 1) a 3D graph convolutional autoencoder learns a latent space of the baby 3D facial shape; and 2) a 2D encoder that maps photographs to the 3D latent space based on representative features extracted using transfer learning. In this way, using the pre-trained 3D decoder, we can recover a 3D face from 2D images. We evaluate BabyNet and show that 1) methods based on adult datasets cannot model the 3D facial geometry of babies, which proves the need for a baby-specific method, and 2) BabyNet outperforms classical model-fitting methods even when a baby-specific 3D morphable model, such as BabyFM, is used.", "paper_url": "http://arxiv.org/abs/2203.05908v1", "pdf_url": "http://arxiv.org/pdf/2203.05908v1", "repo_url": null}, "2203.05882": {"publish_time": "2022-03-11", "title": "Improving the transferability of speech separation by meta-learning", "author": "Kuan-Po Huang et.al.", "abstract": "Speech separation aims to separate multiple speech sources from a speech mixture. Although speech separation is well-solved on some existing English speech separation benchmarks, it is worthy of more investigation on the generalizability of speech separation models on the accents or languages unseen during training. This paper adopts meta-learning based methods to improve the transferability of speech separation models. With the meta-learning based methods, we discovered that only using speech data with one accent, the native English accent, as our training data, the models still can be adapted to new unseen accents on the Speech Accent Archive. We compared the results with a human-rated native-likeness of accents, showing that the transferability of MAML methods has less relation to the similarity of data between the training and testing phase compared to the typical transfer learning methods. Furthermore, we found that models can deal with different language data from the CommonVoice corpus during the testing phase. Most of all, the MAML methods outperform typical transfer learning methods when it comes to new accents, new speakers, new languages, and noisy environments.", "paper_url": "http://arxiv.org/abs/2203.05882v1", "pdf_url": "http://arxiv.org/pdf/2203.05882v1", "repo_url": null}, "2203.05811": {"publish_time": "2022-03-11", "title": "Reprogramming FairGANs with Variational Auto-Encoders: A New Transfer Learning Model", "author": "Beatrice Nobile et.al.", "abstract": "Fairness-aware GANs (FairGANs) exploit the mechanisms of Generative Adversarial Networks (GANs) to impose fairness on the generated data, freeing them from both disparate impact and disparate treatment. Given the model's advantages and performance, we introduce a novel learning framework to transfer a pre-trained FairGAN to other tasks. This reprogramming process has the goal of maintaining the FairGAN's main targets of data utility, classification utility, and data fairness, while widening its applicability and ease of use. In this paper we present the technical extensions required to adapt the original architecture to this new framework (and in particular the use of Variational Auto-Encoders), and discuss the benefits, trade-offs, and limitations of the new model.", "paper_url": "http://arxiv.org/abs/2203.05811v1", "pdf_url": "http://arxiv.org/pdf/2203.05811v1", "repo_url": null}, "2203.05733": {"publish_time": "2022-03-11", "title": "A Survey of Surface Defect Detection of Industrial Products Based on A Small Number of Labeled Data", "author": "Qifan Jin et.al.", "abstract": "The surface defect detection method based on visual perception has been widely used in industrial quality inspection. Because defect data are not easy to obtain and the annotation of a large number of defect data will waste a lot of manpower and material resources. Therefore, this paper reviews the methods of surface defect detection of industrial products based on a small number of labeled data, and this method is divided into traditional image processing-based industrial product surface defect detection methods and deep learning-based industrial product surface defect detection methods suitable for a small number of labeled data. The traditional image processing-based industrial product surface defect detection methods are divided into statistical methods, spectral methods and model methods. Deep learning-based industrial product surface defect detection methods suitable for a small number of labeled data are divided into based on data augmentation, based on transfer learning, model-based fine-tuning, semi-supervised, weak supervised and unsupervised.", "paper_url": "http://arxiv.org/abs/2203.05733v1", "pdf_url": "http://arxiv.org/pdf/2203.05733v1", "repo_url": null}, "2203.06849": {"publish_time": "2022-03-14", "title": "SUPERB-SG: Enhanced Speech processing Universal PERformance Benchmark for Semantic and Generative Capabilities", "author": "Hsiang-Sheng Tsai et.al.", "abstract": "Transfer learning has proven to be crucial in advancing the state of speech and natural language processing research in recent years. In speech, a model pre-trained by self-supervised learning transfers remarkably well on multiple tasks. However, the lack of a consistent evaluation methodology is limiting towards a holistic understanding of the efficacy of such models. SUPERB was a step towards introducing a common benchmark to evaluate pre-trained models across various speech tasks. In this paper, we introduce SUPERB-SG, a new benchmark focused on evaluating the semantic and generative capabilities of pre-trained models by increasing task diversity and difficulty over SUPERB. We use a lightweight methodology to test the robustness of representations learned by pre-trained models under shifts in data domain and quality across different types of tasks. It entails freezing pre-trained model parameters, only using simple task-specific trainable heads. The goal is to be inclusive of all researchers, and encourage efficient use of computational resources. We also show that the task diversity of SUPERB-SG coupled with limited task supervision is an effective recipe for evaluating the generalizability of model representation.", "paper_url": "http://arxiv.org/abs/2203.06849v1", "pdf_url": "http://arxiv.org/pdf/2203.06849v1", "repo_url": "https://github.com/s3prl/s3prl"}, "2203.06836": {"publish_time": "2022-03-14", "title": "Bures Joint Distribution Alignment with Dynamic Margin for Unsupervised Domain Adaptation", "author": "Yong-Hui Liu et.al.", "abstract": "Unsupervised domain adaptation (UDA) is one of the prominent tasks of transfer learning, and it provides an effective approach to mitigate the distribution shift between the labeled source domain and the unlabeled target domain. Prior works mainly focus on aligning the marginal distributions or the estimated class-conditional distributions. However, the joint dependency among the feature and the label is crucial for the adaptation task and is not fully exploited. To address this problem, we propose the Bures Joint Distribution Alignment (BJDA) algorithm which directly models the joint distribution shift based on the optimal transport theory in the infinite-dimensional kernel spaces. Specifically, we propose a novel alignment loss term that minimizes the kernel Bures-Wasserstein distance between the joint distributions. Technically, BJDA can effectively capture the nonlinear structures underlying the data. In addition, we introduce a dynamic margin in contrastive learning phase to flexibly characterize the class separability and improve the discriminative ability of representations. It also avoids the cross-validation procedure to determine the margin parameter in traditional triplet loss based methods. Extensive experiments show that BJDA is very effective for the UDA tasks, as it outperforms state-of-the-art algorithms in most experimental settings. In particular, BJDA improves the average accuracy of UDA tasks by 2.8% on Adaptiope, 1.4% on Office-Caltech10, and 1.1% on ImageCLEF-DA.", "paper_url": "http://arxiv.org/abs/2203.06836v1", "pdf_url": "http://arxiv.org/pdf/2203.06836v1", "repo_url": null}, "2203.06570": {"publish_time": "2022-03-13", "title": "Model Inversion Attack against Transfer Learning: Inverting a Model without Accessing It", "author": "Dayong Ye et.al.", "abstract": "Transfer learning is an important approach that produces pre-trained teacher models which can be used to quickly build specialized student models. However, recent research on transfer learning has found that it is vulnerable to various attacks, e.g., misclassification and backdoor attacks. However, it is still not clear whether transfer learning is vulnerable to model inversion attacks. Launching a model inversion attack against transfer learning scheme is challenging. Not only does the student model hide its structural parameters, but it is also inaccessible to the adversary. Hence, when targeting a student model, both the white-box and black-box versions of existing model inversion attacks fail. White-box attacks fail as they need the target model's parameters. Black-box attacks fail as they depend on making repeated queries of the target model. However, they may not mean that transfer learning models are impervious to model inversion attacks. Hence, with this paper, we initiate research into model inversion attacks against transfer learning schemes with two novel attack methods. Both are black-box attacks, suiting different situations, that do not rely on queries to the target student model. In the first method, the adversary has the data samples that share the same distribution as the training set of the teacher model. In the second method, the adversary does not have any such samples. Experiments show that highly recognizable data records can be recovered with both of these methods. This means that even if a model is an inaccessible black-box, it can still be inverted.", "paper_url": "http://arxiv.org/abs/2203.06570v1", "pdf_url": "http://arxiv.org/pdf/2203.06570v1", "repo_url": null}, "2203.06288": {"publish_time": "2022-03-11", "title": "Estimating Cluster Masses from SDSS Multi-band Images with Transfer Learning", "author": "Sheng-Chieh Lin et.al.", "abstract": "The total masses of galaxy clusters characterize many aspects of astrophysics and the underlying cosmology. It is crucial to obtain reliable and accurate mass estimates for numerous galaxy clusters over a wide range of redshifts and mass scales. We present a transfer-learning approach to estimate cluster masses using the ugriz-band images in the SDSS Data Release 12. The target masses are derived from X-ray or SZ measurements that are only available for a small subset of the clusters. We designed a semi-supervised deep learning model consisting of two convolutional neural networks. In the first network, a feature extractor is trained to classify the SDSS photometric bands. The second network takes the previously trained features as inputs to estimate their total masses. The training and testing processes in this work depend purely on real observational data. Our algorithm reaches a mean absolute error (MAE) of 0.232 dex on average and 0.214 dex for the best fold. The performance is comparable to that given by redMaPPer, 0.192 dex. We have further applied a joint integrated gradient and class activation mapping method to interpret such a two-step neural network. The performance of our algorithm is likely to improve as the size of training dataset increases. This proof-of-concept experiment demonstrates the potential of deep learning in maximizing the scientific return of the current and future large cluster surveys.", "paper_url": "http://arxiv.org/abs/2203.06288v1", "pdf_url": "http://arxiv.org/pdf/2203.06288v1", "repo_url": null}, "2203.06210": {"publish_time": "2022-03-11", "title": "Leveraging universality of jet taggers through transfer learning", "author": "Fr\u00e9d\u00e9ric A. Dreyer et.al.", "abstract": "A significant challenge in the tagging of boosted objects via machine-learning technology is the prohibitive computational cost associated with training sophisticated models. Nevertheless, the universality of QCD suggests that a large amount of the information learnt in the training is common to different physical signals and experimental setups. In this article, we explore the use of transfer learning techniques to develop fast and data-efficient jet taggers that leverage such universality. We consider the graph neural networks LundNet and ParticleNet, and introduce two prescriptions to transfer an existing tagger into a new signal based either on fine-tuning all the weights of a model or alternatively on freezing a fraction of them. In the case of $W$-boson and top-quark tagging, we find that one can obtain reliable taggers using an order of magnitude less data with a corresponding speed-up of the training process. Moreover, while keeping the size of the training data set fixed, we observe a speed-up of the training by up to a factor of three. This offers a promising avenue to facilitate the use of such tools in collider physics experiments.", "paper_url": "http://arxiv.org/abs/2203.06210v1", "pdf_url": "http://arxiv.org/pdf/2203.06210v1", "repo_url": null}, "2203.07910": {"publish_time": "2022-03-14", "title": "Deep Transfer Learning with Graph Neural Network for Sensor-Based Human Activity Recognition", "author": "Yan Yan et.al.", "abstract": "The sensor-based human activity recognition (HAR) in mobile application scenarios is often confronted with sensor modalities variation and annotated data deficiency. Given this observation, we devised a graph-inspired deep learning approach toward the sensor-based HAR tasks, which was further used to build a deep transfer learning model toward giving a tentative solution for these two challenging problems. Specifically, we present a multi-layer residual structure involved graph convolutional neural network (ResGCNN) toward the sensor-based HAR tasks, namely the HAR-ResGCNN approach. Experimental results on the PAMAP2 and mHealth data sets demonstrate that our ResGCNN is effective at capturing the characteristics of actions with comparable results compared to other sensor-based HAR models (with an average accuracy of 98.18% and 99.07%, respectively). More importantly, the deep transfer learning experiments using the ResGCNN model show excellent transferability and few-shot learning performance. The graph-based framework shows good meta-learning ability and is supposed to be a promising solution in sensor-based HAR tasks.", "paper_url": "http://arxiv.org/abs/2203.07910v1", "pdf_url": "http://arxiv.org/pdf/2203.07910v1", "repo_url": null}, "2203.08777": {"publish_time": "2022-03-16", "title": "Object discovery and representation networks", "author": "Olivier J. H\u00e9naff et.al.", "abstract": "The promise of self-supervised learning (SSL) is to leverage large amounts of unlabeled data to solve complex tasks. While there has been excellent progress with simple, image-level learning, recent methods have shown the advantage of including knowledge of image structure. However, by introducing hand-crafted image segmentations to define regions of interest, or specialized augmentation strategies, these methods sacrifice the simplicity and generality that makes SSL so powerful. Instead, we propose a self-supervised learning paradigm that discovers the structure encoded in these priors by itself. Our method, Odin, couples object discovery and representation networks to discover meaningful image segmentations without any supervision. The resulting learning paradigm is simpler, less brittle, and more general, and achieves state-of-the-art transfer learning results for object detection and instance segmentation on COCO, and semantic segmentation on PASCAL and Cityscapes, while strongly surpassing supervised pre-training for video segmentation on DAVIS.", "paper_url": "http://arxiv.org/abs/2203.08777v1", "pdf_url": "http://arxiv.org/pdf/2203.08777v1", "repo_url": null}, "2203.08612": {"publish_time": "2022-03-16", "title": "CtlGAN: Few-shot Artistic Portraits Generation with Contrastive Transfer Learning", "author": "Yue Wang et.al.", "abstract": "Generating artistic portraits is a challenging problem in computer vision. Existing portrait stylization models that generate good quality results are based on Image-to-Image Translation and require abundant data from both source and target domains. However, without enough data, these methods would result in overfitting. In this work, we propose CtlGAN, a new few-shot artistic portraits generation model with a novel contrastive transfer learning strategy. We adapt a pretrained StyleGAN in the source domain to a target artistic domain with no more than 10 artistic faces. To reduce overfitting to the few training examples, we introduce a novel Cross-Domain Triplet loss which explicitly encourages the target instances generated from different latent codes to be distinguishable. We propose a new encoder which embeds real faces into Z+ space and proposes a dual-path training strategy to better cope with the adapted decoder and eliminate the artifacts. Extensive qualitative, quantitative comparisons and a user study show our method significantly outperforms state-of-the-arts under 10-shot and 1-shot settings and generates high quality artistic portraits. The code will be made publicly available.", "paper_url": "http://arxiv.org/abs/2203.08612v1", "pdf_url": "http://arxiv.org/pdf/2203.08612v1", "repo_url": null}, "2203.08378": {"publish_time": "2022-03-16", "title": "Transforming Sequence Tagging Into A Seq2Seq Task", "author": "Karthik Raman et.al.", "abstract": "Pretrained, large, generative language models (LMs) have had great success in a wide range of sequence tagging and structured prediction tasks. Casting a sequence tagging task as a Seq2Seq one requires deciding the formats of the input and output sequences. However, we lack a principled understanding of the trade-offs associated with these formats (such as the effect on model accuracy, sequence length, multilingual generalization, hallucination). In this paper, we rigorously study different formats one could use for casting input text sentences and their output labels into the input and target (i.e., output) of a Seq2Seq model. Along the way, we introduce a new format, which we show to not only be simpler but also more effective. Additionally the new format demonstrates significant gains in the multilingual settings -- both zero-shot transfer learning and joint training. Lastly, we find that the new format is more robust and almost completely devoid of hallucination -- an issue we find common in existing formats. With well over a 1000 experiments studying 14 different formats, over 7 diverse public benchmarks -- including 3 multilingual datasets spanning 7 languages -- we believe our findings provide a strong empirical basis in understanding how we should tackle sequence tagging tasks.", "paper_url": "http://arxiv.org/abs/2203.08378v1", "pdf_url": "http://arxiv.org/pdf/2203.08378v1", "repo_url": null}, "2203.09445": {"publish_time": "2022-03-17", "title": "Image Super-Resolution With Deep Variational Autoencoders", "author": "Darius Chira et.al.", "abstract": "Image super-resolution (SR) techniques are used to generate a high-resolution image from a low-resolution image. Until now, deep generative models such as autoregressive models and Generative Adversarial Networks (GANs) have proven to be effective at modelling high-resolution images. Models based on Variational Autoencoders (VAEs) have often been criticized for their feeble generative performance, but with new advancements such as VDVAE (very deep VAE), there is now strong evidence that deep VAEs have the potential to outperform current state-of-the-art models for high-resolution image generation. In this paper, we introduce VDVAE-SR, a new model that aims to exploit the most recent deep VAE methodologies to improve upon image super-resolution using transfer learning on pretrained VDVAEs. Through qualitative and quantitative evaluations, we show that the proposed model is competitive with other state-of-the-art methods.", "paper_url": "http://arxiv.org/abs/2203.09445v1", "pdf_url": "http://arxiv.org/pdf/2203.09445v1", "repo_url": null}, "2203.09348": {"publish_time": "2022-03-17", "title": "POSTER: Diagnosis of COVID-19 through Transfer Learning Techniques on CT Scans: A Comparison of Deep Learning Models", "author": "Aeyan Ashraf et.al.", "abstract": "The novel coronavirus disease (COVID-19) constitutes a public health emergency globally. It is a deadly disease which has infected more than 230 million people worldwide. Therefore, early and unswerving detection of COVID-19 is necessary. Evidence of this virus is most commonly being tested by RT-PCR test. This test is not 100% reliable as it is known to give false positives and false negatives. Other methods like X-Ray images or CT scans show the detailed imaging of lungs and have been proven more reliable. This paper compares different deep learning models used to detect COVID-19 through transfer learning technique on CT scan dataset. VGG-16 outperforms all the other models achieving an accuracy of 85.33% on the dataset.", "paper_url": "http://arxiv.org/abs/2203.09348v1", "pdf_url": "http://arxiv.org/pdf/2203.09348v1", "repo_url": null}, "2203.09279": {"publish_time": "2022-03-17", "title": "Transfer learning for cross-modal demand prediction of bike-share and public transit", "author": "Mingzhuang Hua et.al.", "abstract": "The urban transportation system is a combination of multiple transport modes, and the interdependencies across those modes exist. This means that the travel demand across different travel modes could be correlated as one mode may receive demand from or create demand for another mode, not to mention natural correlations between different demand time series due to general demand flow patterns across the network. It is expectable that cross-modal ripple effects become more prevalent, with Mobility as a Service. Therefore, by propagating demand data across modes, a better demand prediction could be obtained. To this end, this study explores various machine learning models and transfer learning strategies for cross-modal demand prediction. The trip data of bike-share, metro, and taxi are processed as the station-level passenger flows, and then the proposed prediction method is tested in the large-scale case studies of Nanjing and Chicago. The results suggest that prediction models with transfer learning perform better than unimodal prediction models. Furthermore, stacked Long Short-Term Memory model performs particularly well in cross-modal demand prediction. These results verify our combined method's forecasting improvement over existing benchmarks and demonstrate the good transferability for cross-modal demand prediction in multiple cities.", "paper_url": "http://arxiv.org/abs/2203.09279v1", "pdf_url": "http://arxiv.org/pdf/2203.09279v1", "repo_url": null}, "2203.09270": {"publish_time": "2022-03-17", "title": "Mixing Up Contrastive Learning: Self-Supervised Representation Learning for Time Series", "author": "Kristoffer Wickstr\u00f8m et.al.", "abstract": "The lack of labeled data is a key challenge for learning useful representation from time series data. However, an unsupervised representation framework that is capable of producing high quality representations could be of great value. It is key to enabling transfer learning, which is especially beneficial for medical applications, where there is an abundance of data but labeling is costly and time consuming. We propose an unsupervised contrastive learning framework that is motivated from the perspective of label smoothing. The proposed approach uses a novel contrastive loss that naturally exploits a data augmentation scheme in which new samples are generated by mixing two data samples with a mixing component. The task in the proposed framework is to predict the mixing component, which is utilized as soft targets in the loss function. Experiments demonstrate the framework's superior performance compared to other representation learning approaches on both univariate and multivariate time series and illustrate its benefits for transfer learning for clinical time series.", "paper_url": "http://arxiv.org/abs/2203.09270v1", "pdf_url": "http://arxiv.org/pdf/2203.09270v1", "repo_url": "https://github.com/wickstrom/mixupcontrastivelearning"}, "2203.09132": {"publish_time": "2022-03-17", "title": "Feature-informed Latent Space Regularization for Music Source Separation", "author": "Yun-Ning Hung et.al.", "abstract": "The integration of additional side information to improve music source separation has been investigated numerous times, e.g., by adding features to the input or by adding learning targets in a multi-task learning scenario. These approaches, however, require additional annotations such as musical scores, instrument labels, etc. in training and possibly during inference. The available datasets for source separation do not usually provide these additional annotations. In this work, we explore transfer learning strategies to incorporate VGGish features with a state-of-the-art source separation model; VGGish features are known to be a very condensed representation of audio content and have been successfully used in many MIR tasks. We introduce three approaches to incorporate the features, including two latent space regularization methods and one naive concatenation method. Experimental results show that our proposed approaches improve several evaluation metrics for music source separation.", "paper_url": "http://arxiv.org/abs/2203.09132v1", "pdf_url": "http://arxiv.org/pdf/2203.09132v1", "repo_url": null}, "2203.09855": {"publish_time": "2022-03-18", "title": "Multi-Modal Masked Pre-Training for Monocular Panoramic Depth Completion", "author": "Zhiqiang Yan et.al.", "abstract": "In this paper, we formulate a potentially valuable panoramic depth completion (PDC) task as panoramic 3D cameras often produce 360{\\deg} depth with missing data in complex scenes. Its goal is to recover dense panoramic depths from raw sparse ones and panoramic RGB images. To deal with the PDC task, we train a deep network that takes both depth and image as inputs for the dense panoramic depth recovery. However, it needs to face a challenging optimization problem of the network parameters due to its non-convex objective function. To address this problem, we propose a simple yet effective approach termed M{^3}PT: multi-modal masked pre-training. Specifically, during pre-training, we simultaneously cover up patches of the panoramic RGB image and sparse depth by shared random mask, then reconstruct the sparse depth in the masked regions. To our best knowledge, it is the first time that we show the effectiveness of masked pre-training in a multi-modal vision task, instead of the single-modal task resolved by masked autoencoders (MAE). Different from MAE where fine-tuning completely discards the decoder part of pre-training, there is no architectural difference between the pre-training and fine-tuning stages in our M$^{3}$PT as they only differ in the prediction density, which potentially makes the transfer learning more convenient and effective. Extensive experiments verify the effectiveness of M{^3}PT on three panoramic datasets. Notably, we improve the state-of-the-art baselines by averagely 26.2% in RMSE, 51.7% in MRE, 49.7% in MAE, and 37.5% in RMSElog on three benchmark datasets. Codes and pre-trained models are available at https://github.com/anonymoustbd/MMMPT.", "paper_url": "http://arxiv.org/abs/2203.09855v1", "pdf_url": "http://arxiv.org/pdf/2203.09855v1", "repo_url": null}, "2203.09825": {"publish_time": "2022-03-18", "title": "AdaVocoder: Adaptive Vocoder for Custom Voice", "author": "Xin Yuan et.al.", "abstract": "Custom voice is to construct a personal speech synthesis system by adapting the source speech synthesis model to the target model through the target few recordings. The solution to constructing a custom voice is to combine an adaptive acoustic model with a robust vocoder. However, training a robust vocoder usually requires a multi-speaker dataset, which should include various age groups and various timbres, so that the trained vocoder can be used for unseen speakers. Collecting such a multi-speaker dataset is difficult, and the dataset distribution always has a mismatch with the distribution of the target speaker dataset. This paper proposes an adaptive vocoder for custom voice from another novel perspective to solve the above problems. The adaptive vocoder mainly uses a cross-domain consistency loss to solve the overfitting problem encountered by the GAN-based neural vocoder in the transfer learning of few-shot scenes. We construct two adaptive vocoders, AdaMelGAN and AdaHiFi-GAN. First, We pre-train the source vocoder model on AISHELL3 and CSMSC datasets, respectively. Then, fine-tune it on the internal dataset VXI-children with few adaptation data. The empirical results show that a high-quality custom voice system can be built by combining a adaptive acoustic model with a adaptive vocoder.", "paper_url": "http://arxiv.org/abs/2203.09825v1", "pdf_url": "http://arxiv.org/pdf/2203.09825v1", "repo_url": null}, "2203.09777": {"publish_time": "2022-03-18", "title": "Transferable Class-Modelling for Decentralized Source Attribution of GAN-Generated Images", "author": "Brandon B. G. Khoo et.al.", "abstract": "GAN-generated deepfakes as a genre of digital images are gaining ground as both catalysts of artistic expression and malicious forms of deception, therefore demanding systems to enforce and accredit their ethical use. Existing techniques for the source attribution of synthetic images identify subtle intrinsic fingerprints using multiclass classification neural nets limited in functionality and scalability. Hence, we redefine the deepfake detection and source attribution problems as a series of related binary classification tasks. We leverage transfer learning to rapidly adapt forgery detection networks for multiple independent attribution problems, by proposing a semi-decentralized modular design to solve them simultaneously and efficiently. Class activation mapping is also demonstrated as an effective means of feature localization for model interpretation. Our models are determined via experimentation to be competitive with current benchmarks, and capable of decent performance on human portraits in ideal conditions. Decentralized fingerprint-based attribution is found to retain validity in the presence of novel sources, but is more susceptible to type II errors that intensify with image perturbations and attributive uncertainty. We describe both our conceptual framework and model prototypes for further enhancement when investigating the technical limits of reactive deepfake attribution.", "paper_url": "http://arxiv.org/abs/2203.09777v1", "pdf_url": "http://arxiv.org/pdf/2203.09777v1", "repo_url": "https://github.com/quarxilon/generator_attribution"}, "2203.11096": {"publish_time": "2022-03-21", "title": "CLIP meets GamePhysics: Towards bug identification in gameplay videos using zero-shot transfer learning", "author": "Mohammad Reza Taesiri et.al.", "abstract": "Gameplay videos contain rich information about how players interact with the game and how the game responds. Sharing gameplay videos on social media platforms, such as Reddit, has become a common practice for many players. Often, players will share gameplay videos that showcase video game bugs. Such gameplay videos are software artifacts that can be utilized for game testing, as they provide insight for bug analysis. Although large repositories of gameplay videos exist, parsing and mining them in an effective and structured fashion has still remained a big challenge. In this paper, we propose a search method that accepts any English text query as input to retrieve relevant videos from large repositories of gameplay videos. Our approach does not rely on any external information (such as video metadata); it works solely based on the content of the video. By leveraging the zero-shot transfer capabilities of the Contrastive Language-Image Pre-Training (CLIP) model, our approach does not require any data labeling or training. To evaluate our approach, we present the $\\texttt{GamePhysics}$ dataset consisting of 26,954 videos from 1,873 games, that were collected from the GamePhysics section on the Reddit website. Our approach shows promising results in our extensive analysis of simple queries, compound queries, and bug queries, indicating that our approach is useful for object and event detection in gameplay videos. An example application of our approach is as a gameplay video search engine to aid in reproducing video game bugs. Please visit the following link for the code and the data: $\\href{https://asgaardlab.github.io/CLIPxGamePhysics/}{\\text{asgaardlab.github.io/CLIPxGamePhysics/}}$", "paper_url": "http://arxiv.org/abs/2203.11096v1", "pdf_url": "http://arxiv.org/pdf/2203.11096v1", "repo_url": null}, "2203.11008": {"publish_time": "2022-03-21", "title": "Transformer-based HTR for Historical Documents", "author": "Phillip Benjamin Str\u00f6bel et.al.", "abstract": "We apply the TrOCR framework to real-world, historical manuscripts and show that TrOCR per se is a strong model, ideal for transfer learning. TrOCR has been trained on English only, but it can adapt to other languages that use the Latin alphabet fairly easily and with little training material. We compare TrOCR against a SOTA HTR framework (Transkribus) and show that it can beat such systems. This finding is essential since Transkribus performs best when it has access to baseline information, which is not needed at all to fine-tune TrOCR.", "paper_url": "http://arxiv.org/abs/2203.11008v1", "pdf_url": "http://arxiv.org/pdf/2203.11008v1", "repo_url": null}, "2203.10546": {"publish_time": "2022-03-20", "title": "Towards 3D Scene Understanding by Referring Synthetic Models", "author": "Runnan Chen et.al.", "abstract": "Promising performance has been achieved for visual perception on the point cloud. However, the current methods typically rely on labour-extensive annotations on the scene scans. In this paper, we explore how synthetic models alleviate the real scene annotation burden, i.e., taking the labelled 3D synthetic models as reference for supervision, the neural network aims to recognize specific categories of objects on a real scene scan (without scene annotation for supervision). The problem studies how to transfer knowledge from synthetic 3D models to real 3D scenes and is named Referring Transfer Learning (RTL). The main challenge is solving the model-to-scene (from a single model to the scene) and synthetic-to-real (from synthetic model to real scene's object) gap between the synthetic model and the real scene. To this end, we propose a simple yet effective framework to perform two alignment operations. First, physical data alignment aims to make the synthetic models cover the diversity of the scene's objects with data processing techniques. Then a novel \\textbf{convex-hull regularized feature alignment} introduces learnable prototypes to project the point features of both synthetic models and real scenes to a unified feature space, which alleviates the domain gap. These operations ease the model-to-scene and synthetic-to-real difficulty for a network to recognize the target objects on a real unseen scene. Experiments show that our method achieves the average mAP of 46.08\\% and 55.49\\% on the ScanNet and S3DIS datasets by learning the synthetic models from the ModelNet dataset. Code will be publicly available.", "paper_url": "http://arxiv.org/abs/2203.10546v1", "pdf_url": "http://arxiv.org/pdf/2203.10546v1", "repo_url": null}, "2203.10456": {"publish_time": "2022-03-20", "title": "simCrossTrans: A Simple Cross-Modality Transfer Learning for Object Detection with ConvNets or Vision Transformers", "author": "Xiaoke Shen et.al.", "abstract": "Transfer learning is widely used in computer vision (CV), natural language processing (NLP) and achieves great success. Most transfer learning systems are based on the same modality (e.g. RGB image in CV and text in NLP). However, the cross-modality transfer learning (CMTL) systems are scarce. In this work, we study CMTL from 2D to 3D sensor to explore the upper bound performance of 3D sensor only systems, which play critical roles in robotic navigation and perform well in low light scenarios. While most CMTL pipelines from 2D to 3D vision are complicated and based on Convolutional Neural Networks (ConvNets), ours is easy to implement, expand and based on both ConvNets and Vision transformers(ViTs): 1) By converting point clouds to pseudo-images, we can use an almost identical network from pre-trained models based on 2D images. This makes our system easy to implement and expand. 2) Recently ViTs have been showing good performance and robustness to occlusions, one of the key reasons for poor performance of 3D vision systems. We explored both ViT and ConvNet with similar model sizes to investigate the performance difference. We name our approach simCrossTrans: simple cross-modality transfer learning with ConvNets or ViTs. Experiments on SUN RGB-D dataset show: with simCrossTrans we achieve $13.2\\%$ and $16.1\\%$ absolute performance gain based on ConvNets and ViTs separately. We also observed the ViTs based performs $9.7\\%$ better than the ConvNets one, showing the power of simCrossTrans with ViT. simCrossTrans with ViTs surpasses the previous state-of-the-art (SOTA) by a large margin of $+15.4\\%$ mAP50. Compared with the previous 2D detection SOTA based RGB images, our depth image only system only has a $1\\%$ gap. The code, training/inference logs and models are publicly available at https://github.com/liketheflower/simCrossTrans", "paper_url": "http://arxiv.org/abs/2203.10456v1", "pdf_url": "http://arxiv.org/pdf/2203.10456v1", "repo_url": null}, "2203.10425": {"publish_time": "2022-03-20", "title": "A Study on Robustness to Perturbations for Representations of Environmental Sound", "author": "Sangeeta Srivastava et.al.", "abstract": "Many audio applications, such as environmental sound analysis, are increasingly using general-purpose audio representations for transfer learning. The robustness of such representations has been determined by evaluating them across a variety of domains and applications. However, it is unclear how the application-specific evaluation can be utilized to predict the impact of variability in real-world deployments caused by myriad microphones' range and acoustic conditions, commonly known as \\textit{channel effects}. In this paper, we integrate the results of various distance metrics with downstream performance to make a more informed prediction of how robust the representations or embeddings are to the audio channel effects. To accomplish this, we use two embeddings, YAMNet and OpenL$^3$, and three distance metrics to quantify the change in the embeddings when injecting perturbations to the audio signal that imitate channel effects. In monophonic (UrbanSound8K) and polyphonic (SONYC UST) data, we show a combination of two distances, Fr\\'echet Audio Distance (FAD) and Cophenetic Correlation Distance (CPCD), correlates well with the effects of perturbations. We further discuss the limitations of each distance measure.", "paper_url": "http://arxiv.org/abs/2203.10425v1", "pdf_url": "http://arxiv.org/pdf/2203.10425v1", "repo_url": null}, "2203.11856": {"publish_time": "2022-03-22", "title": "A Computational Approach to Understand Mental Health from Reddit: Knowledge-aware Multitask Learning Framework", "author": "Usha Lokala et.al.", "abstract": "Analyzing gender is critical to study mental health (MH) support in CVD (cardiovascular disease). The existing studies on using social media for extracting MH symptoms consider symptom detection and tend to ignore user context, disease, or gender. The current study aims to design and evaluate a system to capture how MH symptoms associated with CVD are expressed differently with the gender on social media. We observe that the reliable detection of MH symptoms expressed by persons with heart disease in user posts is challenging because of the co-existence of (dis)similar MH symptoms in one post and due to variation in the description of symptoms based on gender. We collect a corpus of $150k$ items (posts and comments) annotated using the subreddit labels and transfer learning approaches. We propose GeM, a novel task-adaptive multi-task learning approach to identify the MH symptoms in CVD patients based on gender. Specifically, we adapt a knowledge-assisted RoBERTa based bi-encoder model to capture CVD-related MH symptoms. Moreover, it enhances the reliability for differentiating the gender language in MH symptoms when compared to the state-of-art language models. Our model achieves high (statistically significant) performance and predicts four labels of MH issues and two gender labels, which outperforms RoBERTa, improving the recall by 2.14% on the symptom identification task and by 2.55% on the gender identification task.", "paper_url": "http://arxiv.org/abs/2203.11856v1", "pdf_url": "http://arxiv.org/pdf/2203.11856v1", "repo_url": null}, "2203.11562": {"publish_time": "2022-03-22", "title": "A Text-to-Speech Pipeline, Evaluation Methodology, and Initial Fine-Tuning Results for Child Speech Synthesis", "author": "Rishabh Jain et.al.", "abstract": "Speech synthesis has come a long way as current text-to-speech (TTS) models can now generate natural human-sounding speech. However, most of the TTS research focuses on using adult speech data and there has been very limited work done on child speech synthesis. This study developed and validated a training pipeline for fine-tuning state-of-the-art (SOTA) neural TTS models using child speech datasets. This approach adopts a multispeaker TTS retuning workflow to provide a transfer-learning pipeline. A publicly available child speech dataset was cleaned to provide a smaller subset of approximately 19 hours, which formed the basis of our fine-tuning experiments. Both subjective and objective evaluations were performed using a pretrained MOSNet for objective evaluation and a novel subjective framework for mean opinion score (MOS) evaluations. Subjective evaluations achieved the MOS of 3.92 for speech intelligibility, 3.85 for voice naturalness, and 3.96 for voice consistency. Objective evaluation using a pretrained MOSNet showed a strong correlation between real and synthetic child voices. The final trained model was able to synthesize child-like speech from reference audio samples as short as 5 seconds.", "paper_url": "http://arxiv.org/abs/2203.11562v1", "pdf_url": "http://arxiv.org/pdf/2203.11562v1", "repo_url": null}, "2203.11544": {"publish_time": "2022-03-22", "title": "Visuo-Haptic Object Perception for Robots: An Overview", "author": "Nicol\u00e1s Navarro-Guerrero et.al.", "abstract": "This article summarizes the current state of multimodal object perception for robotic applications. It covers aspects of biological inspiration, sensor technologies, data sets, and sensory data processing for object recognition and grasping. Firstly, the biological basis of multimodal object perception is outlined. Then the sensing technologies and data collection strategies are discussed. Next, an introduction to the main computational aspects is presented, highlighting a few representative articles for each main application area, including object recognition, object manipulation and grasping, texture recognition, and transfer learning. Finally, informed by the current advancements in each area, this article outlines promising new research directions.", "paper_url": "http://arxiv.org/abs/2203.11544v1", "pdf_url": "http://arxiv.org/pdf/2203.11544v1", "repo_url": null}, "2203.11542": {"publish_time": "2022-03-22", "title": "Mask Usage Recognition using Vision Transformer with Transfer Learning and Data Augmentation", "author": "Hensel Donato Jahja et.al.", "abstract": "The COVID-19 pandemic has disrupted various levels of society. The use of masks is essential in preventing the spread of COVID-19 by identifying an image of a person using a mask. Although only 23.1% of people use masks correctly, Artificial Neural Networks (ANN) can help classify the use of good masks to help slow the spread of the Covid-19 virus. However, it requires a large dataset to train an ANN that can classify the use of masks correctly. MaskedFace-Net is a suitable dataset consisting of 137016 digital images with 4 class labels, namely Mask, Mask Chin, Mask Mouth Chin, and Mask Nose Mouth. Mask classification training utilizes Vision Transformers (ViT) architecture with transfer learning method using pre-trained weights on ImageNet-21k, with random augmentation. In addition, the hyper-parameters of training of 20 epochs, an Stochastic Gradient Descent (SGD) optimizer with a learning rate of 0.03, a batch size of 64, a Gaussian Cumulative Distribution (GeLU) activation function, and a Cross-Entropy loss function are used to be applied on the training of three architectures of ViT, namely Base-16, Large-16, and Huge-14. Furthermore, comparisons of with and without augmentation and transfer learning are conducted. This study found that the best classification is transfer learning and augmentation using ViT Huge-14. Using this method on MaskedFace-Net dataset, the research reaches an accuracy of 0.9601 on training data, 0.9412 on validation data, and 0.9534 on test data. This research shows that training the ViT model with data augmentation and transfer learning improves classification of the mask usage, even better than convolutional-based Residual Network (ResNet).", "paper_url": "http://arxiv.org/abs/2203.11542v1", "pdf_url": "http://arxiv.org/pdf/2203.11542v1", "repo_url": null}, "2203.11461": {"publish_time": "2022-03-22", "title": "Locally Adaptive Transfer Learning Algorithms for Large-Scale Multiple Testing", "author": "Ziyi Liang et.al.", "abstract": "Transfer learning has enjoyed increasing popularity in a range of big data applications. In the context of large-scale multiple testing, the goal is to extract and transfer knowledge learned from related source domains to improve the accuracy of simultaneously testing of a large number of hypotheses in the target domain. This article develops a locally adaptive transfer learning algorithm (LATLA) for transfer learning for multiple testing. In contrast with existing covariate-assisted multiple testing methods that require the auxiliary covariates to be collected alongside the primary data on the same testing units, LATLA provides a principled and generic transfer learning framework that is capable of incorporating multiple samples of auxiliary data from related source domains, possibly in different dimensions/structures and from diverse populations. Both the theoretical and numerical results show that LATLA controls the false discovery rate and outperforms existing methods in power. LATLA is illustrated through an application to genome-wide association studies for the identification of disease-associated SNPs by cross-utilizing the auxiliary data from a related linkage analysis.", "paper_url": "http://arxiv.org/abs/2203.11461v1", "pdf_url": "http://arxiv.org/pdf/2203.11461v1", "repo_url": null}, "2203.12443": {"publish_time": "2022-03-23", "title": "Machine learning aided atomic structure identification of interfacial ionic hydrates from atomic force microscopy images", "author": "Binze Tang et.al.", "abstract": "Relevant to broad applied fields and natural processes, interfacial ionic hydrates has been widely studied by ultrahigh-resolution atomic force microscopy (AFM). However, the complex relationship between AFM signal and the investigated system makes it difficult to determine the atomic structure of such complex system from AFM images alone. Using machine learning, we achieved precise identification of the atomic structures of interfacial water/ionic hydrates based on AFM images, including the position of each atom and the orientation of water molecules. Furthermore, it was found that structure prediction of ionic hydrates can be achieved cost-effectively by transfer learning using neural network (NN) trained with easily available interfacial water data. Thus, this work provides an efficient and economical methodology which not only opens up avenues to determine atomic structures of more complex systems from AFM images, but may also help to interpret other science-wide studies involving sophisticated experimental results.", "paper_url": "http://arxiv.org/abs/2203.12443v1", "pdf_url": "http://arxiv.org/pdf/2203.12443v1", "repo_url": null}, "2203.13248": {"publish_time": "2022-03-24", "title": "Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer", "author": "Shuai Yang et.al.", "abstract": "Recent studies on StyleGAN show high performance on artistic portrait generation by transfer learning with limited data. In this paper, we explore more challenging exemplar-based high-resolution portrait style transfer by introducing a novel DualStyleGAN with flexible control of dual styles of the original face domain and the extended artistic portrait domain. Different from StyleGAN, DualStyleGAN provides a natural way of style transfer by characterizing the content and style of a portrait with an intrinsic style path and a new extrinsic style path, respectively. The delicately designed extrinsic style path enables our model to modulate both the color and complex structural styles hierarchically to precisely pastiche the style example. Furthermore, a novel progressive fine-tuning scheme is introduced to smoothly transform the generative space of the model to the target domain, even with the above modifications on the network architecture. Experiments demonstrate the superiority of DualStyleGAN over state-of-the-art methods in high-quality portrait style transfer and flexible style control.", "paper_url": "http://arxiv.org/abs/2203.13248v1", "pdf_url": "http://arxiv.org/pdf/2203.13248v1", "repo_url": "https://github.com/williamyang1991/DualStyleGAN"}, "2203.12881": {"publish_time": "2022-03-24", "title": "Can Unsupervised Knowledge Transfer from Social Discussions Help Argument Mining?", "author": "Subhabrata Dutta et.al.", "abstract": "Identifying argument components from unstructured texts and predicting the relationships expressed among them are two primary steps of argument mining. The intrinsic complexity of these tasks demands powerful learning models. While pretrained Transformer-based Language Models (LM) have been shown to provide state-of-the-art results over different NLP tasks, the scarcity of manually annotated data and the highly domain-dependent nature of argumentation restrict the capabilities of such models. In this work, we propose a novel transfer learning strategy to overcome these challenges. We utilize argumentation-rich social discussions from the ChangeMyView subreddit as a source of unsupervised, argumentative discourse-aware knowledge by finetuning pretrained LMs on a selectively masked language modeling task. Furthermore, we introduce a novel prompt-based strategy for inter-component relation prediction that compliments our proposed finetuning method while leveraging on the discourse context. Exhaustive experiments show the generalization capability of our method on these two tasks over within-domain as well as out-of-domain datasets, outperforming several existing and employed strong baselines.", "paper_url": "http://arxiv.org/abs/2203.12881v1", "pdf_url": "http://arxiv.org/pdf/2203.12881v1", "repo_url": "https://github.com/jeevesh8/arg_mining"}, "2203.12803": {"publish_time": "2022-03-24", "title": "When Accuracy Meets Privacy: Two-Stage Federated Transfer Learning Framework in Classification of Medical Images on Limited Data: A COVID-19 Case Study", "author": "Alexandros Shikun Zhang et.al.", "abstract": "COVID-19 pandemic has spread rapidly and caused a shortage of global medical resources. The efficiency of COVID-19 diagnosis has become highly significant. As deep learning and convolutional neural network (CNN) has been widely utilized and been verified in analyzing medical images, it has become a powerful tool for computer-assisted diagnosis. However, there are two most significant challenges in medical image classification with the help of deep learning and neural networks, one of them is the difficulty of acquiring enough samples, which may lead to model overfitting. Privacy concerns mainly bring the other challenge since medical-related records are often deemed patients' private information and protected by laws such as GDPR and HIPPA. Federated learning can ensure the model training is decentralized on different devices and no data is shared among them, which guarantees privacy. However, with data located on different devices, the accessible data of each device could be limited. Since transfer learning has been verified in dealing with limited data with good performance, therefore, in this paper, We made a trial to implement federated learning and transfer learning techniques using CNNs to classify COVID-19 using lung CT scans. We also explored the impact of dataset distribution at the client-side in federated learning and the number of training epochs a model is trained. Finally, we obtained very high performance with federated learning, demonstrating our success in leveraging accuracy and privacy.", "paper_url": "http://arxiv.org/abs/2203.12803v1", "pdf_url": "http://arxiv.org/pdf/2203.12803v1", "repo_url": null}, "2203.13718": {"publish_time": "2022-03-25", "title": "Digital Fingerprinting of Microstructures", "author": "Michael D. White et.al.", "abstract": "Finding efficient means of fingerprinting microstructural information is a critical step towards harnessing data-centric machine learning approaches. A statistical framework is systematically developed for compressed characterisation of a population of images, which includes some classical computer vision methods as special cases. The focus is on materials microstructure. The ultimate purpose is to rapidly fingerprint sample images in the context of various high-throughput design/make/test scenarios. This includes, but is not limited to, quantification of the disparity between microstructures for quality control, classifying microstructures, predicting materials properties from image data and identifying potential processing routes to engineer new materials with specific properties. Here, we consider microstructure classification and utilise the resulting features over a range of related machine learning tasks, namely supervised, semi-supervised, and unsupervised learning.   The approach is applied to two distinct datasets to illustrate various aspects and some recommendations are made based on the findings. In particular, methods that leverage transfer learning with convolutional neural networks (CNNs), pretrained on the ImageNet dataset, are generally shown to outperform other methods. Additionally, dimensionality reduction of these CNN-based fingerprints is shown to have negligible impact on classification accuracy for the supervised learning approaches considered. In situations where there is a large dataset with only a handful of images labelled, graph-based label propagation to unlabelled data is shown to be favourable over discarding unlabelled data and performing supervised learning. In particular, label propagation by Poisson learning is shown to be highly effective at low label rates.", "paper_url": "http://arxiv.org/abs/2203.13718v1", "pdf_url": "http://arxiv.org/pdf/2203.13718v1", "repo_url": null}, "2203.13628": {"publish_time": "2022-03-25", "title": "DeLoRes: Decorrelating Latent Spaces for Low-Resource Audio Representation Learning", "author": "Sreyan Ghosh et.al.", "abstract": "Inspired by the recent progress in self-supervised learning for computer vision, in this paper, through the DeLoRes learning framework, we introduce two new general-purpose audio representation learning approaches, the DeLoRes-S and DeLoRes-M. Our main objective is to make our network learn representations in a resource-constrained setting (both data and compute), that can generalize well across a diverse set of downstream tasks. Inspired from the Barlow Twins objective function, we propose to learn embeddings that are invariant to distortions of an input audio sample, while making sure that they contain non-redundant information about the sample. To achieve this, we measure the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of an audio segment sampled from an audio file and make it as close to the identity matrix as possible. We call this the DeLoRes learning framework, which we employ in different fashions with the DeLoRes-S and DeLoRes-M. We use a combination of a small subset of the large-scale AudioSet dataset and FSD50K for self-supervised learning and are able to learn with less than half the parameters compared to state-of-the-art algorithms. For evaluation, we transfer these learned representations to 11 downstream classification tasks, including speech, music, and animal sounds, and achieve state-of-the-art results on 7 out of 11 tasks on linear evaluation with DeLoRes-M and show competitive results with DeLoRes-S, even when pre-trained using only a fraction of the total data when compared to prior art. Our transfer learning evaluation setup also shows extremely competitive results for both DeLoRes-S and DeLoRes-M, with DeLoRes-M achieving state-of-the-art in 4 tasks.", "paper_url": "http://arxiv.org/abs/2203.13628v1", "pdf_url": "http://arxiv.org/pdf/2203.13628v1", "repo_url": null}, "2203.13461": {"publish_time": "2022-03-25", "title": "Interpretation of Chest x-rays affected by bullets using deep transfer learning", "author": "Shaheer Khan et.al.", "abstract": "The potential of deep learning, especially in medical imaging, initiated astonishing results and improved the methodologies after every passing day. Deep learning in radiology provides the opportunity to classify, detect and segment different diseases automatically. In the proposed study, we worked on a non-trivial aspect of medical imaging where we classified and localized the X-Rays affected by bullets. We tested Images on different classification and localization models to get considerable accuracy. The replicated data set used in the study was replicated on different images of chest X-Rays. The proposed model worked not only on chest radiographs but other body organs X-rays like leg, abdomen, head, even the training dataset based on chest radiographs. Custom models have been used for classification and localization purposes after tuning parameters. Finally, the results of our findings manifested using different frameworks. This might assist the research enlightening towards this field. To the best of our knowledge, this is the first study on the detection and classification of radiographs affected by bullets using deep learning.", "paper_url": "http://arxiv.org/abs/2203.13461v1", "pdf_url": "http://arxiv.org/pdf/2203.13461v1", "repo_url": null}, "2203.13453": {"publish_time": "2022-03-25", "title": "CNN LEGO: Disassembling and Assembling Convolutional Neural Network", "author": "Jiacong Hu et.al.", "abstract": "Convolutional Neural Network (CNN), which mimics human visual perception mechanism, has been successfully used in many computer vision areas. Some psychophysical studies show that the visual perception mechanism synchronously processes the form, color, movement, depth, etc., in the initial stage [7,20] and then integrates all information for final recognition [38]. What's more, the human visual system [20] contains different subdivisions or different tasks. Inspired by the above visual perception mechanism, we investigate a new task, termed as Model Disassembling and Assembling (MDA-Task), which can disassemble the deep models into independent parts and assemble those parts into a new deep model without performance cost like playing LEGO toys. To this end, we propose a feature route attribution technique (FRAT) for disassembling CNN classifiers in this paper. In FRAT, the positive derivatives of predicted class probability w.r.t. the feature maps are adopted to locate the critical features in each layer. Then, relevance analysis between the critical features and preceding/subsequent parameter layers is adopted to bridge the route between two adjacent parameter layers. In the assembling phase, class-wise components of each layer are assembled into a new deep model for a specific task. Extensive experiments demonstrate that the assembled CNN classifier can achieve close accuracy with the original classifier without any fine-tune, and excess original performance with one-epoch fine-tune. What's more, we also conduct massive experiments to verify the broad application of MDA-Task on model decision route visualization, model compression, knowledge distillation, transfer learning, incremental learning, and so on.", "paper_url": "http://arxiv.org/abs/2203.13453v1", "pdf_url": "http://arxiv.org/pdf/2203.13453v1", "repo_url": null}, "2203.13421": {"publish_time": "2022-03-25", "title": "Learning Losses for Strategic Classification", "author": "Tosca Lechner et.al.", "abstract": "Strategic classification, i.e. classification under possible strategic manipulations of features, has received a lot of attention from both the machine learning and the game theory community. Most works focus on analysing properties of the optimal decision rule under such manipulations. In our work we take a learning theoretic perspective, focusing on the sample complexity needed to learn a good decision rule which is robust to strategic manipulation. We perform this analysis by introducing a novel loss function, the \\emph{strategic manipulation loss}, which takes into account both the accuracy of the final decision rule and its vulnerability to manipulation. We analyse the sample complexity for a known graph of possible manipulations in terms of the complexity of the function class and the manipulation graph. Additionally, we initialize the study of learning under unknown manipulation capabilities of the involved agents. Using techniques from transfer learning theory, we define a similarity measure for manipulation graphs and show that learning outcomes are robust with respect to small changes in the manipulation graph. Lastly, we analyse the (sample complexity of) learning of the manipulation capability of agents with respect to this similarity measure, providing novel guarantees for strategic classification with respect to an unknown manipulation graph.", "paper_url": "http://arxiv.org/abs/2203.13421v1", "pdf_url": "http://arxiv.org/pdf/2203.13421v1", "repo_url": null}, "2203.14940": {"publish_time": "2022-03-28", "title": "Learning to Prompt for Open-Vocabulary Object Detection with Vision-Language Model", "author": "Yu Du et.al.", "abstract": "Recently, vision-language pre-training shows great potential in open-vocabulary object detection, where detectors trained on base classes are devised for detecting new classes. The class text embedding is firstly generated by feeding prompts to the text encoder of a pre-trained vision-language model. It is then used as the region classifier to supervise the training of a detector. The key element that leads to the success of this model is the proper prompt, which requires careful words tuning and ingenious design. To avoid laborious prompt engineering, there are some prompt representation learning methods being proposed for the image classification task, which however can only be sub-optimal solutions when applied to the detection task. In this paper, we introduce a novel method, detection prompt (DetPro), to learn continuous prompt representations for open-vocabulary object detection based on the pre-trained vision-language model. Different from the previous classification-oriented methods, DetPro has two highlights: 1) a background interpretation scheme to include the proposals in image background into the prompt training; 2) a context grading scheme to separate proposals in image foreground for tailored prompt training. We assemble DetPro with ViLD, a recent state-of-the-art open-world object detector, and conduct experiments on the LVIS as well as transfer learning on the Pascal VOC, COCO, Objects365 datasets. Experimental results show that our DetPro outperforms the baseline ViLD in all settings, e.g., +3.4 APbox and +3.0 APmask improvements on the novel classes of LVIS. Code and models are available at https://github.com/dyabel/detpro.", "paper_url": "http://arxiv.org/abs/2203.14940v1", "pdf_url": "http://arxiv.org/pdf/2203.14940v1", "repo_url": "https://github.com/dyabel/detpro"}, "2203.14865": {"publish_time": "2022-03-28", "title": "Towards Transferable Speech Emotion Representation: On loss functions for cross-lingual latent representations", "author": "Sneha Das et.al.", "abstract": "In recent years, speech emotion recognition (SER) has been used in wide ranging applications, from healthcare to the commercial sector. In addition to signal processing approaches, methods for SER now also use deep learning techniques which provide transfer learning possibilities. However, generalizing over languages, corpora and recording conditions is still an open challenge. In this work we address this gap by exploring loss functions that aid in transferability, specifically to non-tonal languages. We propose a variational autoencoder (VAE) with KL annealing and a semi-supervised VAE to obtain more consistent latent embedding distributions across data sets. To ensure transferability, the distribution of the latent embedding should be similar across non-tonal languages (data sets). We start by presenting a low-complexity SER based on a denoising-autoencoder, which achieves an unweighted classification accuracy of over 52.09% for four-class emotion classification. This performance is comparable to that of similar baseline methods. Following this, we employ a VAE, the semi-supervised VAE and the VAE with KL annealing to obtain a more regularized latent space. We show that while the DAE has the highest classification accuracy among the methods, the semi-supervised VAE has a comparable classification accuracy and a more consistent latent embedding distribution over data sets.", "paper_url": "http://arxiv.org/abs/2203.14865v1", "pdf_url": "http://arxiv.org/pdf/2203.14865v1", "repo_url": null}, "2203.14415": {"publish_time": "2022-03-27", "title": "Mugs: A Multi-Granular Self-Supervised Learning Framework", "author": "Pan Zhou et.al.", "abstract": "In self-supervised learning, multi-granular features are heavily desired though rarely investigated, as different downstream tasks (e.g., general and fine-grained classification) often require different or multi-granular features, e.g.~fine- or coarse-grained one or their mixture. In this work, for the first time, we propose an effective MUlti-Granular Self-supervised learning (Mugs) framework to explicitly learn multi-granular visual features. Mugs has three complementary granular supervisions: 1) an instance discrimination supervision (IDS), 2) a novel local-group discrimination supervision (LGDS), and 3) a group discrimination supervision (GDS). IDS distinguishes different instances to learn instance-level fine-grained features. LGDS aggregates features of an image and its neighbors into a local-group feature, and pulls local-group features from different crops of the same image together and push them away for others. It provides complementary instance supervision to IDS via an extra alignment on local neighbors, and scatters different local-groups separately to increase discriminability. Accordingly, it helps learn high-level fine-grained features at a local-group level. Finally, to prevent similar local-groups from being scattered randomly or far away, GDS brings similar samples close and thus pulls similar local-groups together, capturing coarse-grained features at a (semantic) group level. Consequently, Mugs can capture three granular features that often enjoy higher generality on diverse downstream tasks over single-granular features, e.g.~instance-level fine-grained features in contrastive learning. By only pretraining on ImageNet-1K, Mugs sets new SoTA linear probing accuracy 82.1$\\%$ on ImageNet-1K and improves previous SoTA by $1.1\\%$. It also surpasses SoTAs on other tasks, e.g. transfer learning, detection and segmentation.", "paper_url": "http://arxiv.org/abs/2203.14415v1", "pdf_url": "http://arxiv.org/pdf/2203.14415v1", "repo_url": "https://github.com/sail-sg/mugs"}, "2203.14298": {"publish_time": "2022-03-27", "title": "Benchmarking Algorithms for Automatic License Plate Recognition", "author": "Marcel Del Castillo Velarde et.al.", "abstract": "We evaluated a lightweight Convolutional Neural Network (CNN) called LPRNet [1] for automatic License Plate Recognition (LPR). We evaluated the algorithm on two datasets, one composed of real license plate images and the other of synthetic license plate images. In addition, we compared its performance against Tesseract [2], an Optical Character Recognition engine. We measured performance based on recognition accuracy and Levenshtein Distance. LPRNet is an end-to-end framework and demonstrated robust performance on both datasets, delivering 90 and 89 percent recognition accuracy on test sets of 1000 real and synthetic license plate images, respectively. Tesseract was not trained using real license plate images and performed well only on the synthetic dataset after pre-processing steps delivering 93 percent recognition accuracy. Finally, Pareto analysis for frequency analysis of misclassified characters allowed us to find in detail which characters were the most conflicting ones according to the percentage of accumulated error. Depending on the region, license plate images possess particular characteristics. Once properly trained, LPRNet can be used to recognize characters from a specific region and dataset. Future work can focus on applying transfer learning to utilize the features learned by LPRNet and fine-tune it given a smaller, newer dataset of license plates.", "paper_url": "http://arxiv.org/abs/2203.14298v1", "pdf_url": "http://arxiv.org/pdf/2203.14298v1", "repo_url": null}, "2203.14205": {"publish_time": "2022-03-27", "title": "An Empirical Study and Comparison of Recent Few-Shot Object Detection Algorithms", "author": "Tianying Liu et.al.", "abstract": "The generic object detection (GOD) task has been successfully tackled by recent deep neural networks, trained by an avalanche of annotated training samples from some common classes. However, it is still non-trivial to generalize these object detectors to the novel long-tailed object classes, which has only few labeled training samples. To this end, the Few-Shot Object Detection (FSOD) has been topical recently, as it mimics the humans' ability of learning to learn, and intelligently transfers the learnt generic object knowledge from the common heavy-tailed, to the novel long-tailed object classes. Especially, the research in this emerging field has been flourish in the recent years with various benchmarks, backbones, and methodologies proposed. To review these FSOD works, there are several insightful FSOD survey articles that systematically study and compare them as the groups of fine-tuning/transfer learning, and meta-learning methods. In contrast, we compare these FSOD algorithms from the new perspective and taxonomy of their contributions, i.e., data-oriented, model-oriented, and algorithm oriented ones. Thus, an empirical study and comparison has been conducted on the recent achievements of FSOD. Furthermore, we also analyze the technical challenges, the merits and demerits of these methods, and envision the future directions of FSOD. Specifically, we give an overview of FSOD, including the problem definition, common datasets, and evaluation protocols. A new taxonomy is then proposed based on the role of prior knowledge during object detection of novel classes. Following this taxonomy, we provide a systematic review of the advances in FSOD. Finally, further discussions on performance, challenges, and future directions are presented.", "paper_url": "http://arxiv.org/abs/2203.14205v1", "pdf_url": "http://arxiv.org/pdf/2203.14205v1", "repo_url": null}, "2203.15447": {"publish_time": "2022-03-29", "title": "Transfer Learning Framework for Low-Resource Text-to-Speech using a Large-Scale Unlabeled Speech Corpus", "author": "Minchan Kim et.al.", "abstract": "Training a text-to-speech (TTS) model requires a large scale text labeled speech corpus, which is troublesome to collect. In this paper, we propose a transfer learning framework for TTS that utilizes a large amount of unlabeled speech dataset for pre-training. By leveraging wav2vec2.0 representation, unlabeled speech can highly improve performance, especially in the lack of labeled speech. We also extend the proposed method to zero-shot multi-speaker TTS (ZS-TTS). The experimental results verify the effectiveness of the proposed method in terms of naturalness, intelligibility, and speaker generalization. We highlight that the single speaker TTS model fine-tuned on the only 10 minutes of labeled dataset outperforms the other baselines, and the ZS-TTS model fine-tuned on the only 30 minutes of single speaker dataset can generate the voice of the arbitrary speaker, by pre-training on unlabeled multi-speaker speech corpus.", "paper_url": "http://arxiv.org/abs/2203.15447v1", "pdf_url": "http://arxiv.org/pdf/2203.15447v1", "repo_url": null}, "2203.15297": {"publish_time": "2022-03-29", "title": "Kernel Modulation: A Parameter-Efficient Method for Training Convolutional Neural Networks", "author": "Yuhuang Hu et.al.", "abstract": "Deep Neural Networks, particularly Convolutional Neural Networks (ConvNets), have achieved incredible success in many vision tasks, but they usually require millions of parameters for good accuracy performance. With increasing applications that use ConvNets, updating hundreds of networks for multiple tasks on an embedded device can be costly in terms of memory, bandwidth, and energy. Approaches to reduce this cost include model compression and parameter-efficient models that adapt a subset of network layers for each new task. This work proposes a novel parameter-efficient kernel modulation (KM) method that adapts all parameters of a base network instead of a subset of layers. KM uses lightweight task-specialized kernel modulators that require only an additional 1.4% of the base network parameters. With multiple tasks, only the task-specialized KM weights are communicated and stored on the end-user device. We applied this method in training ConvNets for Transfer Learning and Meta-Learning scenarios. Our results show that KM delivers up to 9% higher accuracy than other parameter-efficient methods on the Transfer Learning benchmark.", "paper_url": "http://arxiv.org/abs/2203.15297v1", "pdf_url": "http://arxiv.org/pdf/2203.15297v1", "repo_url": null}, "2203.17205": {"publish_time": "2022-03-31", "title": "Leverage Your Local and Global Representations: A New Self-Supervised Learning Strategy", "author": "Tong Zhang et.al.", "abstract": "Self-supervised learning (SSL) methods aim to learn view-invariant representations by maximizing the similarity between the features extracted from different crops of the same image regardless of cropping size and content. In essence, this strategy ignores the fact that two crops may truly contain different image information, e.g., background and small objects, and thus tends to restrain the diversity of the learned representations. %To this end, the existing strategies typically employ loss functions that enforces the networks to discard part of valuable information, e.g. background and small objects, and sacrifices the diversity of representation. In this work, we address this issue by introducing a new self-supervised learning strategy, LoGo, that explicitly reasons about {\\bf Lo}cal and {\\bf G}l{\\bf o}bal crops. To achieve view invariance, LoGo encourages similarity between global crops from the same image, as well as between a global and a local crop. However, to correctly encode the fact that the content of smaller crops may differ entirely, LoGo promotes two local crops to have dissimilar representations, while being close to global crops. Our LoGo strategy can easily be applied to existing SSL methods. Our extensive experiments on a variety of datasets and using different self-supervised learning frameworks validate its superiority over existing approaches. Noticeably, we achieve better results than supervised models on transfer learning when using only $1/10$ of the data.", "paper_url": "http://arxiv.org/abs/2203.17205v1", "pdf_url": "http://arxiv.org/pdf/2203.17205v1", "repo_url": null}, "2203.17070": {"publish_time": "2022-03-31", "title": "Traffic4cast at NeurIPS 2021 - Temporal and Spatial Few-Shot Transfer Learning in Gridded Geo-Spatial Processes", "author": "Christian Eichenberger et.al.", "abstract": "The IARAI Traffic4cast competitions at NeurIPS 2019 and 2020 showed that neural networks can successfully predict future traffic conditions 1 hour into the future on simply aggregated GPS probe data in time and space bins. We thus reinterpreted the challenge of forecasting traffic conditions as a movie completion task. U-Nets proved to be the winning architecture, demonstrating an ability to extract relevant features in this complex real-world geo-spatial process. Building on the previous competitions, Traffic4cast 2021 now focuses on the question of model robustness and generalizability across time and space. Moving from one city to an entirely different city, or moving from pre-COVID times to times after COVID hit the world thus introduces a clear domain shift. We thus, for the first time, release data featuring such domain shifts. The competition now covers ten cities over 2 years, providing data compiled from over 10^12 GPS probe data. Winning solutions captured traffic dynamics sufficiently well to even cope with these complex domain shifts. Surprisingly, this seemed to require only the previous 1h traffic dynamic history and static road graph as input.", "paper_url": "http://arxiv.org/abs/2203.17070v1", "pdf_url": "http://arxiv.org/pdf/2203.17070v1", "repo_url": "https://github.com/iarai/NeurIPS2021-traffic4cast"}, "2203.17013": {"publish_time": "2022-03-31", "title": "A Temporal Learning Approach to Inpainting Endoscopic Specularities and Its effect on Image Correspondence", "author": "Rema Daher et.al.", "abstract": "Video streams are utilised to guide minimally-invasive surgery and diagnostic procedures in a wide range of procedures, and many computer assisted techniques have been developed to automatically analyse them. These approaches can provide additional information to the surgeon such as lesion detection, instrument navigation, or anatomy 3D shape modeling. However, the necessary image features to recognise these patterns are not always reliably detected due to the presence of irregular light patterns such as specular highlight reflections. In this paper, we aim at removing specular highlights from endoscopic videos using machine learning. We propose using a temporal generative adversarial network (GAN) to inpaint the hidden anatomy under specularities, inferring its appearance spatially and from neighbouring frames where they are not present in the same location. This is achieved using in-vivo data of gastric endoscopy (Hyper-Kvasir) in a fully unsupervised manner that relies on automatic detection of specular highlights. System evaluations show significant improvements to traditional methods through direct comparison as well as other machine learning techniques through an ablation study that depicts the importance of the network's temporal and transfer learning components. The generalizability of our system to different surgical setups and procedures was also evaluated qualitatively on in-vivo data of gastric endoscopy and ex-vivo porcine data (SERV-CT, SCARED). We also assess the effect of our method in computer vision tasks that underpin 3D reconstruction and camera motion estimation, namely stereo disparity, optical flow, and sparse point feature matching. These are evaluated quantitatively and qualitatively and results show a positive effect of specular highlight inpainting on these tasks in a novel comprehensive analysis.", "paper_url": "http://arxiv.org/abs/2203.17013v1", "pdf_url": "http://arxiv.org/pdf/2203.17013v1", "repo_url": null}, "2203.16926": {"publish_time": "2022-03-31", "title": "Domain Adaptation for Sparse-Data Settings: What Do We Gain by Not Using Bert?", "author": "Marina Sedinkina et.al.", "abstract": "The practical success of much of NLP depends on the availability of training data. However, in real-world scenarios, training data is often scarce, not least because many application domains are restricted and specific. In this work, we compare different methods to handle this problem and provide guidelines for building NLP applications when there is only a small amount of labeled training data available for a specific domain. While transfer learning with pre-trained language models outperforms other methods across tasks, alternatives do not perform much worse while requiring much less computational effort, thus significantly reducing monetary and environmental cost. We examine the performance tradeoffs of several such alternatives, including models that can be trained up to 175K times faster and do not require a single GPU.", "paper_url": "http://arxiv.org/abs/2203.16926v1", "pdf_url": "http://arxiv.org/pdf/2203.16926v1", "repo_url": null}, "2204.00484": {"publish_time": "2022-04-01", "title": "Proper Reuse of Image Classification Features Improves Object Detection", "author": "Cristina Vasconcelos et.al.", "abstract": "A common practice in transfer learning is to initialize the downstream model weights by pre-training on a data-abundant upstream task. In object detection specifically, the feature backbone is typically initialized with Imagenet classifier weights and fine-tuned on the object detection task. Recent works show this is not strictly necessary under longer training regimes and provide recipes for training the backbone from scratch. We investigate the opposite direction of this end-to-end training trend: we show that an extreme form of knowledge preservation -- freezing the classifier-initialized backbone -- consistently improves many different detection models, and leads to considerable resource savings. We hypothesize and corroborate experimentally that the remaining detector components capacity and structure is a crucial factor in leveraging the frozen backbone. Immediate applications of our findings include performance improvements on hard cases like detection of long-tail object classes and computational and memory resource savings that contribute to making the field more accessible to researchers with access to fewer computational resources.", "paper_url": "http://arxiv.org/abs/2204.00484v1", "pdf_url": "http://arxiv.org/pdf/2204.00484v1", "repo_url": null}, "2204.00477": {"publish_time": "2022-04-01", "title": "Autonomous crater detection on asteroids using a fully-convolutional neural network", "author": "Francesco Latorre et.al.", "abstract": "This paper shows the application of autonomous Crater Detection using the U-Net, a Fully-Convolutional Neural Network, on Ceres. The U-Net is trained on optical images of the Moon Global Morphology Mosaic based on data collected by the LRO and manual crater catalogues. The Moon-trained network will be tested on Dawn optical images of Ceres: this task is accomplished by means of a Transfer Learning (TL) approach. The trained model has been fine-tuned using 100, 500 and 1000 additional images of Ceres. The test performance was measured on 350 never before seen images, reaching a testing accuracy of 96.24%, 96.95% and 97.19%, respectively. This means that despite the intrinsic differences between the Moon and Ceres, TL works with encouraging results. The output of the U-Net contains predicted craters: it will be post-processed applying global thresholding for image binarization and a template matching algorithm to extract craters positions and radii in the pixel space. Post-processed craters will be counted and compared to the ground truth data in order to compute image segmentation metrics: precision, recall and F1 score. These indices will be computed, and their effect will be discussed for tasks such as automated crater cataloguing and optical navigation.", "paper_url": "http://arxiv.org/abs/2204.00477v1", "pdf_url": "http://arxiv.org/pdf/2204.00477v1", "repo_url": null}, "2204.00327": {"publish_time": "2022-04-01", "title": "Diverse Preference Augmentation with Multiple Domains for Cold-start Recommendations", "author": "Yan Zhang et.al.", "abstract": "Cold-start issues have been more and more challenging for providing accurate recommendations with the fast increase of users and items. Most existing approaches attempt to solve the intractable problems via content-aware recommendations based on auxiliary information and/or cross-domain recommendations with transfer learning. Their performances are often constrained by the extremely sparse user-item interactions, unavailable side information, or very limited domain-shared users. Recently, meta-learners with meta-augmentation by adding noises to labels have been proven to be effective to avoid overfitting and shown good performance on new tasks. Motivated by the idea of meta-augmentation, in this paper, by treating a user's preference over items as a task, we propose a so-called Diverse Preference Augmentation framework with multiple source domains based on meta-learning (referred to as MetaDPA) to i) generate diverse ratings in a new domain of interest (known as target domain) to handle overfitting on the case of sparse interactions, and to ii) learn a preference model in the target domain via a meta-learning scheme to alleviate cold-start issues. Specifically, we first conduct multi-source domain adaptation by dual conditional variational autoencoders and impose a Multi-domain InfoMax (MDI) constraint on the latent representations to learn domain-shared and domain-specific preference properties. To avoid overfitting, we add a Mutually-Exclusive (ME) constraint on the output of decoders to generate diverse ratings given content data. Finally, these generated diverse ratings and the original ratings are introduced into the meta-training procedure to learn a preference meta-learner, which produces good generalization ability on cold-start recommendation tasks. Experiments on real-world datasets show our proposed MetaDPA clearly outperforms the current state-of-the-art baselines.", "paper_url": "http://arxiv.org/abs/2204.00327v1", "pdf_url": "http://arxiv.org/pdf/2204.00327v1", "repo_url": null}, "2204.00132": {"publish_time": "2022-03-31", "title": "Real-Time and Robust 3D Object Detection Within Road-Side LiDARs Using Domain Adaptation", "author": "Walter Zimmer et.al.", "abstract": "This work aims to address the challenges in domain adaptation of 3D object detection using infrastructure LiDARs. We design a model DASE-ProPillars that can detect vehicles in infrastructure-based LiDARs in real-time. Our model uses PointPillars as the baseline model with additional modules to improve the 3D detection performance. To prove the effectiveness of our proposed modules in DASE-ProPillars, we train and evaluate the model on two datasets, the open source A9-Dataset and a semi-synthetic infrastructure dataset created within the Regensburg Next project. We do several sets of experiments for each module in the DASE-ProPillars detector that show that our model outperforms the SE-ProPillars baseline on the real A9 test set and a semi-synthetic A9 test set, while maintaining an inference speed of 45 Hz (22 ms). We apply domain adaptation from the semi-synthetic A9-Dataset to the semi-synthetic dataset from the Regensburg Next project by applying transfer learning and achieve a 3D mAP@0.25 of 93.49% on the Car class of the target test set using 40 recall positions.", "paper_url": "http://arxiv.org/abs/2204.00132v1", "pdf_url": "http://arxiv.org/pdf/2204.00132v1", "repo_url": null}, "2204.01626": {"publish_time": "2022-04-04", "title": "Stuttgart Open Relay Degradation Dataset (SOReDD)", "author": "Benjamin Maschler et.al.", "abstract": "Real-life industrial use cases for machine learning oftentimes involve heterogeneous and dynamic assets, processes and data, resulting in a need to continuously adapt the learning algorithm accordingly. Industrial transfer learning offers to lower the effort of such adaptation by allowing the utilization of previously acquired knowledge in solving new (variants of) tasks. Being data-driven methods, the development of industrial transfer learning algorithms naturally requires appropriate datasets for training. However, open-source datasets suitable for transfer learning training, i.e. spanning different assets, processes and data (variants), are rare. With the Stuttgart Open Relay Degradation Dataset (SOReDD) we want to offer such a dataset. It provides data on the degradation of different electromechanical relays under different operating conditions, allowing for a large number of different transfer scenarios. Although such relays themselves are usually inexpensive standard components, their failure often leads to the failure of a machine as a whole due to their role as the central power switching element of a machine. The main cost factor in the event of a relay defect is therefore not the relay itself, but the reduced machine availability. It is therefore desirable to predict relay degradation as accurately as possible for specific applications in order to be able to replace relays in good time and avoid unplanned machine downtimes. Nevertheless, data-driven failure prediction for electromechanical relays faces the challenge that relay degradation behavior is highly dependent on the operating conditions, high-resolution measurement data on relay degradation behavior is only collected in rare cases, and such data can then only cover a fraction of the possible operating environments. Relays are thus representative of many other central standard components in automation technology.", "paper_url": "http://arxiv.org/abs/2204.01626v1", "pdf_url": "http://arxiv.org/pdf/2204.01626v1", "repo_url": null}, "2204.01620": {"publish_time": "2022-04-04", "title": "Towards Deep Industrial Transfer Learning: Clustering for Transfer Case Selection", "author": "Benjamin Maschler et.al.", "abstract": "Industrial transfer learning increases the adaptability of deep learning algorithms towards heterogenous and dynamic industrial use cases without high manual efforts. The appropriate selection of what to transfer can vastly improve a transfer's results. In this paper, a transfer case selection based upon clustering is presented. Founded on a survey of clustering algorithms, the BIRCH algorithm is selected for this purpose. It is evaluated on an industrial time series dataset from a discrete manufacturing scenario. Results underline the approaches' applicability caused by its results' reproducibility and practical indifference to sequence, size and dimensionality of (sub-)datasets to be clustered sequentially.", "paper_url": "http://arxiv.org/abs/2204.01620v1", "pdf_url": "http://arxiv.org/pdf/2204.01620v1", "repo_url": null}, "2204.01457": {"publish_time": "2022-04-04", "title": "SHiFT: An Efficient, Flexible Search Engine for Transfer Learning", "author": "Cedric Renggli et.al.", "abstract": "Transfer learning can be seen as a data- and compute-efficient alternative to training models from scratch. The emergence of rich model repositories, such as TensorFlow Hub, enables practitioners and researchers to unleash the potential of these models across a wide range of downstream tasks. As these repositories keep growing exponentially, efficiently selecting a good model for the task at hand becomes paramount. By carefully comparing various selection and search strategies, we realize that no single method outperforms the others, and hybrid or mixed strategies can be beneficial. Therefore, we propose SHiFT, the first downstream task-aware, flexible, and efficient model search engine for transfer learning. These properties are enabled by a custom query language SHiFT-QL together with a cost-based decision maker, which we empirically validate. Motivated by the iterative nature of machine learning development, we further support efficient incremental executions of our queries, which requires a careful implementation when jointly used with our optimizations.", "paper_url": "http://arxiv.org/abs/2204.01457v1", "pdf_url": "http://arxiv.org/pdf/2204.01457v1", "repo_url": "https://github.com/ds3lab/shift"}, "2204.01411": {"publish_time": "2022-04-04", "title": "Computer-Aided Extraction of Select MRI Markers of Cerebral Small Vessel Disease: A Systematic Review", "author": "Jiyang Jiang et.al.", "abstract": "Cerebral small vessel disease (CSVD) is a major vascular contributor to cognitive impairment in ageing, including dementias. Imaging remains the most promising method for in vivo studies of CSVD. To replace the subjective and laborious visual rating approaches, emerging studies have applied state-of-the-art artificial intelligence to extract imaging biomarkers of CSVD from MRI scans. We aimed to summarise published computer-aided methods to examine three imaging biomarkers of CSVD, namely cerebral microbleeds (CMB), dilated perivascular spaces (PVS), and lacunes of presumed vascular origin. Seventy-one classical image processing, classical machine learning, and deep learning studies were identified. CMB and PVS have been better studied, compared to lacunes. While good performance metrics have been achieved in local test datasets, there have not been generalisable pipelines validated in different research or clinical cohorts. Transfer learning and weak supervision techniques have been applied to accommodate the limitations in training data. Future studies could consider pooling data from multiple sources to increase diversity, and validating the performance of the methods using both image processing metrics and associations with clinical measures.", "paper_url": "http://arxiv.org/abs/2204.01411v1", "pdf_url": "http://arxiv.org/pdf/2204.01411v1", "repo_url": null}, "2204.01387": {"publish_time": "2022-04-04", "title": "Anti-Spoofing Using Transfer Learning with Variational Information Bottleneck", "author": "Youngsik Eom et.al.", "abstract": "Recent advances in sophisticated synthetic speech generated from text-to-speech (TTS) or voice conversion (VC) systems cause threats to the existing automatic speaker verification (ASV) systems. Since such synthetic speech is generated from diverse algorithms, generalization ability with using limited training data is indispensable for a robust anti-spoofing system. In this work, we propose a transfer learning scheme based on the wav2vec 2.0 pretrained model with variational information bottleneck (VIB) for speech anti-spoofing task. Evaluation on the ASVspoof 2019 logical access (LA) database shows that our method improves the performance of distinguishing unseen spoofed and genuine speech, outperforming current state-of-the-art anti-spoofing systems. Furthermore, we show that the proposed system improves performance in low-resource and cross-dataset settings of anti-spoofing task significantly, demonstrating that our system is also robust in terms of data size and data distribution.", "paper_url": "http://arxiv.org/abs/2204.01387v1", "pdf_url": "http://arxiv.org/pdf/2204.01387v1", "repo_url": null}, "2204.02325": {"publish_time": "2022-04-05", "title": "A lightweight and accurate YOLO-like network for small target detection in Aerial Imagery", "author": "Alessandro Betti et.al.", "abstract": "Despite the breakthrough deep learning performances achieved for automatic object detection, small target detection is still a challenging problem, especially when looking at fast and accurate solutions suitable for mobile or edge applications. In this work we present YOLO-S, a simple, fast and efficient network for small target detection. The architecture exploits a small feature extractor based on Darknet20, as well as skip connection, via both bypass and concatenation, and reshape-passthrough layer to alleviate the vanishing gradient problem, promote feature reuse across network and combine low-level positional information with more meaningful high-level information. To verify the performances of YOLO-S, we build \"AIRES\", a novel dataset for cAr detectIon fRom hElicopter imageS acquired in Europe, and set up experiments on both AIRES and VEDAI datasets, benchmarking this architecture with four baseline detectors. Furthermore, in order to handle efficiently the issue of data insufficiency and domain gap when dealing with a transfer learning strategy, we introduce a transitional learning task over a combined dataset based on DOTAv2 and VEDAI and demonstrate that can enhance the overall accuracy with respect to more general features transferred from COCO data. YOLO-S is from 25% to 50% faster than YOLOv3 and only 15-25% slower than Tiny-YOLOv3, outperforming also YOLOv3 in terms of accuracy in a wide range of experiments. Further simulations performed on SARD dataset demonstrate also its applicability to different scenarios such as for search and rescue operations. Besides, YOLO-S has an 87% decrease of parameter size and almost one half FLOPs of YOLOv3, making practical the deployment for low-power industrial applications.", "paper_url": "http://arxiv.org/abs/2204.02325v1", "pdf_url": "http://arxiv.org/pdf/2204.02325v1", "repo_url": null}, "2204.01916": {"publish_time": "2022-04-05", "title": "Domain-Aware Contrastive Knowledge Transfer for Multi-domain Imbalanced Data", "author": "Zixuan Ke et.al.", "abstract": "In many real-world machine learning applications, samples belong to a set of domains e.g., for product reviews each review belongs to a product category. In this paper, we study multi-domain imbalanced learning (MIL), the scenario that there is imbalance not only in classes but also in domains. In the MIL setting, different domains exhibit different patterns and there is a varying degree of similarity and divergence among domains posing opportunities and challenges for transfer learning especially when faced with limited or insufficient training data. We propose a novel domain-aware contrastive knowledge transfer method called DCMI to (1) identify the shared domain knowledge to encourage positive transfer among similar domains (in particular from head domains to tail domains); (2) isolate the domain-specific knowledge to minimize the negative transfer from dissimilar domains. We evaluated the performance of DCMI on three different datasets showing significant improvements in different MIL scenarios.", "paper_url": "http://arxiv.org/abs/2204.01916v1", "pdf_url": "http://arxiv.org/pdf/2204.01916v1", "repo_url": null}, "2204.02802": {"publish_time": "2022-04-06", "title": "Dimensionality Expansion and Transfer Learning for Next Generation Energy Management Systems", "author": "Bla\u017e Bertalani\u010d et.al.", "abstract": "Electrical management systems (EMS) are playing a central role in enabling energy savings. They can be deployed within an everyday household where they monitor and manage appliances and help residents be more energy efficient and subsequently also more economical. One of they key functionalities of EMS is to automatically detect and identify appliances within a household through the process of load monitoring. In this paper, we propose a new transfer learning approach for building EMS (BEMS) and study the trade-offs in terms of numbers of samples and target classes in adapting a backbone model during the transfer process. We also perform a first time analysis of feature expansion through video-like transformation of time series data for device classification in non intrusive load monitoring (NILM) and propose a deep learning architecture enabling accurate appliance identification. We examine the relative performance of our method on 5 different representative low-frequency datasets and show that our method performs with an average F1 score of 0.88 on these datasets.", "paper_url": "http://arxiv.org/abs/2204.02802v1", "pdf_url": "http://arxiv.org/pdf/2204.02802v1", "repo_url": null}, "2204.02712": {"publish_time": "2022-04-06", "title": "A New Dataset for Topic-Based Paragraph Classification in Genocide-Related Court Transcripts", "author": "Miriam Schirmer et.al.", "abstract": "Recent progress in natural language processing has been impressive in many different areas with transformer-based approaches setting new benchmarks for a wide range of applications. This development has also lowered the barriers for people outside the NLP community to tap into the tools and resources applied to a variety of domain-specific applications. The bottleneck however still remains the lack of annotated gold-standard collections as soon as one's research or professional interest falls outside the scope of what is readily available. One such area is genocide-related research (also including the work of experts who have a professional interest in accessing, exploring and searching large-scale document collections on the topic, such as lawyers). We present GTC (Genocide Transcript Corpus), the first annotated corpus of genocide-related court transcripts which serves three purposes: (1) to provide a first reference corpus for the community, (2) to establish benchmark performances (using state-of-the-art transformer-based approaches) for the new classification task of paragraph identification of violence-related witness statements, (3) to explore first steps towards transfer learning within the domain. We consider our contribution to be addressing in particular this year's hot topic on Language Technology for All.", "paper_url": "http://arxiv.org/abs/2204.02712v1", "pdf_url": "http://arxiv.org/pdf/2204.02712v1", "repo_url": null}, "2204.02685": {"publish_time": "2022-04-06", "title": "Language Model for Text Analytic in Cybersecurity", "author": "Ehsan Aghaei et.al.", "abstract": "NLP is a form of artificial intelligence and machine learning concerned with a computer or machine's ability to understand and interpret human language. Language models are crucial in text analytics and NLP since they allow computers to interpret qualitative input and convert it to quantitative data that they can use in other tasks. In essence, in the context of transfer learning, language models are typically trained on a large generic corpus, referred to as the pre-training stage, and then fine-tuned to a specific underlying task. As a result, pre-trained language models are mostly used as a baseline model that incorporates a broad grasp of the context and may be further customized to be used in a new NLP task.   The majority of pre-trained models are trained on corpora from general domains, such as Twitter, newswire, Wikipedia, and Web. Such off-the-shelf NLP models trained on general text may be inefficient and inaccurate in specialized fields. In this paper, we propose a cybersecurity language model called SecureBERT, which is able to capture the text connotations in the cybersecurity domain, and therefore could further be used in automation for many important cybersecurity tasks that would otherwise rely on human expertise and tedious manual efforts. SecureBERT is trained on a large corpus of cybersecurity text collected and preprocessed by us from a variety of sources in cybersecurity and the general computing domain. Using our proposed methods for tokenization and model weights adjustment, SecureBERT is not only able to preserve the understanding of general English as most pre-trained language models can do, but also effective when applied to text that has cybersecurity implications.", "paper_url": "http://arxiv.org/abs/2204.02685v1", "pdf_url": "http://arxiv.org/pdf/2204.02685v1", "repo_url": null}, "2204.02581": {"publish_time": "2022-04-06", "title": "Banana Sub-Family Classification and Quality Prediction using Computer Vision", "author": "Narayana Darapaneni et.al.", "abstract": "India is the second largest producer of fruits and vegetables in the world, and one of the largest consumers of fruits like Banana, Papaya and Mangoes through retail and ecommerce giants like BigBasket, Grofers and Amazon Fresh. However, adoption of technology in supply chain and retail stores is still low and there is a great potential to adopt computer-vision based technology for identification and classification of fruits. We have chosen banana fruit to build a computer vision based model to carry out the following three use-cases (a) Identify Banana from a given image (b) Determine sub-family or variety of Banana (c) Determine the quality of Banana. Successful execution of these use-cases using computer-vision model would greatly help with overall inventory management automation, quality control, quick and efficient weighing and billing which all are manual labor intensive currently. In this work, we suggest a machine learning pipeline that combines the ideas of CNNs, transfer learning, and data augmentation towards improving Banana fruit sub family and quality image classification. We have built a basic CNN and then went on to tune a MobileNet Banana classification model using a combination of self-curated and publicly-available dataset of 3064 images. The results show an overall 93.4% and 100% accuracy for sub-family/variety and for quality test classifications respectively.", "paper_url": "http://arxiv.org/abs/2204.02581v1", "pdf_url": "http://arxiv.org/pdf/2204.02581v1", "repo_url": null}, "2204.03649": {"publish_time": "2022-04-07", "title": "Unsupervised Prompt Learning for Vision-Language Models", "author": "Tony Huang et.al.", "abstract": "Contrastive vision-language models like CLIP have shown great progress in zero-shot transfer learning. This new paradigm uses large-scale image-text pairs for training and aligns images and texts in a common embedding space. In the inference stage, the proper text description, known as prompt, needs to be carefully designed for zero-shot transfer. To avoid laborious prompt engineering and simultaneously improve transfer performance, recent works such as CoOp, CLIP-Adapter and Tip-Adapter propose to adapt vision-language models for downstream image recognition tasks by either optimizing the continuous prompt representations or training an additional adapter network on top of the pre-trained vision-language models on a small set of labeled data. Though promising improvements are achieved, using labeled images from target datasets may violate the intention of zero-shot transfer of pre-trained vision-language models. In this paper, we propose an unsupervised prompt learning (UPL) framework, which does not require any annotations of the target dataset, to improve the zero-shot transfer of CLIP-like vision-language models. Experimentally, for zero-shot transfer, our UPL outperforms original CLIP with prompt engineering and on ImageNet as well as other 10 datasets. An enhanced version of UPL is even on par with the 8-shot CoOp and the 8-shot TIP-Adapter on most datasets while our method does not need any labeled images for training. Code and models are available at https://github.com/tonyhuang2022/UPL.", "paper_url": "http://arxiv.org/abs/2204.03649v1", "pdf_url": "http://arxiv.org/pdf/2204.03649v1", "repo_url": "https://github.com/tonyhuang2022/upl"}, "2204.03618": {"publish_time": "2022-04-07", "title": "Pneumonia Detection in Chest X-Rays using Neural Networks", "author": "Narayana Darapaneni et.al.", "abstract": "With the advancement in AI, deep learning techniques are widely used to design robust classification models in several areas such as medical diagnosis tasks in which it achieves good performance. In this paper, we have proposed the CNN model (Convolutional Neural Network) for the classification of Chest X-ray images for Radiological Society of North America Pneumonia (RSNA) datasets. The study also tries to achieve the same RSNA benchmark results using the limited computational resources by trying out various approaches to the methodologies that have been implemented in recent years. The proposed method is based on a non-complex CNN and the use of transfer learning algorithms like Xception, InceptionV3/V4, EfficientNetB7. Along with this, the study also tries to achieve the same RSNA benchmark results using the limited computational resources by trying out various approaches to the methodologies that have been implemented in recent years. The RSNA benchmark MAP score is 0.25, but using the Mask RCNN model on a stratified sample of 3017 along with image augmentation gave a MAP score of 0.15. Meanwhile, the YoloV3 without any hyperparameter tuning gave the MAP score of 0.32 but still, the loss keeps decreasing. Running the model for a greater number of iterations can give better results.", "paper_url": "http://arxiv.org/abs/2204.03618v1", "pdf_url": "http://arxiv.org/pdf/2204.03618v1", "repo_url": null}, "2204.03610": {"publish_time": "2022-04-07", "title": "Unified Contrastive Learning in Image-Text-Label Space", "author": "Jianwei Yang et.al.", "abstract": "Visual recognition is recently learned via either supervised learning on human-annotated image-label data or language-image contrastive learning with webly-crawled image-text pairs. While supervised learning may result in a more discriminative representation, language-image pretraining shows unprecedented zero-shot recognition capability, largely due to the different properties of data sources and learning objectives. In this work, we introduce a new formulation by combining the two data sources into a common image-text-label space. In this space, we propose a new learning paradigm, called Unified Contrastive Learning (UniCL) with a single learning objective to seamlessly prompt the synergy of two data types. Extensive experiments show that our UniCL is an effective way of learning semantically rich yet discriminative representations, universally for image recognition in zero-shot, linear-probe, fully finetuning and transfer learning scenarios. Particularly, it attains gains up to 9.2% and 14.5% in average on zero-shot recognition benchmarks over the language-image contrastive learning and supervised learning methods, respectively. In linear probe setting, it also boosts the performance over the two methods by 7.3% and 3.4%, respectively. Our study also indicates that UniCL stand-alone is a good learner on pure image-label data, rivaling the supervised learning methods across three image classification datasets and two types of vision backbones, ResNet and Swin Transformer. Code is available at https://github.com/microsoft/UniCL.", "paper_url": "http://arxiv.org/abs/2204.03610v1", "pdf_url": "http://arxiv.org/pdf/2204.03610v1", "repo_url": "https://github.com/microsoft/unicl"}, "2204.03572": {"publish_time": "2022-04-07", "title": "A Pathology-Based Machine Learning Method to Assist in Epithelial Dysplasia Diagnosis", "author": "Karoline da Rocha et.al.", "abstract": "The Epithelial Dysplasia (ED) is a tissue alteration commonly present in lesions preceding oral cancer, being its presence one of the most important factors in the progression toward carcinoma. This study proposes a method to design a low computational cost classification system to support the detection of dysplastic epithelia, contributing to reduce the variability of pathologist assessments. We employ a multilayer artificial neural network (MLP-ANN) and defining the regions of the epithelium to be assessed based on the knowledge of the pathologist. The performance of the proposed solution was statistically evaluated. The implemented MLP-ANN presented an average accuracy of 87%, with a variability much inferior to that obtained from three trained evaluators. Moreover, the proposed solution led to results which are very close to those obtained using a convolutional neural network (CNN) implemented by transfer learning, with 100 times less computational complexity. In conclusion, our results show that a simple neural network structure can lead to a performance equivalent to that of much more complex structures, which are routinely used in the literature.", "paper_url": "http://arxiv.org/abs/2204.03572v1", "pdf_url": "http://arxiv.org/pdf/2204.03572v1", "repo_url": null}, "2204.03350": {"publish_time": "2022-04-07", "title": "Implementing a Real-Time, YOLOv5 based Social Distancing Measuring System for Covid-19", "author": "Narayana Darapaneni et.al.", "abstract": "The purpose of this work is, to provide a YOLOv5 deep learning-based social distance monitoring framework using an overhead view perspective. In addition, we have developed a custom defined model YOLOv5 modified CSP (Cross Stage Partial Network) and assessed the performance on COCO and Visdrone dataset with and without transfer learning. Our findings show that the developed model successfully identifies the individual who violates the social distances. The accuracy of 81.7% for the modified bottleneck CSP without transfer learning is observed on COCO dataset after training the model for 300 epochs whereas for the same epochs, the default YOLOv5 model is attaining 80.1% accuracy with transfer learning. This shows an improvement in accuracy by our modified bottleneck CSP model. For the Visdrone dataset, we are able to achieve an accuracy of upto 56.5% for certain classes and especially an accuracy of 40% for people and pedestrians with transfer learning using the default YOLOv5s model for 30 epochs. While the modified bottleneck CSP is able to perform slightly better than the default model with an accuracy score of upto 58.1% for certain classes and an accuracy of ~40.4% for people and pedestrians.", "paper_url": "http://arxiv.org/abs/2204.03350v1", "pdf_url": "http://arxiv.org/pdf/2204.03350v1", "repo_url": null}, "2204.03959": {"publish_time": "2022-04-08", "title": "Blockchain as an Enabler for Transfer Learning in Smart Environments", "author": "Amin Anjomshoaa et.al.", "abstract": "The knowledge, embodied in machine learning models for intelligent systems, is commonly associated with time-consuming and costly processes such as large-scale data collection, data labelling, network training, and fine-tuning of models. Sharing and reuse of these elaborated models between intelligent systems deployed in a different environment, which is known as transfer learning, would facilitate the adoption of services for the users and accelerates the uptake of intelligent systems in environments such as smart building and smart city applications. In this context, the communication and knowledge exchange between AI-enabled environments depend on a complicated networks of systems, system of systems, digital assets, and their chain of dependencies that hardly follows the centralized schema of traditional information systems. Rather, it requires an adaptive decentralized system architecture that is empowered by features such as data provenance, workflow transparency, and validation of process participants. In this research, we propose a decentralized and adaptive software framework based on blockchain and knowledge graph technologies that supports the knowledge exchange and interoperability between IoT-enabled environments, in a transparent and trustworthy way.", "paper_url": "http://arxiv.org/abs/2204.03959v1", "pdf_url": "http://arxiv.org/pdf/2204.03959v1", "repo_url": null}, "2204.03934": {"publish_time": "2022-04-08", "title": "Does Robustness on ImageNet Transfer to Downstream Tasks?", "author": "Yutaro Yamada et.al.", "abstract": "As clean ImageNet accuracy nears its ceiling, the research community is increasingly more concerned about robust accuracy under distributional shifts. While a variety of methods have been proposed to robustify neural networks, these techniques often target models trained on ImageNet classification. At the same time, it is a common practice to use ImageNet pretrained backbones for downstream tasks such as object detection, semantic segmentation, and image classification from different domains. This raises a question: Can these robust image classifiers transfer robustness to downstream tasks? For object detection and semantic segmentation, we find that a vanilla Swin Transformer, a variant of Vision Transformer tailored for dense prediction tasks, transfers robustness better than Convolutional Neural Networks that are trained to be robust to the corrupted version of ImageNet. For CIFAR10 classification, we find that models that are robustified for ImageNet do not retain robustness when fully fine-tuned. These findings suggest that current robustification techniques tend to emphasize ImageNet evaluations. Moreover, network architecture is a strong source of robustness when we consider transfer learning.", "paper_url": "http://arxiv.org/abs/2204.03934v1", "pdf_url": "http://arxiv.org/pdf/2204.03934v1", "repo_url": null}, "2204.03831": {"publish_time": "2022-04-08", "title": "Marvelous Agglutinative Language Effect on Cross Lingual Transfer Learning", "author": "Wooyoung Kim et.al.", "abstract": "As for multilingual language models, it is important to select languages for training because of the curse of multilinguality. (Conneau et al., 2020). It is known that using languages with similar language structures is effective for cross lingual transfer learning (Pires et al., 2019). However, we demonstrate that using agglutinative languages such as Korean is more effective in cross lingual transfer learning. This is a great discovery that will change the training strategy of cross lingual transfer learning.", "paper_url": "http://arxiv.org/abs/2204.03831v1", "pdf_url": "http://arxiv.org/pdf/2204.03831v1", "repo_url": null}, "2204.05003": {"publish_time": "2022-04-11", "title": "Local convergence rates of the least squares estimator with applications to transfer learning", "author": "Johannes Schmidt-Hieber et.al.", "abstract": "Convergence properties of empirical risk minimizers can be conveniently expressed in terms of the associated population risk. To derive bounds for the performance of the estimator under covariate shift, however, pointwise convergence rates are required. Under weak assumptions on the design distribution, it is shown that the least squares estimator (LSE) over $1$-Lipschitz functions is also minimax rate optimal with respect to a weighted uniform norm, where the weighting accounts in a natural way for the non-uniformity of the design distribution. This moreover implies that although least squares is a global criterion, the LSE turns out to be locally adaptive. We develop a new indirect proof technique that establishes the local convergence behavior based on a carefully chosen local perturbation of the LSE. These local rates are then used to construct a rate-optimal estimator for transfer learning under covariate shift.", "paper_url": "http://arxiv.org/abs/2204.05003v1", "pdf_url": "http://arxiv.org/pdf/2204.05003v1", "repo_url": null}, "2204.04950": {"publish_time": "2022-04-11", "title": "Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data", "author": "Kyungjune Baek et.al.", "abstract": "Transfer learning for GANs successfully improves generation performance under low-shot regimes. However, existing studies show that the pretrained model using a single benchmark dataset is not generalized to various target datasets. More importantly, the pretrained model can be vulnerable to copyright or privacy risks as membership inference attack advances. To resolve both issues, we propose an effective and unbiased data synthesizer, namely Primitives-PS, inspired by the generic characteristics of natural images. Specifically, we utilize 1) the generic statistics on the frequency magnitude spectrum, 2) the elementary shape (i.e., image composition via elementary shapes) for representing the structure information, and 3) the existence of saliency as prior. Since our synthesizer only considers the generic properties of natural images, the single model pretrained on our dataset can be consistently transferred to various target datasets, and even outperforms the previous methods pretrained with the natural images in terms of Fr'echet inception distance. Extensive analysis, ablation study, and evaluations demonstrate that each component of our data synthesizer is effective, and provide insights on the desirable nature of the pretrained model for the transferability of GANs.", "paper_url": "http://arxiv.org/abs/2204.04950v1", "pdf_url": "http://arxiv.org/pdf/2204.04950v1", "repo_url": "https://github.com/friedronaldo/primitives-ps"}, "2204.04837": {"publish_time": "2022-04-11", "title": "Dependable Intrusion Detection System for IoT: A Deep Transfer Learning-based Approach", "author": "Sk. Tanzir Mehedi et.al.", "abstract": "Security concerns for IoT applications have been alarming because of their widespread use in different enterprise systems. The potential threats to these applications are constantly emerging and changing, and therefore, sophisticated and dependable defense solutions are necessary against such threats. With the rapid development of IoT networks and evolving threat types, the traditional machine learning-based IDS must update to cope with the security requirements of the current sustainable IoT environment. In recent years, deep learning, and deep transfer learning have progressed and experienced great success in different fields and have emerged as a potential solution for dependable network intrusion detection. However, new and emerging challenges have arisen related to the accuracy, efficiency, scalability, and dependability of the traditional IDS in a heterogeneous IoT setup. This manuscript proposes a deep transfer learning-based dependable IDS model that outperforms several existing approaches. The unique contributions include effective attribute selection, which is best suited to identify normal and attack scenarios for a small amount of labeled data, designing a dependable deep transfer learning-based ResNet model, and evaluating considering real-world data. To this end, a comprehensive experimental performance evaluation has been conducted. Extensive analysis and performance evaluation show that the proposed model is robust, more efficient, and has demonstrated better performance, ensuring dependability.", "paper_url": "http://arxiv.org/abs/2204.04837v1", "pdf_url": "http://arxiv.org/pdf/2204.04837v1", "repo_url": null}, "2204.04793": {"publish_time": "2022-04-10", "title": "Fake news detection using parallel BERT deep neural networks", "author": "Mahmood Farokhian et.al.", "abstract": "Fake news is a growing challenge for social networks and media. Detection of fake news always has been a problem for many years, but after the evolution of social networks and increasing speed of news dissemination in recent years has been considered again. There are several approaches to solving this problem, one of which is to detect fake news based on its text style using deep neural networks. In recent years, one of the most used forms of deep neural networks for natural language processing is transfer learning with transformers. BERT is one of the most promising transformers who outperforms other models in many NLP benchmarks. This article, we introduce MWPBert, which uses two parallel BERT networks to perform veracity detection on full-text news articles. One of the BERT networks encodes news headline, and another encodes news body. Since the input length of the BERT network is limited and constant and the news body is usually a long text, we cannot fed the whole news text into the BERT. Therefore, using the MaxWorth algorithm, we selected the part of the news text that is more valuable for fact-checking, and fed it into the BERT network. Finally, we encode the output of the two BERT networks to an output network to classify the news. The experiment results showed that the proposed model outperformed previous models in terms of accuracy and other performance measures.", "paper_url": "http://arxiv.org/abs/2204.04793v1", "pdf_url": "http://arxiv.org/pdf/2204.04793v1", "repo_url": null}, "2204.04497": {"publish_time": "2022-04-09", "title": "IDPG: An Instance-Dependent Prompt Generation Method", "author": "Zhuofeng Wu et.al.", "abstract": "Prompt tuning is a new, efficient NLP transfer learning paradigm that adds a task-specific prompt in each input instance during the model training stage. It freezes the pre-trained language model and only optimizes a few task-specific prompts. In this paper, we propose a conditional prompt generation method to generate prompts for each input instance, referred to as the Instance-Dependent Prompt Generation (IDPG). Unlike traditional prompt tuning methods that use a fixed prompt, IDPG introduces a lightweight and trainable component to generate prompts based on each input sentence. Extensive experiments on ten natural language understanding (NLU) tasks show that the proposed strategy consistently outperforms various prompt tuning baselines and is on par with other efficient transfer learning methods such as Compacter while tuning far fewer model parameters.", "paper_url": "http://arxiv.org/abs/2204.04497v1", "pdf_url": "http://arxiv.org/pdf/2204.04497v1", "repo_url": null}, "2204.05432": {"publish_time": "2022-04-11", "title": "A Simple Approach to Adversarial Robustness in Few-shot Image Classification", "author": "Akshayvarun Subramanya et.al.", "abstract": "Few-shot image classification, where the goal is to generalize to tasks with limited labeled data, has seen great progress over the years. However, the classifiers are vulnerable to adversarial examples, posing a question regarding their generalization capabilities. Recent works have tried to combine meta-learning approaches with adversarial training to improve the robustness of few-shot classifiers. We show that a simple transfer-learning based approach can be used to train adversarially robust few-shot classifiers. We also present a method for novel classification task based on calibrating the centroid of the few-shot category towards the base classes. We show that standard adversarial training on base categories along with calibrated centroid-based classifier in the novel categories, outperforms or is on-par with state-of-the-art advanced methods on standard benchmarks for few-shot learning. Our method is simple, easy to scale, and with little effort can lead to robust few-shot classifiers. Code is available here: \\url{https://github.com/UCDvision/Simple_few_shot.git}", "paper_url": "http://arxiv.org/abs/2204.05432v1", "pdf_url": "http://arxiv.org/pdf/2204.05432v1", "repo_url": null}, "2204.05400": {"publish_time": "2022-04-11", "title": "Transfer Learning for Autonomous Chatter Detection in Machining", "author": "Melih C. Yesilli et.al.", "abstract": "Large-amplitude chatter vibrations are one of the most important phenomena in machining processes. It is often detrimental in cutting operations causing a poor surface finish and decreased tool life. Therefore, chatter detection using machine learning has been an active research area over the last decade. Three challenges can be identified in applying machine learning for chatter detection at large in industry: an insufficient understanding of the universality of chatter features across different processes, the need for automating feature extraction, and the existence of limited data for each specific workpiece-machine tool combination. These three challenges can be grouped under the umbrella of transfer learning. This paper studies automating chatter detection by evaluating transfer learning of prominent as well as novel chatter detection methods. We investigate chatter classification accuracy using a variety of features extracted from turning and milling experiments with different cutting configurations. The studied methods include Fast Fourier Transform (FFT), Power Spectral Density (PSD), the Auto-correlation Function (ACF), Wavelet Packet Transform (WPT), and Ensemble Empirical Mode Decomposition (EEMD). We also examine more recent approaches based on Topological Data Analysis (TDA) and similarity measures of time series based on Discrete Time Warping (DTW). We evaluate the transfer learning potential of each approach by training and testing both within and across the turning and milling data sets. Our results show that carefully chosen time-frequency features can lead to high classification accuracies albeit at the cost of requiring manual pre-processing and the tagging of an expert user. On the other hand, we found that the TDA and DTW approaches can provide accuracies and F1 scores on par with the time-frequency methods without the need for manual preprocessing.", "paper_url": "http://arxiv.org/abs/2204.05400v1", "pdf_url": "http://arxiv.org/pdf/2204.05400v1", "repo_url": null}}}, "Graph Neural Network": {"Graph Neural Network": {"2202.12619": {"publish_time": "2022-02-25", "title": "Fluid Simulation System Based on Graph Neural Network", "author": "Qiang Liu et.al.", "abstract": "Traditional computational fluid dynamics calculates the physical information of the flow field by solving partial differential equations, which takes a long time to calculate and consumes a lot of computational resources. We build a fluid simulation simulator based on the graph neural network architecture. The simulator has fast computing speed and low consumption of computing resources. We regard the computational domain as a structural graph, and the computational nodes in the structural graph determine neighbor nodes through adaptive sampling. Building deep learning architectures with attention graph neural networks. The fluid simulation simulator is trained according to the simulation results of the flow field around the cylinder with different Reynolds numbers. The trained fluid simulation simulator not only has a very high accuracy for the prediction of the flow field in the training set, but also can extrapolate the flow field outside the training set. Compared to traditional CFD solvers, the fluid simulation simulator achieves a speedup of 2-3 orders of magnitude. The fluid simulation simulator provides new ideas for the rapid optimization and design of fluid mechanics models and the real-time control of intelligent fluid mechanisms.", "paper_url": "http://arxiv.org/abs/2202.12619v1", "pdf_url": "http://arxiv.org/pdf/2202.12619v1", "repo_url": null}, "2202.12586": {"publish_time": "2022-02-25", "title": "Spatio-Temporal Latent Graph Structure Learning for Traffic Forecasting", "author": "Jiabin Tang et.al.", "abstract": "Accurate traffic forecasting, the foundation of intelligent transportation systems (ITS), has never been more significant than nowadays due to the prosperity of the smart cities and urban computing. Recently, Graph Neural Network truly outperforms the traditional methods. Nevertheless, the most conventional GNN based model works well while given a pre-defined graph structure. And the existing methods of defining the graph structures focus purely on spatial dependencies and ignored the temporal correlation. Besides, the semantics of the static pre-defined graph adjacency applied during the whole training progress is always incomplete, thus overlooking the latent topologies that may fine-tune the model. To tackle these challenges, we proposed a new traffic forecasting framework--Spatio-Temporal Latent Graph Structure Learning networks (ST-LGSL). More specifically, the model employed a graph generator based on Multilayer perceptron and K-Nearest Neighbor, which learns the latent graph topological information from the entire data considering both spatial and temporal dynamics. Furthermore, with the initialization of MLP-kNN based on ground-truth adjacency matrix and similarity metric in kNN, ST-LGSL aggregates the topologies focusing on geography and node similarity. Additionally, the generated graphs act as the input of spatio-temporal prediction module combined with the Diffusion Graph Convolutions and Gated Temporal Convolutions Networks. Experimental results on two benchmarking datasets in real world demonstrate that ST-LGSL outperforms various types of state-of-art baselines.", "paper_url": "http://arxiv.org/abs/2202.12586v1", "pdf_url": "http://arxiv.org/pdf/2202.12586v1", "repo_url": null}, "2202.12508": {"publish_time": "2022-02-25", "title": "Addressing Over-Smoothing in Graph Neural Networks via Deep Supervision", "author": "Pantelis Elinas et.al.", "abstract": "Learning useful node and graph representations with graph neural networks (GNNs) is a challenging task. It is known that deep GNNs suffer from over-smoothing where, as the number of layers increases, node representations become nearly indistinguishable and model performance on the downstream task degrades significantly. To address this problem, we propose deeply-supervised GNNs (DSGNNs), i.e., GNNs enhanced with deep supervision where representations learned at all layers are used for training. We show empirically that DSGNNs are resilient to over-smoothing and can outperform competitive benchmarks on node and graph property prediction problems.", "paper_url": "http://arxiv.org/abs/2202.12508v1", "pdf_url": "http://arxiv.org/pdf/2202.12508v1", "repo_url": null}, "2202.12481": {"publish_time": "2022-02-25", "title": "Multi-View Graph Representation for Programming Language Processing: An Investigation into Algorithm Detection", "author": "Ting Long et.al.", "abstract": "Program representation, which aims at converting program source code into vectors with automatically extracted features, is a fundamental problem in programming language processing (PLP). Recent work tries to represent programs with neural networks based on source code structures. However, such methods often focus on the syntax and consider only one single perspective of programs, limiting the representation power of models. This paper proposes a multi-view graph (MVG) program representation method. MVG pays more attention to code semantics and simultaneously includes both data flow and control flow as multiple views. These views are then combined and processed by a graph neural network (GNN) to obtain a comprehensive program representation that covers various aspects. We thoroughly evaluate our proposed MVG approach in the context of algorithm detection, an important and challenging subfield of PLP. Specifically, we use a public dataset POJ-104 and also construct a new challenging dataset ALG-109 to test our method. In experiments, MVG outperforms previous methods significantly, demonstrating our model's strong capability of representing source code.", "paper_url": "http://arxiv.org/abs/2202.12481v1", "pdf_url": "http://arxiv.org/pdf/2202.12481v1", "repo_url": null}, "2202.12478": {"publish_time": "2022-02-25", "title": "GAME-ON: Graph Attention Network based Multimodal Fusion for Fake News Detection", "author": "Mudit Dhawan et.al.", "abstract": "Social media in present times has a significant and growing influence. Fake news being spread on these platforms have a disruptive and damaging impact on our lives. Furthermore, as multimedia content improves the visibility of posts more than text data, it has been observed that often multimedia is being used for creating fake content. A plethora of previous multimodal-based work has tried to address the problem of modeling heterogeneous modalities in identifying fake content. However, these works have the following limitations: (1) inefficient encoding of inter-modal relations by utilizing a simple concatenation operator on the modalities at a later stage in a model, which might result in information loss; (2) training very deep neural networks with a disproportionate number of parameters on small but complex real-life multimodal datasets result in higher chances of overfitting. To address these limitations, we propose GAME-ON, a Graph Neural Network based end-to-end trainable framework that allows granular interactions within and across different modalities to learn more robust data representations for multimodal fake news detection. We use two publicly available fake news datasets, Twitter and Weibo, for evaluations. Our model outperforms on Twitter by an average of 11% and keeps competitive performance on Weibo, within a 2.6% margin, while using 65% fewer parameters than the best comparable state-of-the-art baseline.", "paper_url": "http://arxiv.org/abs/2202.12478v1", "pdf_url": "http://arxiv.org/pdf/2202.12478v1", "repo_url": null}, "2202.13956": {"publish_time": "2022-02-28", "title": "RouteNet-Erlang: A Graph Neural Network for Network Performance Evaluation", "author": "Miquel Ferriol-Galm\u00e9s et.al.", "abstract": "Network modeling is a fundamental tool in network research, design, and operation. Arguably the most popular method for modeling is Queuing Theory (QT). Its main limitation is that it imposes strong assumptions on the packet arrival process, which typically do not hold in real networks. In the field of Deep Learning, Graph Neural Networks (GNN) have emerged as a new technique to build data-driven models that can learn complex and non-linear behavior. In this paper, we present \\emph{RouteNet-Erlang}, a pioneering GNN architecture designed to model computer networks. RouteNet-Erlang supports complex traffic models, multi-queue scheduling policies, routing policies and can provide accurate estimates in networks not seen in the training phase. We benchmark RouteNet-Erlang against a state-of-the-art QT model, and our results show that it outperforms QT in all the network scenarios.", "paper_url": "http://arxiv.org/abs/2202.13956v1", "pdf_url": "http://arxiv.org/pdf/2202.13956v1", "repo_url": null}, "2202.13947": {"publish_time": "2022-02-28", "title": "Data-Augmentation for Graph Neural Network Learning of the Relaxed Energies of Unrelaxed Structures", "author": "Jason B. Gibson et.al.", "abstract": "Computational materials discovery has continually grown in utility over the past decade due to advances in computing power and crystal structure prediction algorithms (CSPA). However, the computational cost of the \\textit{ab initio} calculations required by CSPA limits its utility to small unit cells, reducing the compositional and structural space the algorithms can explore. Past studies have bypassed many unneeded \\textit{ab initio} calculations by utilizing machine learning methods to predict formation energy and determine the stability of a material. Specifically, graph neural networks display high fidelity in predicting formation energy. Traditionally graph neural networks are trained on large data sets of relaxed structures. Unfortunately, the geometries of unrelaxed candidate structures produced by CSPA often deviate from the relaxed state, which leads to poor predictions hindering the model's ability to filter energetically unfavorable prior to \\textit{ab initio} evaluation. This work shows that the prediction error on relaxed structures reduces as training progresses, while the prediction error on unrelaxed structures increases, suggesting an inverse correlation between relaxed and unrelaxed structure prediction accuracy. To remedy this behavior, we propose a simple, physically motivated, computationally cheap perturbation technique that augments training data to improve predictions on unrelaxed structures dramatically. On our test set consisting of 623 Nb-Sr-H hydride structures, we found that training a crystal graph convolutional neural networks, utilizing our augmentation method, reduced the MAE of formation energy prediction by 66\\% compared to training with only relaxed structures. We then show how this error reduction can accelerates CSPA by improving the model's ability to filter out energetically unfavorable structures accurately.", "paper_url": "http://arxiv.org/abs/2202.13947v1", "pdf_url": "http://arxiv.org/pdf/2202.13947v1", "repo_url": null}, "2202.13852": {"publish_time": "2022-02-28", "title": "Hyperbolic Graph Neural Networks: A Review of Methods and Applications", "author": "Menglin Yang et.al.", "abstract": "Graph neural networks generalize conventional neural networks to graph-structured data and have received widespread attention due to their impressive representation ability. In spite of the remarkable achievements, the performance of Euclidean models in graph-related learning is still bounded and limited by the representation ability of Euclidean geometry, especially for datasets with highly non-Euclidean latent anatomy. Recently, hyperbolic space has gained increasing popularity in processing graph data with tree-like structure and power-law distribution, owing to its exponential growth property. In this survey, we comprehensively revisit the technical details of the current hyperbolic graph neural networks, unifying them into a general framework and summarizing the variants of each component. More importantly, we present various HGNN-related applications. Last, we also identify several challenges, which potentially serve as guidelines for further flourishing the achievements of graph learning in hyperbolic spaces.", "paper_url": "http://arxiv.org/abs/2202.13852v1", "pdf_url": "http://arxiv.org/pdf/2202.13852v1", "repo_url": "https://github.com/marlin-codes/HGNNs"}, "2202.13800": {"publish_time": "2022-02-28", "title": "Differential equation and probability inspired graph neural networks for latent variable learning", "author": "Zhuangwei Shi et.al.", "abstract": "Probabilistic theory and differential equation are powerful tools for the interpretability and guidance of the design of machine learning models, especially for illuminating the mathematical motivation of learning latent variable from observation. State estimation and subspace learning are two classical problems in latent variable learning. State estimation solves optimal value for latent variable (i.e. state) from noised observation. Subspace learning maps high-dimensional features on low-dimensional subspace to capture efficient representation. Graphs are widely applied for modeling latent variable learning problems, and graph neural networks implement deep learning architectures on graphs. Inspired by probabilistic theory and differential equations, this paper proposes graph neural networks to solve state estimation and subspace learning problems. This paper conducts theoretical studies, and adopts empirical studies on several tasks, including text classification, protein classification, stock prediction and state estimation for robotics. Experiments illustrate that the proposed graph neural networks are superior to the current methods. Source code of this paper is available at https://github.com/zshicode/Latent-variable-GNN.", "paper_url": "http://arxiv.org/abs/2202.13800v1", "pdf_url": "http://arxiv.org/pdf/2202.13800v1", "repo_url": "https://github.com/zshicode/latent-variable-gnn"}, "2202.13686": {"publish_time": "2022-02-28", "title": "Points-of-Interest Relationship Inference with Spatial-enriched Graph Neural Networks", "author": "Yile Chen et.al.", "abstract": "As a fundamental component in location-based services, inferring the relationship between points-of-interests (POIs) is very critical for service providers to offer good user experience to business owners and customers. Most of the existing methods for relationship inference are not targeted at POI, thus failing to capture unique spatial characteristics that have huge effects on POI relationships. In this work we propose PRIM to tackle POI relationship inference for multiple relation types. PRIM features four novel components, including a weighted relational graph neural network, category taxonomy integration, a self-attentive spatial context extractor, and a distance-specific scoring function. Extensive experiments on two real-world datasets show that PRIM achieves the best results compared to state-of-the-art baselines and it is robust against data sparsity and is applicable to unseen cases in practice.", "paper_url": "http://arxiv.org/abs/2202.13686v1", "pdf_url": "http://arxiv.org/pdf/2202.13686v1", "repo_url": null}, "2203.00638": {"publish_time": "2022-03-01", "title": "PaSca: a Graph Neural Architecture Search System under the Scalable Paradigm", "author": "Wentao Zhang et.al.", "abstract": "Graph neural networks (GNNs) have achieved state-of-the-art performance in various graph-based tasks. However, as mainstream GNNs are designed based on the neural message passing mechanism, they do not scale well to data size and message passing steps. Although there has been an emerging interest in the design of scalable GNNs, current researches focus on specific GNN design, rather than the general design space, limiting the discovery of potential scalable GNN models. This paper proposes PasCa, a new paradigm and system that offers a principled approach to systemically construct and explore the design space for scalable GNNs, rather than studying individual designs. Through deconstructing the message passing mechanism, PasCa presents a novel Scalable Graph Neural Architecture Paradigm (SGAP), together with a general architecture design space consisting of 150k different designs. Following the paradigm, we implement an auto-search engine that can automatically search well-performing and scalable GNN architectures to balance the trade-off between multiple criteria (e.g., accuracy and efficiency) via multi-objective optimization. Empirical studies on ten benchmark datasets demonstrate that the representative instances (i.e., PasCa-V1, V2, and V3) discovered by our system achieve consistent performance among competitive baselines. Concretely, PasCa-V3 outperforms the state-of-the-art GNN method JK-Net by 0.4\\% in terms of predictive accuracy on our large industry dataset while achieving up to $28.3\\times$ training speedups.", "paper_url": "http://arxiv.org/abs/2203.00638v1", "pdf_url": "http://arxiv.org/pdf/2203.00638v1", "repo_url": null}, "2203.00611": {"publish_time": "2022-03-01", "title": "Learning Intermediate Representations using Graph Neural Networks for NUMA and Prefetchers Optimization", "author": "Ali TehraniJamsaz et.al.", "abstract": "There is a large space of NUMA and hardware prefetcher configurations that can significantly impact the performance of an application. Previous studies have demonstrated how a model can automatically select configurations based on the dynamic properties of the code to achieve speedups. This paper demonstrates how the static Intermediate Representation (IR) of the code can guide NUMA/prefetcher optimizations without the prohibitive cost of performance profiling. We propose a method to create a comprehensive dataset that includes a diverse set of intermediate representations along with optimum configurations. We then apply a graph neural network model in order to validate this dataset. We show that our static intermediate representation based model achieves 80% of the performance gains provided by expensive dynamic performance profiling based strategies. We further develop a hybrid model that uses both static and dynamic information. Our hybrid model achieves the same gains as the dynamic models but at a reduced cost by only profiling 30% of the programs.", "paper_url": "http://arxiv.org/abs/2203.00611v1", "pdf_url": "http://arxiv.org/pdf/2203.00611v1", "repo_url": null}, "2203.00387": {"publish_time": "2022-03-01", "title": "Motion-aware Dynamic Graph Neural Network for Video Compressive Sensing", "author": "Ruiying Lu et.al.", "abstract": "Video snapshot compressive imaging (SCI) utilizes a 2D detector to capture sequential video frames and compresses them into a single measurement. Various reconstruction methods have been developed to recover the high-speed video frames from the snapshot measurement. However, most existing reconstruction methods are incapable of capturing long-range spatial and temporal dependencies, which are critical for video processing. In this paper, we propose a flexible and robust approach based on graph neural network (GNN) to efficiently model non-local interactions between pixels in space as well as time regardless of the distance. Specifically, we develop a motion-aware dynamic GNN for better video representation, i.e., represent each pixel as the aggregation of relative nodes under the guidance of frame-by-frame motions, which consists of motion-aware dynamic sampling, cross-scale node sampling and graph aggregation. Extensive results on both simulation and real data demonstrate both the effectiveness and efficiency of the proposed approach, and the visualization clearly illustrates the intrinsic dynamic sampling operations of our proposed model for boosting the video SCI reconstruction results. The code and models will be released to the public.", "paper_url": "http://arxiv.org/abs/2203.00387v1", "pdf_url": "http://arxiv.org/pdf/2203.00387v1", "repo_url": null}, "2203.00330": {"publish_time": "2022-03-01", "title": "Machine Learning for Particle Flow Reconstruction at CMS", "author": "Joosep Pata et.al.", "abstract": "We provide details on the implementation of a machine-learning based particle flow algorithm for CMS. The standard particle flow algorithm reconstructs stable particles based on calorimeter clusters and tracks to provide a global event reconstruction that exploits the combined information of multiple detector subsystems, leading to strong improvements for quantities such as jets and missing transverse energy. We have studied a possible evolution of particle flow towards heterogeneous computing platforms such as GPUs using a graph neural network. The machine-learned PF model reconstructs particle candidates based on the full list of tracks and calorimeter clusters in the event. For validation, we determine the physics performance directly in the CMS software framework when the proposed algorithm is interfaced with the offline reconstruction of jets and missing transverse energy. We also report the computational performance of the algorithm, which scales approximately linearly in runtime and memory usage with the input size.", "paper_url": "http://arxiv.org/abs/2203.00330v1", "pdf_url": "http://arxiv.org/pdf/2203.00330v1", "repo_url": null}, "2203.00199": {"publish_time": "2022-03-01", "title": "Equivariant and Stable Positional Encoding for More Powerful Graph Neural Networks", "author": "Haorui Wang et.al.", "abstract": "Graph neural networks (GNN) have shown great advantages in many graph-based learning tasks but often fail to predict accurately for a task-based on sets of nodes such as link/motif prediction and so on. Many works have recently proposed to address this problem by using random node features or node distance features. However, they suffer from either slow convergence, inaccurate prediction, or high complexity. In this work, we revisit GNNs that allow using positional features of nodes given by positional encoding (PE) techniques such as Laplacian Eigenmap, Deepwalk, etc. GNNs with PE often get criticized because they are not generalizable to unseen graphs (inductive) or stable. Here, we study these issues in a principled way and propose a provable solution, a class of GNN layers termed PEG with rigorous mathematical analysis. PEG uses separate channels to update the original node features and positional features. PEG imposes permutation equivariance w.r.t. the original node features and rotation equivariance w.r.t. the positional features simultaneously. Extensive link prediction experiments over 8 real-world networks demonstrate the advantages of PEG in generalization and scalability.", "paper_url": "http://arxiv.org/abs/2203.00199v1", "pdf_url": "http://arxiv.org/pdf/2203.00199v1", "repo_url": "https://github.com/graph-com/peg"}, "2203.01189": {"publish_time": "2022-03-02", "title": "GNN-based end-to-end reconstruction in the CMS Phase 2 High-Granularity Calorimeter", "author": "Saptaparna Bhattacharya et.al.", "abstract": "We present the current stage of research progress towards a one-pass, completely Machine Learning (ML) based imaging calorimeter reconstruction. The model used is based on Graph Neural Networks (GNNs) and directly analyzes the hits in each HGCAL endcap. The ML algorithm is trained to predict clusters of hits originating from the same incident particle by labeling the hits with the same cluster index. We impose simple criteria to assess whether the hits associated as a cluster by the prediction are matched to those hits resulting from any particular individual incident particles. The algorithm is studied by simulating two tau leptons in each of the two HGCAL endcaps, where each tau may decay according to its measured standard model branching probabilities. The simulation includes the material interaction of the tau decay products which may create additional particles incident upon the calorimeter. Using this varied multiparticle environment we can investigate the application of this reconstruction technique and begin to characterize energy containment and performance.", "paper_url": "http://arxiv.org/abs/2203.01189v1", "pdf_url": "http://arxiv.org/pdf/2203.01189v1", "repo_url": null}, "2203.01187": {"publish_time": "2022-03-02", "title": "Visual Feature Encoding for GNNs on Road Networks", "author": "Oliver Stromann et.al.", "abstract": "In this work, we present a novel approach to learning an encoding of visual features into graph neural networks with the application on road network data. We propose an architecture that combines state-of-the-art vision backbone networks with graph neural networks. More specifically, we perform a road type classification task on an Open Street Map road network through encoding of satellite imagery using various ResNet architectures. Our architecture further enables fine-tuning and a transfer-learning approach is evaluated by pretraining on the NWPU-RESISC45 image classification dataset for remote sensing and comparing them to purely ImageNet-pretrained ResNet models as visual feature encoders. The results show not only that the visual feature encoders are superior to low-level visual features, but also that the fine-tuning of the visual feature encoder to a general remote sensing dataset such as NWPU-RESISC45 can further improve the performance of a GNN on a machine learning task like road type classification.", "paper_url": "http://arxiv.org/abs/2203.01187v1", "pdf_url": "http://arxiv.org/pdf/2203.01187v1", "repo_url": null}, "2203.01112": {"publish_time": "2022-03-02", "title": "Hyperparameter optimization of data-driven AI models on HPC systems", "author": "Eric Wulff et.al.", "abstract": "In the European Center of Excellence in Exascale computing \"Research on AI- and Simulation-Based Engineering at Exascale\" (CoE RAISE), researchers develop novel, scalable AI technologies towards Exascale. This work exercises High Performance Computing resources to perform large-scale hyperparameter optimization using distributed training on multiple compute nodes. This is part of RAISE's work on data-driven use cases which leverages AI- and HPC cross-methods developed within the project. In response to the demand for parallelizable and resource efficient hyperparameter optimization methods, advanced hyperparameter search algorithms are benchmarked and compared. The evaluated algorithms, including Random Search, Hyperband and ASHA, are tested and compared in terms of both accuracy and accuracy per compute resources spent. As an example use case, a graph neural network model known as MLPF, developed for the task of Machine-Learned Particle-Flow reconstruction in High Energy Physics, acts as the base model for optimization. Results show that hyperparameter optimization significantly increased the performance of MLPF and that this would not have been possible without access to large-scale High Performance Computing resources. It is also shown that, in the case of MLPF, the ASHA algorithm in combination with Bayesian optimization gives the largest performance increase per compute resources spent out of the investigated algorithms.", "paper_url": "http://arxiv.org/abs/2203.01112v1", "pdf_url": "http://arxiv.org/pdf/2203.01112v1", "repo_url": null}, "2203.01093": {"publish_time": "2022-03-02", "title": "Information Gain Propagation: a new way to Graph Active Learning with Soft Labels", "author": "Wentao Zhang et.al.", "abstract": "Graph Neural Networks (GNNs) have achieved great success in various tasks, but their performance highly relies on a large number of labeled nodes, which typically requires considerable human effort. GNN-based Active Learning (AL) methods are proposed to improve the labeling efficiency by selecting the most valuable nodes to label. Existing methods assume an oracle can correctly categorize all the selected nodes and thus just focus on the node selection. However, such an exact labeling task is costly, especially when the categorization is out of the domain of individual expert (oracle). The paper goes further, presenting a soft-label approach to AL on GNNs. Our key innovations are: i) relaxed queries where a domain expert (oracle) only judges the correctness of the predicted labels (a binary question) rather than identifying the exact class (a multi-class question), and ii) new criteria of maximizing information gain propagation for active learner with relaxed queries and soft labels. Empirical studies on public datasets demonstrate that our method significantly outperforms the state-of-the-art GNN-based AL methods in terms of both accuracy and labeling cost.", "paper_url": "http://arxiv.org/abs/2203.01093v1", "pdf_url": "http://arxiv.org/pdf/2203.01093v1", "repo_url": "https://github.com/zwt233/igp"}, "2203.00949": {"publish_time": "2022-03-02", "title": "GAP: Differentially Private Graph Neural Networks with Aggregation Perturbation", "author": "Sina Sajadmanesh et.al.", "abstract": "Graph Neural Networks (GNNs) are powerful models designed for graph data that learn node representation by recursively aggregating information from each node's local neighborhood. However, despite their state-of-the-art performance in predictive graph-based applications, recent studies have shown that GNNs can raise significant privacy concerns when graph data contain sensitive information. As a result, in this paper, we study the problem of learning GNNs with Differential Privacy (DP). We propose GAP, a novel differentially private GNN that safeguards the privacy of nodes and edges using aggregation perturbation, i.e., adding calibrated stochastic noise to the output of the GNN's aggregation function, which statistically obfuscates the presence of a single edge (edge-level privacy) or a single node and all its adjacent edges (node-level privacy). To circumvent the accumulation of privacy cost at every forward pass of the model, we tailor the GNN architecture to the specifics of private learning. In particular, we first precompute private aggregations by recursively applying neighborhood aggregation and perturbing the output of each aggregation step. Then, we privately train a deep neural network on the resulting perturbed aggregations for any node-wise classification task. A major advantage of GAP over previous approaches is that we guarantee edge-level and node-level DP not only for training, but also at inference time with no additional costs beyond the training's privacy budget. We theoretically analyze the formal privacy guarantees of GAP using R\\'enyi DP. Empirical experiments conducted over three real-world graph datasets demonstrate that GAP achieves a favorable privacy-accuracy trade-off and significantly outperforms existing approaches.", "paper_url": "http://arxiv.org/abs/2203.00949v1", "pdf_url": "http://arxiv.org/pdf/2203.00949v1", "repo_url": null}, "2203.01884": {"publish_time": "2022-03-03", "title": "Graph Neural Networks for Multimodal Single-Cell Data Integration", "author": "Hongzhi Wen et.al.", "abstract": "Recent advances in multimodal single-cell technologies have enabled simultaneous acquisitions of multiple omics data from the same cell, providing deeper insights into cellular states and dynamics. However, it is challenging to learn the joint representations from the multimodal data, model the relationship between modalities, and, more importantly, incorporate the vast amount of single-modality datasets into the downstream analyses. To address these challenges and correspondingly facilitate multimodal single-cell data analyses, three key tasks have been introduced: $\\textit{modality prediction}$, $\\textit{modality matching}$ and $\\textit{joint embedding}$. In this work, we present a general Graph Neural Network framework $\\textit{scMoGNN}$ to tackle these three tasks and show that $\\textit{scMoGNN}$ demonstrates superior results in all three tasks compared with the state-of-the-art and conventional approaches. Our method is an official winner in the overall ranking of $\\textit{modality prediction}$ from $\\href{https://openproblems.bio/neurips_2021/}{\\textit{NeurIPS 2021 Competition}}$.", "paper_url": "http://arxiv.org/abs/2203.01884v1", "pdf_url": "http://arxiv.org/pdf/2203.01884v1", "repo_url": "https://github.com/openproblems-bio/neurips2021_multimodal_topmethods"}, "2203.01874": {"publish_time": "2022-03-03", "title": "Thermodynamics-informed graph neural networks", "author": "Quercus Hern\u00e1ndez et.al.", "abstract": "In this paper we present a deep learning method to predict the time evolution of dissipative dynamical systems. We propose using both geometric and thermodynamic inductive biases to improve accuracy and generalization of the resulting integration scheme. The first is achieved with Graph Neural Networks, which induces a non-Euclidean geometrical prior and permutation invariant node and edge update functions. The second bias is forced by learning the GENERIC structure of the problem, an extension of the Hamiltonian formalism, to model more general non-conservative dynamics. Several examples are provided in both Eulerian and Lagrangian description in the context of fluid and solid mechanics respectively.", "paper_url": "http://arxiv.org/abs/2203.01874v1", "pdf_url": "http://arxiv.org/pdf/2203.01874v1", "repo_url": null}, "2203.01821": {"publish_time": "2022-03-03", "title": "Socially Aware Robot Crowd Navigation with Interaction Graphs and Human Trajectory Prediction", "author": "Shuijing Liu et.al.", "abstract": "We study the problem of safe and socially aware robot navigation in dense and interactive human crowds. Previous works use simplified methods to model the personal spaces of pedestrians and ignore the social compliance of the robot behaviors. In this paper, we provide a more accurate representation of personal zones of walking pedestrians with their future trajectories. The predicted personal zones are incorporated into a reinforcement learning framework to prevent the robot from intruding into the personal zones. To learn socially aware navigation policies, we propose a novel recurrent graph neural network with attention mechanisms to capture the interactions among agents through space and time. We demonstrate that our method enables the robot to achieve good navigation performance and non-invasiveness in challenging crowd navigation scenarios. We successfully transfer the policy learned in the simulator to a real-world TurtleBot 2i.", "paper_url": "http://arxiv.org/abs/2203.01821v1", "pdf_url": "http://arxiv.org/pdf/2203.01821v1", "repo_url": null}, "2203.01646": {"publish_time": "2022-03-03", "title": "On an application of graph neural networks in population based SHM", "author": "G. Tsialiamanis et.al.", "abstract": "Attempts have been made recently in the field of population-based structural health monitoring (PBSHM), to transfer knowledge between SHM models of different structures. The attempts have been focussed on homogeneous and heterogeneous populations. A more general approach to transferring knowledge between structures, is by considering all plausible structures as points on a multidimensional base manifold and building a fibre bundle. The idea is quite powerful, since, a mapping between points in the base manifold and their fibres, the potential states of any arbitrary structure, can be learnt. A smaller scale problem, but still useful, is that of learning a specific point of every fibre, i.e. that corresponding to the undamaged state of structures within a population. Under the framework of PBSHM, a data-driven approach to the aforementioned problem is developed. Structures are converted into graphs and inference is attempted within a population, using a graph neural network (GNN) algorithm. The algorithm solves a major problem existing in such applications. Structures comprise different sizes and are defined as abstract objects, thus attempting to perform inference within a heterogeneous population is not trivial. The proposed approach is tested in a simulated population of trusses. The goal of the application is to predict the first natural frequency of trusses of different sizes, across different environmental temperatures and having different bar member types. After training the GNN using part of the total population, it was tested on trusses that were not included in the training dataset. Results show that the accuracy of the regression is satisfactory even in structures with higher number of nodes and members than those used to train it.", "paper_url": "http://arxiv.org/abs/2203.01646v1", "pdf_url": "http://arxiv.org/pdf/2203.01646v1", "repo_url": null}, "2203.01597": {"publish_time": "2022-03-03", "title": "Neural Graph Matching for Pre-training Graph Neural Networks", "author": "Yupeng Hou et.al.", "abstract": "Recently, graph neural networks (GNNs) have been shown powerful capacity at modeling structural data. However, when adapted to downstream tasks, it usually requires abundant task-specific labeled data, which can be extremely scarce in practice. A promising solution to data scarcity is to pre-train a transferable and expressive GNN model on large amounts of unlabeled graphs or coarse-grained labeled graphs. Then the pre-trained GNN is fine-tuned on downstream datasets with task-specific fine-grained labels. In this paper, we present a novel Graph Matching based GNN Pre-Training framework, called GMPT. Focusing on a pair of graphs, we propose to learn structural correspondences between them via neural graph matching, consisting of both intra-graph message passing and inter-graph message passing. In this way, we can learn adaptive representations for a given graph when paired with different graphs, and both node- and graph-level characteristics are naturally considered in a single pre-training task. The proposed method can be applied to fully self-supervised pre-training and coarse-grained supervised pre-training. We further propose an approximate contrastive training strategy to significantly reduce time/memory consumption. Extensive experiments on multi-domain, out-of-distribution benchmarks have demonstrated the effectiveness of our approach. The code is available at: https://github.com/RUCAIBox/GMPT.", "paper_url": "http://arxiv.org/abs/2203.01597v1", "pdf_url": "http://arxiv.org/pdf/2203.01597v1", "repo_url": "https://github.com/rucaibox/gmpt"}, "2203.02177": {"publish_time": "2022-03-04", "title": "GCNet: Graph Completion Network for Incomplete Multimodal Learning in Conversation", "author": "Zheng Lian et.al.", "abstract": "Conversations have become a critical data format on social media platforms. Understanding conversation from emotion, content, and other aspects also attracts increasing attention from researchers due to its widespread application in human-computer interaction. In real-world environments, we often encounter the problem of incomplete modalities, which has become a core issue of conversation understanding. To address this problem, researchers propose various methods. However, existing approaches are mainly designed for individual utterances or medical images rather than conversational data, which cannot exploit temporal and speaker information in conversations. To this end, we propose a novel framework for incomplete multimodal learning in conversations, called \"Graph Complete Network (GCNet)\", filling the gap of existing works. Our GCNet contains two well-designed graph neural network-based modules, \"Speaker GNN\" and \"Temporal GNN\", to capture temporal and speaker information in conversations. To make full use of complete and incomplete data in feature learning, we jointly optimize classification and reconstruction in an end-to-end manner. To verify the effectiveness of our method, we conduct experiments on three benchmark conversational datasets. Experimental results demonstrate that our GCNet is superior to existing state-of-the-art approaches in incomplete multimodal learning.", "paper_url": "http://arxiv.org/abs/2203.02177v1", "pdf_url": "http://arxiv.org/pdf/2203.02177v1", "repo_url": null}, "2203.02150": {"publish_time": "2022-03-04", "title": "Time-aware Graph Neural Networks for Entity Alignment between Temporal Knowledge Graphs", "author": "Chengjin_Xu et.al.", "abstract": "Entity alignment aims to identify equivalent entity pairs between different knowledge graphs (KGs). Recently, the availability of temporal KGs (TKGs) that contain time information created the need for reasoning over time in such TKGs. Existing embedding-based entity alignment approaches disregard time information that commonly exists in many large-scale KGs, leaving much room for improvement. In this paper, we focus on the task of aligning entity pairs between TKGs and propose a novel Time-aware Entity Alignment approach based on Graph Neural Networks (TEA-GNN). We embed entities, relations and timestamps of different KGs into a vector space and use GNNs to learn entity representations. To incorporate both relation and time information into the GNN structure of our model, we use a time-aware attention mechanism which assigns different weights to different nodes with orthogonal transformation matrices computed from embeddings of the relevant relations and timestamps in a neighborhood. Experimental results on multiple real-world TKG datasets show that our method significantly outperforms the state-of-the-art methods due to the inclusion of time information.", "paper_url": "http://arxiv.org/abs/2203.02150v1", "pdf_url": "http://arxiv.org/pdf/2203.02150v1", "repo_url": "https://github.com/soledad921/tea-gnn"}, "2203.02018": {"publish_time": "2022-03-03", "title": "Zero-shot Domain Adaptation of Heterogeneous Graphs via Knowledge Transfer Networks", "author": "Minji Yoon et.al.", "abstract": "How can we make predictions for nodes in a heterogeneous graph when an entire type of node (e.g. user) has no labels (perhaps due to privacy issues) at all? Although heterogeneous graph neural networks (HGNNs) have shown superior performance as powerful representation learning techniques, there is no direct way to learn using labels rooted at different node types. Domain adaptation (DA) targets this setting, however, existing DA can not be applied directly to HGNNs. In heterogeneous graphs, the source and target domains have different modalities, thus HGNNs provide different feature extractors to them, while most of DA assumes source and target domains share a common feature extractor. In this work, we address the issue of zero-shot domain adaptation in HGNNs. We first theoretically induce a relationship between source and target domain features extracted from HGNNs, then propose a novel domain adaptation method, Knowledge Transfer Networks for HGNNs (HGNN-KTN). HGNN-KTN learns the relationship between source and target features, then maps the target distributions into the source domain. HGNN-KTN outperforms state-of-the-art baselines, showing up to 73.3% higher in MRR on 18 different domain adaptation tasks running on real-world benchmark graphs.", "paper_url": "http://arxiv.org/abs/2203.02018v1", "pdf_url": "http://arxiv.org/pdf/2203.02018v1", "repo_url": null}, "2203.03610": {"publish_time": "2022-03-07", "title": "ZippyPoint: Fast Interest Point Detection, Description, and Matching through Mixed Precision Discretization", "author": "Simon Maurer et.al.", "abstract": "The design of more complex and powerful neural network models has significantly advanced the state-of-the-art in local feature detection and description. These advances can be attributed to deeper networks, improved training methodologies through self-supervision, or the introduction of new building blocks, such as graph neural networks for feature matching. However, in the pursuit of increased performance, efficient architectures that generate lightweight descriptors have received surprisingly little attention. In this paper, we investigate the adaptations neural networks for detection and description require in order to enable their use in embedded platforms. To that end, we investigate and adapt network quantization techniques for use in real-time applications. In addition, we revisit common practices in descriptor quantization and propose the use of a binary descriptor normalization layer, enabling the generation of distinctive length-invariant binary descriptors. ZippyPoint, our efficient network, runs at 47.2 fps on the Apple M1 CPU. This is up to 5x faster than other learned detection and description models, making it the only real-time learned network. ZippyPoint consistently outperforms all other binary detection and descriptor methods in visual localization and homography estimation tasks. Code and trained models will be released upon publication.", "paper_url": "http://arxiv.org/abs/2203.03610v1", "pdf_url": "http://arxiv.org/pdf/2203.03610v1", "repo_url": null}, "2203.03457": {"publish_time": "2022-03-07", "title": "Graph Neural Networks for Image Classification and Reinforcement Learning using Graph representations", "author": "Naman Goyal et.al.", "abstract": "In this paper, we will evaluate the performance of graph neural networks in two distinct domains: computer vision and reinforcement learning. In the computer vision section, we seek to learn whether a novel non-redundant representation for images as graphs can improve performance over trivial pixel to node mapping on a graph-level prediction graph, specifically image classification. For the reinforcement learning section, we seek to learn if explicitly modeling solving a Rubik's cube as a graph problem can improve performance over a standard model-free technique with no inductive bias.", "paper_url": "http://arxiv.org/abs/2203.03457v1", "pdf_url": "http://arxiv.org/pdf/2203.03457v1", "repo_url": null}, "2203.03195": {"publish_time": "2022-03-07", "title": "Unpaired Image Captioning by Image-level Weakly-Supervised Visual Concept Recognition", "author": "Peipei Zhu et.al.", "abstract": "The goal of unpaired image captioning (UIC) is to describe images without using image-caption pairs in the training phase. Although challenging, we except the task can be accomplished by leveraging a training set of images aligned with visual concepts. Most existing studies use off-the-shelf algorithms to obtain the visual concepts because the Bounding Box (BBox) labels or relationship-triplet labels used for the training are expensive to acquire. In order to resolve the problem in expensive annotations, we propose a novel approach to achieve cost-effective UIC. Specifically, we adopt image-level labels for the optimization of the UIC model in a weakly-supervised manner. For each image, we assume that only the image-level labels are available without specific locations and numbers. The image-level labels are utilized to train a weakly-supervised object recognition model to extract object information (e.g., instance) in an image, and the extracted instances are adopted to infer the relationships among different objects based on an enhanced graph neural network (GNN). The proposed approach achieves comparable or even better performance compared with previous methods without the expensive cost of annotations. Furthermore, we design an unrecognized object (UnO) loss combined with a visual concept reward to improve the alignment of the inferred object and relationship information with the images. It can effectively alleviate the issue encountered by existing UIC models about generating sentences with nonexistent objects. To the best of our knowledge, this is the first attempt to solve the problem of Weakly-Supervised visual concept recognition for UIC (WS-UIC) based only on image-level labels. Extensive experiments have been carried out to demonstrate that the proposed WS-UIC model achieves inspiring results on the COCO dataset while significantly reducing the cost of labeling.", "paper_url": "http://arxiv.org/abs/2203.03195v1", "pdf_url": "http://arxiv.org/pdf/2203.03195v1", "repo_url": null}, "2203.03153": {"publish_time": "2022-03-07", "title": "Scalable Verification of GNN-based Job Schedulers", "author": "Haoze Wu et.al.", "abstract": "Recently, Graph Neural Networks (GNNs) have been applied for scheduling jobs over clusters achieving better performance than hand-crafted heuristics. Despite their impressive performance, concerns remain over their trustworthiness when deployed in a real-world environment due to their black-box nature. To address these limitations, we consider formal verification of their expected properties such as strategy proofness and locality in this work. We address several domain-specific challenges such as deeper networks and richer specifications not encountered by existing verifiers for image and NLP classifiers. We develop GNN-Verify, the first general framework for verifying both single-step and multi-step properties of these schedulers based on carefully designed algorithms that combine abstractions, refinements, solvers, and proof transfer. Our experimental results on challenging benchmarks show that our approach can provide precise and scalable formal guarantees on the trustworthiness of state-of-the-art GNN-based scheduler.", "paper_url": "http://arxiv.org/abs/2203.03153v1", "pdf_url": "http://arxiv.org/pdf/2203.03153v1", "repo_url": null}, "2203.03145": {"publish_time": "2022-03-07", "title": "End-to-end video instance segmentation via spatial-temporal graph neural networks", "author": "Tao Wang et.al.", "abstract": "Video instance segmentation is a challenging task that extends image instance segmentation to the video domain. Existing methods either rely only on single-frame information for the detection and segmentation subproblems or handle tracking as a separate post-processing step, which limit their capability to fully leverage and share useful spatial-temporal information for all the subproblems. In this paper, we propose a novel graph-neural-network (GNN) based method to handle the aforementioned limitation. Specifically, graph nodes representing instance features are used for detection and segmentation while graph edges representing instance relations are used for tracking. Both inter and intra-frame information is effectively propagated and shared via graph updates and all the subproblems (i.e. detection, segmentation and tracking) are jointly optimized in an unified framework. The performance of our method shows great improvement on the YoutubeVIS validation dataset compared to existing methods and achieves 35.2% AP with a ResNet-50 backbone, operating at 22 FPS. Code is available at http://github.com/lucaswithai/visgraph.git .", "paper_url": "http://arxiv.org/abs/2203.03145v1", "pdf_url": "http://arxiv.org/pdf/2203.03145v1", "repo_url": "https://github.com/lucaswithai/visgraph"}, "2203.03991": {"publish_time": "2022-03-08", "title": "Sparsification and Filtering for Spatial-temporal GNN in Multivariate Time-series", "author": "Yuanrong Wang et.al.", "abstract": "We propose an end-to-end architecture for multivariate time-series prediction that integrates a spatial-temporal graph neural network with a matrix filtering module. This module generates filtered (inverse) correlation graphs from multivariate time series before inputting them into a GNN. In contrast with existing sparsification methods adopted in graph neural network, our model explicitly leverage time-series filtering to overcome the low signal-to-noise ratio typical of complex systems data. We present a set of experiments, where we predict future sales from a synthetic time-series sales dataset. The proposed spatial-temporal graph neural network displays superior performances with respect to baseline approaches, with no graphical information, and with fully connected, disconnected graphs and unfiltered graphs.", "paper_url": "http://arxiv.org/abs/2203.03991v1", "pdf_url": "http://arxiv.org/pdf/2203.03991v1", "repo_url": null}, "2203.03965": {"publish_time": "2022-03-08", "title": "Few-Shot Traffic Prediction with Graph Networks using Locale as Relational Inductive Biases", "author": "Mingxi Li et.al.", "abstract": "Accurate short-term traffic prediction plays a pivotal role in various smart mobility operation and management systems. Currently, most of the state-of-the-art prediction models are based on graph neural networks (GNNs), and the required training samples are proportional to the size of the traffic network. In many cities, the available amount of traffic data is substantially below the minimum requirement due to the data collection expense. It is still an open question to develop traffic prediction models with a small size of training data on large-scale networks. We notice that the traffic states of a node for the near future only depend on the traffic states of its localized neighborhoods, which can be represented using the graph relational inductive biases. In view of this, this paper develops a graph network (GN)-based deep learning model LocaleGn that depicts the traffic dynamics using localized data aggregating and updating functions, as well as the node-wise recurrent neural networks. LocaleGn is a light-weighted model designed for training on few samples without over-fitting, and hence it can solve the problem of few-shot traffic prediction. The proposed model is examined on predicting both traffic speed and flow with six datasets, and the experimental results demonstrate that LocaleGn outperforms existing state-of-the-art baseline models. It is also demonstrated that the learned knowledge from LocaleGn can be transferred across cities. The research outcomes can help to develop light-weighted traffic prediction systems, especially for cities lacking in historically archived traffic data.", "paper_url": "http://arxiv.org/abs/2203.03965v1", "pdf_url": "http://arxiv.org/pdf/2203.03965v1", "repo_url": "https://github.com/mingxilii/localegn"}, "2203.03906": {"publish_time": "2022-03-08", "title": "Graph Reinforcement Learning for Predictive Power Allocation to Mobile Users", "author": "Jianyu Zhao et.al.", "abstract": "Allocating resources with future channels can save resource to ensure quality-of-service of video streaming. In this paper, we optimize predictive power allocation to minimize the energy consumed at distributed units (DUs) by using deep deterministic policy gradient (DDPG) to find optimal policy and predict average channel gains. To improve training efficiency, we resort to graph DDPG for exploiting two kinds of relational priors: (a) permutation equivariant (PE) and permutation invariant (PI) properties of policy function and action-value function, (b) topology relation among users and DUs. To design graph DDPG framework more systematically in harnessing the priors, we first demonstrate how to transform matrix-based DDPG into graph-based DDPG. Then, we respectively design the actor and critic networks to satisfy the permutation properties when graph neural networks are used in embedding and end to-end manners. To avoid destroying the PE/PI properties of the actor and critic networks, we conceive a batch normalization method. Finally, we show the impact of leveraging each prior. Simulation results show that the learned predictive policy performs close to the optimal solution with perfect future information, and the graph DDPG algorithms converge much faster than existing DDPG algorithms.", "paper_url": "http://arxiv.org/abs/2203.03906v1", "pdf_url": "http://arxiv.org/pdf/2203.03906v1", "repo_url": null}, "2203.03806": {"publish_time": "2022-03-08", "title": "Panoramic Human Activity Recognition", "author": "Ruize Han et.al.", "abstract": "To obtain a more comprehensive activity understanding for a crowded scene, in this paper, we propose a new problem of panoramic human activity recognition (PAR), which aims to simultaneous achieve the individual action, social group activity, and global activity recognition. This is a challenging yet practical problem in real-world applications. For this problem, we develop a novel hierarchical graph neural network to progressively represent and model the multi-granularity human activities and mutual social relations for a crowd of people. We further build a benchmark to evaluate the proposed method and other existing related methods. Experimental results verify the rationality of the proposed PAR problem, the effectiveness of our method and the usefulness of the benchmark. We will release the source code and benchmark to the public for promoting the study on this problem.", "paper_url": "http://arxiv.org/abs/2203.03806v1", "pdf_url": "http://arxiv.org/pdf/2203.03806v1", "repo_url": null}, "2203.04910": {"publish_time": "2022-03-09", "title": "BaM: A Case for Enabling Fine-grain High Throughput GPU-Orchestrated Access to Storage", "author": "Zaid Qureshi et.al.", "abstract": "Accelerators like Graphics Processing Units (GPUs) have been increasingly deployed in modern data centers because of their compute capabilities and memory bandwidth. These accelerators have traditionally relied on the \"application host code\" and the OS running on the CPU to orchestrate their access to the data storage devices. CPU orchestration of storage data accesses works well for classic GPU applications, like dense neural network training, where data access patterns are predefined, regular, dense, and independent of the data values, enabling the CPU to partition the storage data into coarse-grain chunks and coordinate the storage device accesses and data transfers to the accelerators. Unfortunately, such a CPU-centric strategy causes excessive CPU-GPU synchronization overhead and/or I/O traffic amplification, diminishing the effective storage bandwidth for emerging applications with fine-grain data-dependent access patterns like graph and data analytics, recommender systems, and graph neural networks. In this work, we make a case for enabling GPUs to orchestrate high-throughput, fine-grain accesses into NVMe Solid State Drives (SSDs) in a new system architecture called BaM. BaM mitigates the I/O traffic amplification by enabling the GPU threads to read or write small amounts of data on-demand, as determined by the compute. We show that (1) the BaM infrastructure software running on GPUs can identify and communicate the fine-grain accesses at a sufficiently high rate to fully utilize the underlying storage devices, (2) even with consumer-grade SSDs, a BaM system can support application performance that is competitive against a much more expensive DRAM-only solution, and (3) the reduction in I/O amplification can yield significant performance benefit.", "paper_url": "http://arxiv.org/abs/2203.04910v1", "pdf_url": "http://arxiv.org/pdf/2203.04910v1", "repo_url": null}, "2203.04746": {"publish_time": "2022-03-09", "title": "SkinningNet: Two-Stream Graph Convolutional Neural Network for Skinning Prediction of Synthetic Characters", "author": "Albert Mosella-Montoro et.al.", "abstract": "This work presents SkinningNet, an end-to-end Two-Stream Graph Neural Network architecture that computes skinning weights from an input mesh and its associated skeleton, without making any assumptions on shape class and structure of the provided mesh. Whereas previous methods pre-compute handcrafted features that relate the mesh and the skeleton or assume a fixed topology of the skeleton, the proposed method extracts this information in an end-to-end learnable fashion by jointly learning the best relationship between mesh vertices and skeleton joints. The proposed method exploits the benefits of the novel Multi-Aggregator Graph Convolution that combines the results of different aggregators during the summarizing step of the Message-Passing scheme, helping the operation to generalize for unseen topologies. Experimental results demonstrate the effectiveness of the contributions of our novel architecture, with SkinningNet outperforming current state-of-the-art alternatives.", "paper_url": "http://arxiv.org/abs/2203.04746v1", "pdf_url": "http://arxiv.org/pdf/2203.04746v1", "repo_url": null}, "2203.05380": {"publish_time": "2022-03-10", "title": "Spatial Commonsense Graph for Object Localisation in Partial Scenes", "author": "Francesco Giuliari et.al.", "abstract": "We solve object localisation in partial scenes, a new problem of estimating the unknown position of an object (e.g. where is the bag?) given a partial 3D scan of a scene. The proposed solution is based on a novel scene graph model, the Spatial Commonsense Graph (SCG), where objects are the nodes and edges define pairwise distances between them, enriched by concept nodes and relationships from a commonsense knowledge base. This allows SCG to better generalise its spatial inference over unknown 3D scenes. The SCG is used to estimate the unknown position of the target object in two steps: first, we feed the SCG into a novel Proximity Prediction Network, a graph neural network that uses attention to perform distance prediction between the node representing the target object and the nodes representing the observed objects in the SCG; second, we propose a Localisation Module based on circular intersection to estimate the object position using all the predicted pairwise distances in order to be independent of any reference system. We create a new dataset of partially reconstructed scenes to benchmark our method and baselines for object localisation in partial scenes, where our proposed method achieves the best localisation performance.", "paper_url": "http://arxiv.org/abs/2203.05380v1", "pdf_url": "http://arxiv.org/pdf/2203.05380v1", "repo_url": "https://github.com/fgiuliari/spatialcommonsensegraph-dataset"}, "2203.05181": {"publish_time": "2022-03-10", "title": "LineVD: Statement-level Vulnerability Detection using Graph Neural Networks", "author": "David Hin et.al.", "abstract": "Current machine-learning based software vulnerability detection methods are primarily conducted at the function-level. However, a key limitation of these methods is that they do not indicate the specific lines of code contributing to vulnerabilities. This limits the ability of developers to efficiently inspect and interpret the predictions from a learnt model, which is crucial for integrating machine-learning based tools into the software development workflow. Graph-based models have shown promising performance in function-level vulnerability detection, but their capability for statement-level vulnerability detection has not been extensively explored. While interpreting function-level predictions through explainable AI is one promising direction, we herein consider the statement-level software vulnerability detection task from a fully supervised learning perspective. We propose a novel deep learning framework, LineVD, which formulates statement-level vulnerability detection as a node classification task. LineVD leverages control and data dependencies between statements using graph neural networks, and a transformer-based model to encode the raw source code tokens. In particular, by addressing the conflicting outputs between function-level and statement-level information, LineVD significantly improve the prediction performance without vulnerability status for function code. We have conducted extensive experiments against a large-scale collection of real-world C/C++ vulnerabilities obtained from multiple real-world projects, and demonstrate an increase of 105\\% in F1-score over the current state-of-the-art.", "paper_url": "http://arxiv.org/abs/2203.05181v1", "pdf_url": "http://arxiv.org/pdf/2203.05181v1", "repo_url": null}, "2203.05144": {"publish_time": "2022-03-10", "title": "Earthquake Location and Magnitude Estimation with Graph Neural Networks", "author": "Ian W. McBrearty et.al.", "abstract": "We solve the traditional problems of earthquake location and magnitude estimation through a supervised learning approach, where we train a Graph Neural Network to predict estimates directly from input pick data, and each input allows a distinct seismic network with variable number of stations and positions. We train the model using synthetic simulations from assumed travel-time and amplitude-distance attenuation models. The architecture uses one graph to represent the station set, and another to represent the model space. The input includes theoretical predictions of data, given model parameters, and the adjacency matrices of the graphs defined link spatially local elements. As we show, graph convolutions on this combined representation are highly effective at inference, data fusion, and outlier suppression. We compare our results with traditional methods and observe favorable performance.", "paper_url": "http://arxiv.org/abs/2203.05144v1", "pdf_url": "http://arxiv.org/pdf/2203.05144v1", "repo_url": null}, "2203.05095": {"publish_time": "2022-03-10", "title": "Model-Architecture Co-Design for High Performance Temporal GNN Inference on FPGA", "author": "Hongkuan Zhou et.al.", "abstract": "Temporal Graph Neural Networks (TGNNs) are powerful models to capture temporal, structural, and contextual information on temporal graphs. The generated temporal node embeddings outperform other methods in many downstream tasks. Real-world applications require high performance inference on real-time streaming dynamic graphs. However, these models usually rely on complex attention mechanisms to capture relationships between temporal neighbors. In addition, maintaining vertex memory suffers from intrinsic temporal data dependency that hinders task-level parallelism, making it inefficient on general-purpose processors. In this work, we present a novel model-architecture co-design for inference in memory-based TGNNs on FPGAs. The key modeling optimizations we propose include a light-weight method to compute attention scores and a related temporal neighbor pruning strategy to further reduce computation and memory accesses. These are holistically coupled with key hardware optimizations that leverage FPGA hardware. We replace the temporal sampler with an on-chip FIFO based hardware sampler and the time encoder with a look-up-table. We train our simplified models using knowledge distillation to ensure similar accuracy vis-\\'a-vis the original model. Taking advantage of the model optimizations, we propose a principled hardware architecture using batching, pipelining, and prefetching techniques to further improve the performance. We also propose a hardware mechanism to ensure the chronological vertex updating without sacrificing the computation parallelism. We evaluate the performance of the proposed hardware accelerator on three real-world datasets.", "paper_url": "http://arxiv.org/abs/2203.05095v1", "pdf_url": "http://arxiv.org/pdf/2203.05095v1", "repo_url": "https://github.com/zjjzby/tgnn-fpga-ipdps2022"}, "2203.05046": {"publish_time": "2022-03-09", "title": "Adaptive Trajectory Prediction via Transferable GNN", "author": "Yi Xu et.al.", "abstract": "Pedestrian trajectory prediction is an essential component in a wide range of AI applications such as autonomous driving and robotics. Existing methods usually assume the training and testing motions follow the same pattern while ignoring the potential distribution differences (e.g., shopping mall and street). This issue results in inevitable performance decrease. To address this issue, we propose a novel Transferable Graph Neural Network (T-GNN) framework, which jointly conducts trajectory prediction as well as domain alignment in a unified framework. Specifically, a domain invariant GNN is proposed to explore the structural motion knowledge where the domain specific knowledge is reduced. Moreover, an attention-based adaptive knowledge learning module is further proposed to explore fine-grained individual-level feature representation for knowledge transfer. By this way, disparities across different trajectory domains will be better alleviated. More challenging while practical trajectory prediction experiments are designed, and the experimental results verify the superior performance of our proposed model. To the best of our knowledge, our work is the pioneer which fills the gap in benchmarks and techniques for practical pedestrian trajectory prediction across different domains.", "paper_url": "http://arxiv.org/abs/2203.05046v1", "pdf_url": "http://arxiv.org/pdf/2203.05046v1", "repo_url": null}, "2203.05985": {"publish_time": "2022-03-11", "title": "Graph Neural Networks for Relational Inductive Bias in Vision-based Deep Reinforcement Learning of Robot Control", "author": "Marco Oliva et.al.", "abstract": "State-of-the-art reinforcement learning algorithms predominantly learn a policy from either a numerical state vector or images. Both approaches generally do not take structural knowledge of the task into account, which is especially prevalent in robotic applications and can benefit learning if exploited. This work introduces a neural network architecture that combines relational inductive bias and visual feedback to learn an efficient position control policy for robotic manipulation. We derive a graph representation that models the physical structure of the manipulator and combines the robot's internal state with a low-dimensional description of the visual scene generated by an image encoding network. On this basis, a graph neural network trained with reinforcement learning predicts joint velocities to control the robot. We further introduce an asymmetric approach of training the image encoder separately from the policy using supervised learning. Experimental results demonstrate that, for a 2-DoF planar robot in a geometrically simplistic 2D environment, a learned representation of the visual scene can replace access to the explicit coordinates of the reaching target without compromising on the quality and sample efficiency of the policy. We further show the ability of the model to improve sample efficiency for a 6-DoF robot arm in a visually realistic 3D environment.", "paper_url": "http://arxiv.org/abs/2203.05985v1", "pdf_url": "http://arxiv.org/pdf/2203.05985v1", "repo_url": null}, "2203.05919": {"publish_time": "2022-03-11", "title": "Graph Summarization with Graph Neural Networks", "author": "Maximilian Blasi et.al.", "abstract": "The goal of graph summarization is to represent large graphs in a structured and compact way. A graph summary based on equivalence classes preserves pre-defined features of a graph's vertex within a $k$-hop neighborhood such as the vertex labels and edge labels. Based on these neighborhood characteristics, the vertex is assigned to an equivalence class. The calculation of the assigned equivalence class must be a permutation invariant operation on the pre-defined features. This is achieved by sorting on the feature values, e. g., the edge labels, which is computationally expensive, and subsequently hashing the result. Graph Neural Networks (GNN) fulfill the permutation invariance requirement. We formulate the problem of graph summarization as a subgraph classification task on the root vertex of the $k$-hop neighborhood. We adapt different GNN architectures, both based on the popular message-passing protocol and alternative approaches, to perform the structural graph summarization task. We compare different GNNs with a standard multi-layer perceptron (MLP) and Bloom filter as non-neural method. For our experiments, we consider four popular graph summary models on a large web graph. This resembles challenging multi-class vertex classification tasks with the numbers of classes ranging from $576$ to multiple hundreds of thousands. Our results show that the performance of GNNs are close to each other. In three out of four experiments, the non-message-passing GraphMLP model outperforms the other GNNs. The performance of the standard MLP is extraordinary good, especially in the presence of many classes. Finally, the Bloom filter outperforms all neural architectures by a large margin, except for the dataset with the fewest number of $576$ classes.", "paper_url": "http://arxiv.org/abs/2203.05919v1", "pdf_url": "http://arxiv.org/pdf/2203.05919v1", "repo_url": null}, "2203.07353": {"publish_time": "2022-03-14", "title": "Improving Di-Higgs Sensitivity at Future Colliders in Hadronic Final States with Machine Learning", "author": "Daniel Diaz et.al.", "abstract": "One of the central goals of the physics program at the future colliders is to elucidate the origin of electroweak symmetry breaking, including precision measurements of the Higgs sector. This includes a detailed study of Higgs boson (H) pair production, which can reveal the H self-coupling. Since the discovery of the Higgs boson, a large campaign of measurements of the properties of the Higgs boson has begun and many new ideas have emerged during the completion of this program. One such idea is the use of highly boosted and merged hadronic decays of the Higgs boson ($\\mathrm{H}\\to\\mathrm{b}\\overline{\\mathrm{b}}$, $\\mathrm{H}\\to\\mathrm{W}\\mathrm{W}\\to\\mathrm{q}\\overline{\\mathrm{q}}\\mathrm{q}\\overline{\\mathrm{q}}$) with machine learning methods to improve the signal-to-background discrimination. In this white paper, we champion the use of these modes to boost the sensitivity of future collider physics programs to Higgs boson pair production, the Higgs self-coupling, and Higgs-vector boson couplings. We demonstrate the potential improvement possible thanks to use of graph neural networks.", "paper_url": "http://arxiv.org/abs/2203.07353v1", "pdf_url": "http://arxiv.org/pdf/2203.07353v1", "repo_url": null}, "2203.06944": {"publish_time": "2022-03-14", "title": "Towards Neural Sparse Linear Solvers", "author": "Luca Grementieri et.al.", "abstract": "Large sparse symmetric linear systems appear in several branches of science and engineering thanks to the widespread use of the finite element method (FEM). The fastest sparse linear solvers available implement hybrid iterative methods. These methods are based on heuristic algorithms to permute rows and columns or find a preconditioner matrix. In addition, they are inherently sequential, making them unable to leverage the GPU processing power entirely. We propose neural sparse linear solvers, a deep learning framework to learn approximate solvers for sparse symmetric linear systems. Our method relies on representing a sparse symmetric linear system as an undirected weighted graph. Such graph representation is inherently permutation-equivariant and scale-invariant, and it can become the input to a graph neural network trained to regress the solution. We test neural sparse linear solvers on static linear analysis problems from structural engineering. Our method is less accurate than classic algorithms, but it is hardware-independent, fast on GPUs, and applicable to generic sparse symmetric systems without any additional hypothesis. Although many limitations remain, this study shows a general approach to tackle problems involving sparse symmetric matrices using graph neural networks.", "paper_url": "http://arxiv.org/abs/2203.06944v1", "pdf_url": "http://arxiv.org/pdf/2203.06944v1", "repo_url": null}, "2203.06852": {"publish_time": "2022-03-14", "title": "Continual Learning for Multivariate Time Series Tasks with Variable Input Dimensions", "author": "Vibhor Gupta et.al.", "abstract": "We consider a sequence of related multivariate time series learning tasks, such as predicting failures for different instances of a machine from time series of multi-sensor data, or activity recognition tasks over different individuals from multiple wearable sensors. We focus on two under-explored practical challenges arising in such settings: (i) Each task may have a different subset of sensors, i.e., providing different partial observations of the underlying 'system'. This restriction can be due to different manufacturers in the former case, and people wearing more or less measurement devices in the latter (ii) We are not allowed to store or re-access data from a task once it has been observed at the task level. This may be due to privacy considerations in the case of people, or legal restrictions placed by machine owners. Nevertheless, we would like to (a) improve performance on subsequent tasks using experience from completed tasks as well as (b) continue to perform better on past tasks, e.g., update the model and improve predictions on even the first machine after learning from subsequently observed ones. We note that existing continual learning methods do not take into account variability in input dimensions arising due to different subsets of sensors being available across tasks, and struggle to adapt to such variable input dimensions (VID) tasks. In this work, we address this shortcoming of existing methods. To this end, we learn task-specific generative models and classifiers, and use these to augment data for target tasks. Since the input dimensions across tasks vary, we propose a novel conditioning module based on graph neural networks to aid a standard recurrent neural network. We evaluate the efficacy of the proposed approach on three publicly available datasets corresponding to two activity recognition tasks (classification) and one prognostics task (regression).", "paper_url": "http://arxiv.org/abs/2203.06852v1", "pdf_url": "http://arxiv.org/pdf/2203.06852v1", "repo_url": null}, "2203.06778": {"publish_time": "2022-03-13", "title": "Pruned Graph Neural Network for Short Story Ordering", "author": "Melika Golestani et.al.", "abstract": "Text coherence is a fundamental problem in natural language generation and understanding. Organizing sentences into an order that maximizes coherence is known as sentence ordering. This paper is proposing a new approach based on the graph neural network approach to encode a set of sentences and learn orderings of short stories. We propose a new method for constructing sentence-entity graphs of short stories to create the edges between sentences and reduce noise in our graph by replacing the pronouns with their referring entities. We improve the sentence ordering by introducing an aggregation method based on majority voting of state-of-the-art methods and our proposed one. Our approach employs a BERT-based model to learn semantic representations of the sentences. The results demonstrate that the proposed method significantly outperforms existing baselines on a corpus of short stories with a new state-of-the-art performance in terms of Perfect Match Ratio (PMR) and Kendall's Tau (Tau) metrics. More precisely, our method increases PMR and Tau criteria by more than 5% and 4.3%, respectively. These outcomes highlight the benefit of forming the edges between sentences based on their cosine similarity. We also observe that replacing pronouns with their referring entities effectively encodes sentences in sentence-entity graphs.", "paper_url": "http://arxiv.org/abs/2203.06778v1", "pdf_url": "http://arxiv.org/pdf/2203.06778v1", "repo_url": null}, "2203.06442": {"publish_time": "2022-03-12", "title": "Equivariant Graph Mechanics Networks with Constraints", "author": "Wenbing Huang et.al.", "abstract": "Learning to reason about relations and dynamics over multiple interacting objects is a challenging topic in machine learning. The challenges mainly stem from that the interacting systems are exponentially-compositional, symmetrical, and commonly geometrically-constrained. Current methods, particularly the ones based on equivariant Graph Neural Networks (GNNs), have targeted on the first two challenges but remain immature for constrained systems. In this paper, we propose Graph Mechanics Network (GMN) which is combinatorially efficient, equivariant and constraint-aware. The core of GMN is that it represents, by generalized coordinates, the forward kinematics information (positions and velocities) of a structural object. In this manner, the geometrical constraints are implicitly and naturally encoded in the forward kinematics. Moreover, to allow equivariant message passing in GMN, we have developed a general form of orthogonality-equivariant functions, given that the dynamics of constrained systems are more complicated than the unconstrained counterparts. Theoretically, the proposed equivariant formulation is proved to be universally expressive under certain conditions. Extensive experiments support the advantages of GMN compared to the state-of-the-art GNNs in terms of prediction accuracy, constraint satisfaction and data efficiency on the simulated systems consisting of particles, sticks and hinges, as well as two real-world datasets for molecular dynamics prediction and human motion capture.", "paper_url": "http://arxiv.org/abs/2203.06442v1", "pdf_url": "http://arxiv.org/pdf/2203.06442v1", "repo_url": "https://github.com/hanjq17/gmn"}, "2203.07977": {"publish_time": "2022-03-15", "title": "OcclusionFusion: Occlusion-aware Motion Estimation for Real-time Dynamic 3D Reconstruction", "author": "Wenbin Lin et.al.", "abstract": "RGBD-based real-time dynamic 3D reconstruction suffers from inaccurate inter-frame motion estimation as errors may accumulate with online tracking. This problem is even more severe for single-view-based systems due to strong occlusions. Based on these observations, we propose OcclusionFusion, a novel method to calculate occlusion-aware 3D motion to guide the reconstruction. In our technique, the motion of visible regions is first estimated and combined with temporal information to infer the motion of the occluded regions through an LSTM-involved graph neural network. Furthermore, our method computes the confidence of the estimated motion by modeling the network output with a probabilistic model, which alleviates untrustworthy motions and enables robust tracking. Experimental results on public datasets and our own recorded data show that our technique outperforms existing single-view-based real-time methods by a large margin. With the reduction of the motion errors, the proposed technique can handle long and challenging motion sequences. Please check out the project page for sequence results: https://wenbin-lin.github.io/OcclusionFusion.", "paper_url": "http://arxiv.org/abs/2203.07977v1", "pdf_url": "http://arxiv.org/pdf/2203.07977v1", "repo_url": null}, "2203.07969": {"publish_time": "2022-03-15", "title": "PDNS-Net: A Large Heterogeneous Graph Benchmark Dataset of Network Resolutions for Graph Learning", "author": "Udesh Kumarasinghe et.al.", "abstract": "In order to advance the state of the art in graph learning algorithms, it is necessary to construct large real-world datasets. While there are many benchmark datasets for homogeneous graphs, only a few of them are available for heterogeneous graphs. Furthermore, the latter graphs are small in size rendering them insufficient to understand how graph learning algorithms perform in terms of classification metrics and computational resource utilization. We introduce, PDNS-Net, the largest public heterogeneous graph dataset containing 447K nodes and 897K edges for the malicious domain classification task. Compared to the popular heterogeneous datasets IMDB and DBLP, PDNS-Net is 38 and 17 times bigger respectively. We provide a detailed analysis of PDNS-Net including the data collection methodology, heterogeneous graph construction, descriptive statistics and preliminary graph classification performance. The dataset is publicly available at https://github.com/qcri/PDNS-Net. Our preliminary evaluation of both popular homogeneous and heterogeneous graph neural networks on PDNS-Net reveals that further research is required to improve the performance of these models on large heterogeneous graphs.", "paper_url": "http://arxiv.org/abs/2203.07969v1", "pdf_url": "http://arxiv.org/pdf/2203.07969v1", "repo_url": "https://github.com/qcri/pdns-net"}, "2203.07961": {"publish_time": "2022-03-15", "title": "Amortised inference of fractional Brownian motion with linear computational complexity", "author": "Fran\u00e7ois Laurent et.al.", "abstract": "We introduce a simulation-based, amortised Bayesian inference scheme to infer the parameters of random walks. Our approach learns the posterior distribution of the walks' parameters with a likelihood-free method. In the first step a graph neural network is trained on simulated data to learn optimized low-dimensional summary statistics of the random walk. In the second step an invertible neural network generates the posterior distribution of the parameters from the learnt summary statistics using variational inference. We apply our method to infer the parameters of the fractional Brownian motion model from single trajectories. The computational complexity of the amortized inference procedure scales linearly with trajectory length, and its precision scales similarly to the Cram{\\'e}r-Rao bound over a wide range of lengths. The approach is robust to positional noise, and generalizes well to trajectories longer than those seen during training. Finally, we adapt this scheme to show that a finite decorrelation time in the environment can furthermore be inferred from individual trajectories.", "paper_url": "http://arxiv.org/abs/2203.07961v1", "pdf_url": "http://arxiv.org/pdf/2203.07961v1", "repo_url": null}, "2203.07831": {"publish_time": "2022-03-15", "title": "Graph Neural Network Sensitivity Under Probabilistic Error Model", "author": "Xinjue Wang et.al.", "abstract": "Graph convolutional networks (GCNs) can successfully learn the graph signal representation by graph convolution. The graph convolution depends on the graph filter, which contains the topological dependency of data and propagates data features. However, the estimation errors in the propagation matrix (e.g., the adjacency matrix) can have a significant impact on graph filters and GCNs. In this paper, we study the effect of a probabilistic graph error model on the performance of the GCNs. We prove that the adjacency matrix under the error model is bounded by a function of graph size and error probability. We further analytically specify the upper bound of a normalized adjacency matrix with self-loop added. Finally, we illustrate the error bounds by running experiments on a synthetic dataset and study the sensitivity of a simple GCN under this probabilistic error model on accuracy.", "paper_url": "http://arxiv.org/abs/2203.07831v1", "pdf_url": "http://arxiv.org/pdf/2203.07831v1", "repo_url": null}, "2203.07691": {"publish_time": "2022-03-15", "title": "Supervised Contrastive Learning with Structure Inference for Graph Classification", "author": "Hao Jia et.al.", "abstract": "Advanced graph neural networks have shown great potentials in graph classification tasks recently. Different from node classification where node embeddings aggregated from local neighbors can be directly used to learn node labels, graph classification requires a hierarchical accumulation of different levels of topological information to generate discriminative graph embeddings. Still, how to fully explore graph structures and formulate an effective graph classification pipeline remains rudimentary. In this paper, we propose a novel graph neural network based on supervised contrastive learning with structure inference for graph classification. First, we propose a data-driven graph augmentation strategy that can discover additional connections to enhance the existing edge set. Concretely, we resort to a structure inference stage based on diffusion cascades to recover possible connections with high node similarities. Second, to improve the contrastive power of graph neural networks, we propose to use a supervised contrastive loss for graph classification. With the integration of label information, the one-vs-many contrastive learning can be extended to a many-vs-many setting, so that the graph-level embeddings with higher topological similarities will be pulled closer. The supervised contrastive loss and structure inference can be naturally incorporated within the hierarchical graph neural networks where the topological patterns can be fully explored to produce discriminative graph embeddings. Experiment results show the effectiveness of the proposed method compared with recent state-of-the-art methods.", "paper_url": "http://arxiv.org/abs/2203.07691v1", "pdf_url": "http://arxiv.org/pdf/2203.07691v1", "repo_url": null}, "2203.08654": {"publish_time": "2022-03-16", "title": "Graph Neural Networks for Multiparallel Word Alignment", "author": "Ayyoob Imani et.al.", "abstract": "After a period of decrease, interest in word alignments is increasing again for their usefulness in domains such as typological research, cross-lingual annotation projection, and machine translation. Generally, alignment algorithms only use bitext and do not make use of the fact that many parallel corpora are multiparallel. Here, we compute high-quality word alignments between multiple language pairs by considering all language pairs together. First, we create a multiparallel word alignment graph, joining all bilingual word alignment pairs in one graph. Next, we use graph neural networks (GNNs) to exploit the graph structure. Our GNN approach (i) utilizes information about the meaning, position, and language of the input words, (ii) incorporates information from multiple parallel sentences, (iii) adds and removes edges from the initial alignments, and (iv) yields a prediction model that can generalize beyond the training sentences. We show that community detection provides valuable information for multiparallel word alignment. Our method outperforms previous work on three word-alignment datasets and on a downstream task.", "paper_url": "http://arxiv.org/abs/2203.08654v1", "pdf_url": "http://arxiv.org/pdf/2203.08654v1", "repo_url": null}, "2203.08500": {"publish_time": "2022-03-16", "title": "HeterMPC: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations", "author": "Jia-Chen Gu et.al.", "abstract": "Recently, various response generation models for two-party conversations have achieved impressive improvements, but less effort has been paid to multi-party conversations (MPCs) which are more practical and complicated. Compared with a two-party conversation where a dialogue context is a sequence of utterances, building a response generation model for MPCs is more challenging, since there exist complicated context structures and the generated responses heavily rely on both interlocutors (i.e., speaker and addressee) and history utterances. To address these challenges, we present HeterMPC, a heterogeneous graph-based neural network for response generation in MPCs which models the semantics of utterances and interlocutors simultaneously with two types of nodes in a graph. Besides, we also design six types of meta relations with node-edge-type-dependent parameters to characterize the heterogeneous interactions within the graph. Through multi-hop updating, HeterMPC can adequately utilize the structural knowledge of conversations for response generation. Experimental results on the Ubuntu Internet Relay Chat (IRC) channel benchmark show that HeterMPC outperforms various baseline models for response generation in MPCs.", "paper_url": "http://arxiv.org/abs/2203.08500v1", "pdf_url": "http://arxiv.org/pdf/2203.08500v1", "repo_url": "https://github.com/lxchtan/hetermpc"}, "2203.09360": {"publish_time": "2022-03-17", "title": "Behavior-aware Account De-anonymization on Ethereum Interaction Graph", "author": "Jiajun Zhou et.al.", "abstract": "Blockchain technology has the characteristics of decentralization, traceability and tamper-proof, which creates a reliable decentralized trust mechanism, further accelerating the development of blockchain finance. However, the anonymization of blockchain hinders market regulation, resulting in increasing illegal activities such as money laundering, gambling and phishing fraud on blockchain financial platforms. Thus financial security has become a top priority in the blockchain ecosystem, calling for effective market regulation. In this paper, we consider identifying Ethereum accounts from a graph classification perspective, and propose an end-to-end graph neural network framework named Ethident, to characterize the behavior patterns of accounts and further achieve account de-anonymization. Specifically, we first construct an Account Interaction Graph (AIG) using raw Ethereum data. Then we design a hierarchical graph attention encoder named HGATE as the backbone of our framework, which can effectively characterize the node-level account features and subgraph-level behavior patterns. For alleviating account label sparsity, we further introduce contrastive self-supervision mechanism as regularization to jointly train our framework. Comprehensive experiments on Ethereum datasets demonstrate that our framework achieves superior performance in account identification, yielding 1.13% - 4.93% relative improvement over previous state-of-the-art. Furthermore, detailed analyses illustrate the effectiveness of Ethident in identifying and understanding the behavior of known participants in Ethereum (e.g. exchanges, miners, etc.), as well as that of the lawbreakers (e.g. phishing scammers, hackers, etc.), which may aid in risk assessment and market regulation.", "paper_url": "http://arxiv.org/abs/2203.09360v2", "pdf_url": "http://arxiv.org/pdf/2203.09360v2", "repo_url": null}, "2203.09258": {"publish_time": "2022-03-17", "title": "Explainability in Graph Neural Networks: An Experimental Survey", "author": "Peibo Li et.al.", "abstract": "Graph neural networks (GNNs) have been extensively developed for graph representation learning in various application domains. However, similar to all other neural networks models, GNNs suffer from the black-box problem as people cannot understand the mechanism underlying them. To solve this problem, several GNN explainability methods have been proposed to explain the decisions made by GNNs. In this survey, we give an overview of the state-of-the-art GNN explainability methods and how they are evaluated. Furthermore, we propose a new evaluation metric and conduct thorough experiments to compare GNN explainability methods on real world datasets. We also suggest future directions for GNN explainability.", "paper_url": "http://arxiv.org/abs/2203.09258v1", "pdf_url": "http://arxiv.org/pdf/2203.09258v1", "repo_url": null}, "2203.09205": {"publish_time": "2022-03-17", "title": "SoK: Differential Privacy on Graph-Structured Data", "author": "Tamara T. Mueller et.al.", "abstract": "In this work, we study the applications of differential privacy (DP) in the context of graph-structured data. We discuss the formulations of DP applicable to the publication of graphs and their associated statistics as well as machine learning on graph-based data, including graph neural networks (GNNs). The formulation of DP in the context of graph-structured data is difficult, as individual data points are interconnected (often non-linearly or sparsely). This connectivity complicates the computation of individual privacy loss in differentially private learning. The problem is exacerbated by an absence of a single, well-established formulation of DP in graph settings. This issue extends to the domain of GNNs, rendering private machine learning on graph-structured data a challenging task. A lack of prior systematisation work motivated us to study graph-based learning from a privacy perspective. In this work, we systematise different formulations of DP on graphs, discuss challenges and promising applications, including the GNN domain. We compare and separate works into graph analysis tasks and graph learning tasks with GNNs. Finally, we conclude our work with a discussion of open questions and potential directions for further research in this area.", "paper_url": "http://arxiv.org/abs/2203.09205v1", "pdf_url": "http://arxiv.org/pdf/2203.09205v1", "repo_url": null}, "2203.09141": {"publish_time": "2022-03-17", "title": "Graph Representation Learning with Individualization and Refinement", "author": "Mohammed Haroon Dupty et.al.", "abstract": "Graph Neural Networks (GNNs) have emerged as prominent models for representation learning on graph structured data. GNNs follow an approach of message passing analogous to 1-dimensional Weisfeiler Lehman (1-WL) test for graph isomorphism and consequently are limited by the distinguishing power of 1-WL. More expressive higher-order GNNs which operate on k-tuples of nodes need increased computational resources in order to process higher-order tensors. Instead of the WL approach, in this work, we follow the classical approach of Individualization and Refinement (IR), a technique followed by most practical isomorphism solvers. Individualization refers to artificially distinguishing a node in the graph and refinement is the propagation of this information to other nodes through message passing. We learn to adaptively select nodes to individualize and to aggregate the resulting graphs after refinement to help handle the complexity. Our technique lets us learn richer node embeddings while keeping the computational complexity manageable. Theoretically, we show that our procedure is more expressive than the 1-WL test. Experiments show that our method outperforms prominent 1-WL GNN models as well as competitive higher-order baselines on several benchmark synthetic and real datasets. Furthermore, our method opens new doors for exploring the paradigm of learning on graph structures with individualization and refinement.", "paper_url": "http://arxiv.org/abs/2203.09141v1", "pdf_url": "http://arxiv.org/pdf/2203.09141v1", "repo_url": null}, "2203.08852": {"publish_time": "2022-03-16", "title": "Learning the Dynamics of Physical Systems from Sparse Observations with Finite Element Networks", "author": "Marten Lienen et.al.", "abstract": "We propose a new method for spatio-temporal forecasting on arbitrarily distributed points. Assuming that the observed system follows an unknown partial differential equation, we derive a continuous-time model for the dynamics of the data via the finite element method. The resulting graph neural network estimates the instantaneous effects of the unknown dynamics on each cell in a meshing of the spatial domain. Our model can incorporate prior knowledge via assumptions on the form of the unknown PDE, which induce a structural bias towards learning specific processes. Through this mechanism, we derive a transport variant of our model from the convection equation and show that it improves the transfer performance to higher-resolution meshes on sea surface temperature and gas flow forecasting against baseline models representing a selection of spatio-temporal forecasting methods. A qualitative analysis shows that our model disentangles the data dynamics into their constituent parts, which makes it uniquely interpretable.", "paper_url": "http://arxiv.org/abs/2203.08852v1", "pdf_url": "http://arxiv.org/pdf/2203.08852v1", "repo_url": "https://github.com/martenlienen/finite-element-networks"}, "2203.09736": {"publish_time": "2022-03-18", "title": "Series Photo Selection via Multi-view Graph Learning", "author": "Jin Huang et.al.", "abstract": "Series photo selection (SPS) is an important branch of the image aesthetics quality assessment, which focuses on finding the best one from a series of nearly identical photos. While a great progress has been observed, most of the existing SPS approaches concentrate solely on extracting features from the original image, neglecting that multiple views, e.g, saturation level, color histogram and depth of field of the image, will be of benefit to successfully reflecting the subtle aesthetic changes. Taken multi-view into consideration, we leverage a graph neural network to construct the relationships between multi-view features. Besides, multiple views are aggregated with an adaptive-weight self-attention module to verify the significance of each view. Finally, a siamese network is proposed to select the best one from a series of nearly identical photos. Experimental results demonstrate that our model accomplish the highest success rates compared with competitive methods.", "paper_url": "http://arxiv.org/abs/2203.09736v1", "pdf_url": "http://arxiv.org/pdf/2203.09736v1", "repo_url": null}, "2203.09697": {"publish_time": "2022-03-18", "title": "Towards Training Billion Parameter Graph Neural Networks for Atomic Simulations", "author": "Anuroop Sriram et.al.", "abstract": "Recent progress in Graph Neural Networks (GNNs) for modeling atomic simulations has the potential to revolutionize catalyst discovery, which is a key step in making progress towards the energy breakthroughs needed to combat climate change. However, the GNNs that have proven most effective for this task are memory intensive as they model higher-order interactions in the graphs such as those between triplets or quadruplets of atoms, making it challenging to scale these models. In this paper, we introduce Graph Parallelism, a method to distribute input graphs across multiple GPUs, enabling us to train very large GNNs with hundreds of millions or billions of parameters. We empirically evaluate our method by scaling up the number of parameters of the recently proposed DimeNet++ and GemNet models by over an order of magnitude. On the large-scale Open Catalyst 2020 (OC20) dataset, these graph-parallelized models lead to relative improvements of 1) 15% on the force MAE metric for the S2EF task and 2) 21% on the AFbT metric for the IS2RS task, establishing new state-of-the-art results.", "paper_url": "http://arxiv.org/abs/2203.09697v1", "pdf_url": "http://arxiv.org/pdf/2203.09697v1", "repo_url": null}, "2203.10977": {"publish_time": "2022-03-21", "title": "Improving anatomical plausibility in medical image segmentation via hybrid graph neural networks: applications to chest x-ray analysis", "author": "Nicol\u00e1s Gaggion et.al.", "abstract": "Anatomical segmentation is a fundamental task in medical image computing, generally tackled with fully convolutional neural networks which produce dense segmentation masks. These models are often trained with loss functions such as cross-entropy or Dice, which assume pixels to be independent of each other, thus ignoring topological errors and anatomical inconsistencies. We address this limitation by moving from pixel-level to graph representations, which allow to naturally incorporate anatomical constraints by construction. To this end, we introduce HybridGNet, an encoder-decoder neural architecture that leverages standard convolutions for image feature encoding and graph convolutional neural networks (GCNNs) to decode plausible representations of anatomical structures. We also propose a novel image-to-graph skip connection layer which allows localized features to flow from standard convolutional blocks to GCNN blocks, and show that it improves segmentation accuracy. The proposed architecture is extensively evaluated in a variety of domain shift and image occlusion scenarios, and audited considering different types of demographic domain shift. Our comprehensive experimental setup compares HybridGNet with other landmark and pixel-based models for anatomical segmentation in chest x-ray images, and shows that it produces anatomically plausible results in challenging scenarios where other models tend to fail.", "paper_url": "http://arxiv.org/abs/2203.10977v1", "pdf_url": "http://arxiv.org/pdf/2203.10977v1", "repo_url": "https://github.com/ngaggion/HybridGNet"}, "2203.10926": {"publish_time": "2022-03-21", "title": "3D Multi-Object Tracking Using Graph Neural Networks with Cross-Edge Modality Attention", "author": "Martin Buchner et.al.", "abstract": "Online 3D multi-object tracking (MOT) has witnessed significant research interest in recent years, largely driven by demand from the autonomous systems community. However, 3D offline MOT is relatively less explored. Labeling 3D trajectory scene data at a large scale while not relying on high-cost human experts is still an open research question. In this work, we propose Batch3DMOT that follows the tracking-by-detection paradigm and represents real-world scenes as directed, acyclic, and category-disjoint tracking graphs that are attributed using various modalities such as camera, LiDAR, and radar. We present a multi-modal graph neural network that uses a cross-edge attention mechanism mitigating modality intermittence, which translates into sparsity in the graph domain. Additionally, we present attention-weighted convolutions over frame-wise k-NN neighborhoods as suitable means to allow information exchange across disconnected graph components. We evaluate our approach using various sensor modalities and model configurations on the challenging nuScenes and KITTI datasets. Extensive experiments demonstrate that our proposed approach yields an overall improvement of 2.8% in the AMOTA score on nuScenes thereby setting a new benchmark for 3D tracking methods and successfully enhances false positive filtering.", "paper_url": "http://arxiv.org/abs/2203.10926v1", "pdf_url": "http://arxiv.org/pdf/2203.10926v1", "repo_url": null}, "2203.10800": {"publish_time": "2022-03-21", "title": "Graph Neural Networks for Wireless Communications: From Theory to Practice", "author": "Yifei Shen et.al.", "abstract": "Deep learning-based approaches have been developed to solve challenging problems in wireless communications, leading to promising results. Early attempts adopted neural network architectures inherited from applications such as computer vision. They often require huge amounts of training samples (i.e., poor generalization), and yield poor performance in large-scale networks (i.e., poor scalability). To resolve these issues, graph neural networks (GNNs) have been recently adopted, as they can effectively exploit the domain knowledge, i.e., the graph topology in wireless communication problems. GNN-based methods can achieve near-optimal performance in large-scale networks and generalize well under different system settings, but the theoretical underpinnings and design guidelines remain elusive, which may hinder their practical implementations. This paper endeavors to fill both the theoretical and practical gaps. For theoretical guarantees, we prove that GNNs achieve near-optimal performance in wireless networks with much fewer training samples than traditional neural architectures. Specifically, to solve an optimization problem on an $n$-node graph (where the nodes may represent users, base stations, or antennas), GNNs' generalization error and required number of training samples are $\\mathcal{O}(n)$ and $\\mathcal{O}(n^2)$ times lower than the unstructured multi-layer perceptrons. For design guidelines, we propose a unified framework that is applicable to general design problems in wireless networks, which includes graph modeling, neural architecture design, and theory-guided performance enhancement. Extensive simulations, which cover a variety of important problems and network settings, verify our theory and effectiveness of the proposed design framework.", "paper_url": "http://arxiv.org/abs/2203.10800v1", "pdf_url": "http://arxiv.org/pdf/2203.10800v1", "repo_url": "https://github.com/yshenaw/gnn4com"}, "2203.10620": {"publish_time": "2022-03-20", "title": "Differentiable Reasoning over Long Stories -- Assessing Systematic Generalisation in Neural Models", "author": "Wanshui Li et.al.", "abstract": "Contemporary neural networks have achieved a series of developments and successes in many aspects; however, when exposed to data outside the training distribution, they may fail to predict correct answers. In this work, we were concerned about this generalisation issue and thus analysed a broad set of models systematically and robustly over long stories. Related experiments were conducted based on the CLUTRR, which is a diagnostic benchmark suite that can analyse generalisation of natural language understanding (NLU) systems by training over small story graphs and testing on larger ones. In order to handle the multi-relational story graph, we consider two classes of neural models: \"E-GNN\", the graph-based models that can process graph-structured data and consider the edge attributes simultaneously; and \"L-Graph\", the sequence-based models which can process linearized version of the graphs. We performed an extensive empirical evaluation, and we found that the modified recurrent neural network yield surprisingly accurate results across every systematic generalisation tasks which outperform the modified graph neural network, while the latter produced more robust models.", "paper_url": "http://arxiv.org/abs/2203.10620v1", "pdf_url": "http://arxiv.org/pdf/2203.10620v1", "repo_url": null}, "2203.10565": {"publish_time": "2022-03-20", "title": "LEReg: Empower Graph Neural Networks with Local Energy Regularization", "author": "Xiaojun Ma et.al.", "abstract": "Researches on analyzing graphs with Graph Neural Networks (GNNs) have been receiving more and more attention because of the great expressive power of graphs. GNNs map the adjacency matrix and node features to node representations by message passing through edges on each convolution layer. However, the message passed through GNNs is not always beneficial for all parts in a graph. Specifically, as the data distribution is different over the graph, the receptive field (the farthest nodes that a node can obtain information from) needed to gather information is also different. Existing GNNs treat all parts of the graph uniformly, which makes it difficult to adaptively pass the most informative message for each unique part. To solve this problem, we propose two regularization terms that consider message passing locally: (1) Intra-Energy Reg and (2) Inter-Energy Reg. Through experiments and theoretical discussion, we first show that the speed of smoothing of different parts varies enormously and the topology of each part affects the way of smoothing. With Intra-Energy Reg, we strengthen the message passing within each part, which is beneficial for getting more useful information. With Inter-Energy Reg, we improve the ability of GNNs to distinguish different nodes. With the proposed two regularization terms, GNNs are able to filter the most useful information adaptively, learn more robustly and gain higher expressiveness. Moreover, the proposed LEReg can be easily applied to other GNN models with plug-and-play characteristics. Extensive experiments on several benchmarks verify that GNNs with LEReg outperform or match the state-of-the-art methods. The effectiveness and efficiency are also empirically visualized with elaborate experiments.", "paper_url": "http://arxiv.org/abs/2203.10565v1", "pdf_url": "http://arxiv.org/pdf/2203.10565v1", "repo_url": null}, "2203.11492": {"publish_time": "2022-03-22", "title": "Exploring High-Order Structure for Robust Graph Structure Learning", "author": "Guangqian Yang et.al.", "abstract": "Recent studies show that Graph Neural Networks (GNNs) are vulnerable to adversarial attack, i.e., an imperceptible structure perturbation can fool GNNs to make wrong predictions. Some researches explore specific properties of clean graphs such as the feature smoothness to defense the attack, but the analysis of it has not been well-studied. In this paper, we analyze the adversarial attack on graphs from the perspective of feature smoothness which further contributes to an efficient new adversarial defensive algorithm for GNNs. We discover that the effect of the high-order graph structure is a smoother filter for processing graph structures. Intuitively, the high-order graph structure denotes the path number between nodes, where larger number indicates closer connection, so it naturally contributes to defense the adversarial perturbation. Further, we propose a novel algorithm that incorporates the high-order structural information into the graph structure learning. We perform experiments on three popular benchmark datasets, Cora, Citeseer and Polblogs. Extensive experiments demonstrate the effectiveness of our method for defending against graph adversarial attacks.", "paper_url": "http://arxiv.org/abs/2203.11492v1", "pdf_url": "http://arxiv.org/pdf/2203.11492v1", "repo_url": null}, "2203.12522": {"publish_time": "2022-03-23", "title": "Semi-Supervised Graph Learning Meets Dimensionality Reduction", "author": "Alex Morehead et.al.", "abstract": "Semi-supervised learning (SSL) has recently received increased attention from machine learning researchers. By enabling effective propagation of known labels in graph-based deep learning (GDL) algorithms, SSL is poised to become an increasingly used technique in GDL in the coming years. However, there are currently few explorations in the graph-based SSL literature on exploiting classical dimensionality reduction techniques for improved label propagation. In this work, we investigate the use of dimensionality reduction techniques such as PCA, t-SNE, and UMAP to see their effect on the performance of graph neural networks (GNNs) designed for semi-supervised propagation of node labels. Our study makes use of benchmark semi-supervised GDL datasets such as the Cora and Citeseer datasets to allow meaningful comparisons of the representations learned by each algorithm when paired with a dimensionality reduction technique. Our comprehensive benchmarks and clustering visualizations quantitatively and qualitatively demonstrate that, under certain conditions, employing a priori and a posteriori dimensionality reduction to GNN inputs and outputs, respectively, can simultaneously improve the effectiveness of semi-supervised node label propagation and node clustering. Our source code is freely available on GitHub.", "paper_url": "http://arxiv.org/abs/2203.12522v1", "pdf_url": "http://arxiv.org/pdf/2203.12522v1", "repo_url": "https://github.com/amorehead/SSL-With-DR-And-GNNs"}, "2203.12363": {"publish_time": "2022-03-23", "title": "Ethereum Fraud Detection with Heterogeneous Graph Neural Networks", "author": "Hiroki Kanezashi et.al.", "abstract": "While transactions with cryptocurrencies such as Ethereum are becoming more prevalent, fraud and other criminal transactions are not uncommon. Graph analysis algorithms and machine learning techniques detect suspicious transactions that lead to phishing in large transaction networks. Many graph neural network (GNN) models have been proposed to apply deep learning techniques to graph structures. Although there is research on phishing detection using GNN models in the Ethereum transaction network, models that address the scale of the number of vertices and edges and the imbalance of labels have not yet been studied. In this paper, we compared the model performance of GNN models on the actual Ethereum transaction network dataset and phishing reported label data to exhaustively compare and verify which GNN models and hyperparameters produce the best accuracy. Specifically, we evaluated the model performance of representative homogeneous GNN models which consider single-type nodes and edges and heterogeneous GNN models which support different types of nodes and edges. We showed that heterogeneous models had better model performance than homogeneous models. In particular, the RGCN model achieved the best performance in the overall metrics.", "paper_url": "http://arxiv.org/abs/2203.12363v1", "pdf_url": "http://arxiv.org/pdf/2203.12363v1", "repo_url": null}, "2203.12848": {"publish_time": "2022-03-24", "title": "Keypoints Tracking via Transformer Networks", "author": "Oleksii Nasypanyi et.al.", "abstract": "In this thesis, we propose a pioneering work on sparse keypoints tracking across images using transformer networks. While deep learning-based keypoints matching have been widely investigated using graph neural networks - and more recently transformer networks, they remain relatively too slow to operate in real-time and are particularly sensitive to the poor repeatability of the keypoints detectors. In order to address these shortcomings, we propose to study the particular case of real-time and robust keypoints tracking. Specifically, we propose a novel architecture which ensures a fast and robust estimation of the keypoints tracking between successive images of a video sequence. Our method takes advantage of a recent breakthrough in computer vision, namely, visual transformer networks. Our method consists of two successive stages, a coarse matching followed by a fine localization of the keypoints' correspondences prediction. Through various experiments, we demonstrate that our approach achieves competitive results and demonstrates high robustness against adverse conditions, such as illumination change, occlusion and viewpoint differences.", "paper_url": "http://arxiv.org/abs/2203.12848v1", "pdf_url": "http://arxiv.org/pdf/2203.12848v1", "repo_url": "https://github.com/lexanagibator228/keypoints-tracking-via-transformer-networks"}, "2203.12831": {"publish_time": "2022-03-24", "title": "LHNN: Lattice Hypergraph Neural Network for VLSI Congestion Prediction", "author": "Bowen Wang et.al.", "abstract": "Precise congestion prediction from a placement solution plays a crucial role in circuit placement. This work proposes the lattice hypergraph (LH-graph), a novel graph formulation for circuits, which preserves netlist data during the whole learning process, and enables the congestion information propagated geometrically and topologically. Based on the formulation, we further developed a heterogeneous graph neural network architecture LHNN, jointing the routing demand regression to support the congestion spot classification. LHNN constantly achieves more than 35% improvements compared with U-nets and Pix2Pix on the F1 score. We expect our work shall highlight essential procedures using machine learning for congestion prediction.", "paper_url": "http://arxiv.org/abs/2203.12831v1", "pdf_url": "http://arxiv.org/pdf/2203.12831v1", "repo_url": null}, "2203.12852": {"publish_time": "2022-03-23", "title": "Graph Neural Networks in Particle Physics: Implementations, Innovations, and Challenges", "author": "Savannah Thais et.al.", "abstract": "Many physical systems can be best understood as sets of discrete data with associated relationships. Where previously these sets of data have been formulated as series or image data to match the available machine learning architectures, with the advent of graph neural networks (GNNs), these systems can be learned natively as graphs. This allows a wide variety of high- and low-level physical features to be attached to measurements and, by the same token, a wide variety of HEP tasks to be accomplished by the same GNN architectures. GNNs have found powerful use-cases in reconstruction, tagging, generation and end-to-end analysis. With the wide-spread adoption of GNNs in industry, the HEP community is well-placed to benefit from rapid improvements in GNN latency and memory usage. However, industry use-cases are not perfectly aligned with HEP and much work needs to be done to best match unique GNN capabilities to unique HEP obstacles. We present here a range of these capabilities, predictions of which are currently being well-adopted in HEP communities, and which are still immature. We hope to capture the landscape of graph techniques in machine learning as well as point out the most significant gaps that are inhibiting potentially large leaps in research.", "paper_url": "http://arxiv.org/abs/2203.12852v1", "pdf_url": "http://arxiv.org/pdf/2203.12852v1", "repo_url": null}, "2203.13783": {"publish_time": "2022-03-25", "title": "Ensemble Spectral Prediction (ESP) Model for Metabolite Annotation", "author": "Xinmeng Li et.al.", "abstract": "A key challenge in metabolomics is annotating measured spectra from a biological sample with chemical identities. Currently, only a small fraction of measurements can be assigned identities. Two complementary computational approaches have emerged to address the annotation problem: mapping candidate molecules to spectra, and mapping query spectra to molecular candidates. In essence, the candidate molecule with the spectrum that best explains the query spectrum is recommended as the target molecule. Despite candidate ranking being fundamental in both approaches, no prior works utilized rank learning tasks in determining the target molecule. We propose a novel machine learning model, Ensemble Spectral Prediction (ESP), for metabolite annotation. ESP takes advantage of prior neural network-based annotation models that utilize multilayer perceptron (MLP) networks and Graph Neural Networks (GNNs). Based on the ranking results of the MLP and GNN-based models, ESP learns a weighting for the outputs of MLP and GNN spectral predictors to generate a spectral prediction for a query molecule. Importantly, training data is stratified by molecular formula to provide candidate sets during model training. Further, baseline MLP and GNN models are enhanced by considering peak dependencies through multi-head attention mechanism and multi-tasking on spectral topic distributions. ESP improves average rank by 41% and 30% over the MLP and GNN baselines, respectively, demonstrating remarkable performance gain over state-of-the-art neural network approaches. We show that annotation performance, for ESP and other models, is a strong function of the number of molecules in the candidate set and their similarity to the target molecule.", "paper_url": "http://arxiv.org/abs/2203.13783v1", "pdf_url": "http://arxiv.org/pdf/2203.13783v1", "repo_url": null}, "2203.13424": {"publish_time": "2022-03-25", "title": "Dealing with Sparse Rewards Using Graph Neural Networks", "author": "Matvey Gerasyov et.al.", "abstract": "Deep reinforcement learning in partially observable environments is a difficult task in itself, and can be further complicated by a sparse reward signal. Most tasks involving navigation in three-dimensional environments provide the agent with extremely limited information. Typically, the agent receives a visual observation input from the environment and is rewarded once at the end of the episode. A good reward function could substantially improve the convergence of reinforcement learning algorithms for such tasks. The classic approach to increase the density of the reward signal is to augment it with supplementary rewards. This technique is called the reward shaping. In this study, we propose two modifications of one of the recent reward shaping methods based on graph convolutional networks: the first involving advanced aggregation functions, and the second utilizing the attention mechanism. We empirically validate the effectiveness of our solutions for the task of navigation in a 3D environment with sparse rewards. For the solution featuring attention mechanism, we are also able to show that the learned attention is concentrated on edges corresponding to important transitions in 3D environment.", "paper_url": "http://arxiv.org/abs/2203.13424v1", "pdf_url": "http://arxiv.org/pdf/2203.13424v1", "repo_url": null}, "2203.14921": {"publish_time": "2022-03-28", "title": "Learning What You Need from What You Did: Product Taxonomy Expansion with User Behaviors Supervision", "author": "Sijie Cheng et.al.", "abstract": "Taxonomies have been widely used in various domains to underpin numerous applications. Specially, product taxonomies serve an essential role in the e-commerce domain for the recommendation, browsing, and query understanding. However, taxonomies need to constantly capture the newly emerged terms or concepts in e-commerce platforms to keep up-to-date, which is expensive and labor-intensive if it relies on manual maintenance and updates. Therefore, we target the taxonomy expansion task to attach new concepts to existing taxonomies automatically. In this paper, we present a self-supervised and user behavior-oriented product taxonomy expansion framework to append new concepts into existing taxonomies. Our framework extracts hyponymy relations that conform to users' intentions and cognition. Specifically, i) to fully exploit user behavioral information, we extract candidate hyponymy relations that match user interests from query-click concepts; ii) to enhance the semantic information of new concepts and better detect hyponymy relations, we model concepts and relations through both user-generated content and structural information in existing taxonomies and user click logs, by leveraging Pre-trained Language Models and Graph Neural Network combined with Contrastive Learning; iii) to reduce the cost of dataset construction and overcome data skews, we construct a high-quality and balanced training dataset from existing taxonomy with no supervision. Extensive experiments on real-world product taxonomies in Meituan Platform, a leading Chinese vertical e-commerce platform to order take-out with more than 70 million daily active users, demonstrate the superiority of our proposed framework over state-of-the-art methods. Notably, our method enlarges the size of real-world product taxonomies from 39,263 to 94,698 relations with 88% precision.", "paper_url": "http://arxiv.org/abs/2203.14921v1", "pdf_url": "http://arxiv.org/pdf/2203.14921v1", "repo_url": "https://github.com/adacheng/product_taxonomy_expansion"}, "2203.14883": {"publish_time": "2022-03-28", "title": "TGL: A General Framework for Temporal GNN Training on Billion-Scale Graphs", "author": "Hongkuan Zhou et.al.", "abstract": "Many real world graphs contain time domain information. Temporal Graph Neural Networks capture temporal information as well as structural and contextual information in the generated dynamic node embeddings. Researchers have shown that these embeddings achieve state-of-the-art performance in many different tasks. In this work, we propose TGL, a unified framework for large-scale offline Temporal Graph Neural Network training where users can compose various Temporal Graph Neural Networks with simple configuration files. TGL comprises five main components, a temporal sampler, a mailbox, a node memory module, a memory updater, and a message passing engine. We design a Temporal-CSR data structure and a parallel sampler to efficiently sample temporal neighbors to formtraining mini-batches. We propose a novel random chunk scheduling technique that mitigates the problem of obsolete node memory when training with a large batch size. To address the limitations of current TGNNs only being evaluated on small-scale datasets, we introduce two large-scale real-world datasets with 0.2 and 1.3 billion temporal edges. We evaluate the performance of TGL on four small-scale datasets with a single GPU and the two large datasets with multiple GPUs for both link prediction and node classification tasks. We compare TGL with the open-sourced code of five methods and show that TGL achieves similar or better accuracy with an average of 13x speedup. Our temporal parallel sampler achieves an average of 173x speedup on a multi-core CPU compared with the baselines. On a 4-GPU machine, TGL can train one epoch of more than one billion temporal edges within 1-10 hours. To the best of our knowledge, this is the first work that proposes a general framework for large-scale Temporal Graph Neural Networks training on multiple GPUs.", "paper_url": "http://arxiv.org/abs/2203.14883v1", "pdf_url": "http://arxiv.org/pdf/2203.14883v1", "repo_url": null}, "2203.14846": {"publish_time": "2022-03-28", "title": "Discovering dynamic laws from observations with Graph Neural Networks: the case of self-propelled, interacting colloids", "author": "Miguel Ruiz-Garcia et.al.", "abstract": "Active matter spans a wide range of time and length scales, from groups of cells and synthetic self-propelled particles to schools of fish or even human crowds. The theoretical framework describing these systems has shown tremendous success at finding universal phenomenology. However, further progress is often burdened by the difficulty of determining the forces that control the dynamics of the individual elements within each system. Accessing this local information is key to understanding the physics dominating the system and to create the models that can explain the observed collective phenomena. In this work, we present a machine learning model, a graph neural network, that uses the collective movement of the system to learn the active and two-body forces controlling the individual dynamics of the particles. We verify our approach using numerical simulations of active brownian particles, considering different interaction potentials and levels of activity. Finally, we apply our model to experiments of electrophoretic Janus particles, extracting the active and two-body forces that control the dynamics of the colloids. Due to this, we can uncover the physics dominating the behavior of the system. We extract an active force that depends on the electric field and also area fraction. We also discover a dependence of the two-body interaction with the electric field that leads us to propose that the dominant force between these colloids is a screened electrostatic interaction with a constant length scale. We expect that this methodology can open a new avenue for the study and modeling of experimental systems of active particles.", "paper_url": "http://arxiv.org/abs/2203.14846v1", "pdf_url": "http://arxiv.org/pdf/2203.14846v1", "repo_url": null}, "2203.14487": {"publish_time": "2022-03-28", "title": "Enhancing Neural Mathematical Reasoning by Abductive Combination with Symbolic Library", "author": "Yangyang Hu et.al.", "abstract": "Mathematical reasoning recently has been shown as a hard challenge for neural systems. Abilities including expression translation, logical reasoning, and mathematics knowledge acquiring appear to be essential to overcome the challenge. This paper demonstrates that some abilities can be achieved through abductive combination with discrete systems that have been programmed with human knowledge. On a mathematical reasoning dataset, we adopt the recently proposed abductive learning framework, and propose the ABL-Sym algorithm that combines the Transformer neural models with a symbolic mathematics library. ABL-Sym shows 9.73% accuracy improvement on the interpolation tasks and 47.22% accuracy improvement on the extrapolation tasks, over the state-of-the-art approaches. Online demonstration: http://math.polixir.ai", "paper_url": "http://arxiv.org/abs/2203.14487v1", "pdf_url": "http://arxiv.org/pdf/2203.14487v1", "repo_url": null}, "2203.14486": {"publish_time": "2022-03-28", "title": "Equivariant Point Cloud Analysis via Learning Orientations for Message Passing", "author": "Shitong Luo et.al.", "abstract": "Equivariance has been a long-standing concern in various fields ranging from computer vision to physical modeling. Most previous methods struggle with generality, simplicity, and expressiveness -- some are designed ad hoc for specific data types, some are too complex to be accessible, and some sacrifice flexible transformations. In this work, we propose a novel and simple framework to achieve equivariance for point cloud analysis based on the message passing (graph neural network) scheme. We find the equivariant property could be obtained by introducing an orientation for each point to decouple the relative position for each point from the global pose of the entire point cloud. Therefore, we extend current message passing networks with a module that learns orientations for each point. Before aggregating information from the neighbors of a point, the networks transforms the neighbors' coordinates based on the point's learned orientations. We provide formal proofs to show the equivariance of the proposed framework. Empirically, we demonstrate that our proposed method is competitive on both point cloud analysis and physical modeling tasks. Code is available at https://github.com/luost26/Equivariant-OrientedMP .", "paper_url": "http://arxiv.org/abs/2203.14486v1", "pdf_url": "http://arxiv.org/pdf/2203.14486v1", "repo_url": "https://github.com/luost26/equivariant-orientedmp"}, "2203.15789": {"publish_time": "2022-03-29", "title": "Revisiting Neighborhood-based Link Prediction for Collaborative Filtering", "author": "Hao-Ming Fu et.al.", "abstract": "Collaborative filtering (CF) is one of the most successful and fundamental techniques in recommendation systems. In recent years, Graph Neural Network (GNN)-based CF models, such as NGCF [31], LightGCN [10] and GTN [9] have achieved tremendous success and significantly advanced the state-of-the-art. While there is a rich literature of such works using advanced models for learning user and item representations separately, item recommendation is essentially a link prediction problem between users and items. Furthermore, while there have been early works employing link prediction for collaborative filtering [5, 6], this trend has largely given way to works focused on aggregating information from user and item nodes, rather than modeling links directly. In this paper, we propose a new linkage (connectivity) score for bipartite graphs, generalizing multiple standard link prediction methods. We combine this new score with an iterative degree update process in the user-item interaction bipartite graph to exploit local graph structures without any node modeling. The result is a simple, non-deep learning model with only six learnable parameters. Despite its simplicity, we demonstrate our approach significantly outperforms existing state-of-the-art GNN-based CF approaches on four widely used benchmarks. In particular, on Amazon-Book, we demonstrate an over 60% improvement for both Recall and NDCG. We hope our work would invite the community to revisit the link prediction aspect of collaborative filtering, where significant performance gains could be achieved through aligning link prediction with item recommendations.", "paper_url": "http://arxiv.org/abs/2203.15789v1", "pdf_url": "http://arxiv.org/pdf/2203.15789v1", "repo_url": null}, "2203.15544": {"publish_time": "2022-03-29", "title": "Graph Neural Networks are Dynamic Programmers", "author": "Andrew Dudzik et.al.", "abstract": "Recent advances in neural algorithmic reasoning with graph neural networks (GNNs) are propped up by the notion of algorithmic alignment. Broadly, a neural network will be better at learning to execute a reasoning task (in terms of sample complexity) if its individual components align well with the target algorithm. Specifically, GNNs are claimed to align with dynamic programming (DP), a general problem-solving strategy which expresses many polynomial-time algorithms. However, has this alignment truly been demonstrated and theoretically quantified? Here we show, using methods from category theory and abstract algebra, that there exists an intricate connection between GNNs and DP, going well beyond the initial observations over individual algorithms such as Bellman-Ford. Exposing this connection, we easily verify several prior findings in the literature, and hope it will serve as a foundation for building stronger algorithmically aligned GNNs.", "paper_url": "http://arxiv.org/abs/2203.15544v1", "pdf_url": "http://arxiv.org/pdf/2203.15544v1", "repo_url": null}, "2203.15470": {"publish_time": "2022-03-29", "title": "Graph similarity learning for change-point detection in dynamic networks", "author": "Deborah Sulem et.al.", "abstract": "Dynamic networks are ubiquitous for modelling sequential graph-structured data, e.g., brain connectome, population flows and messages exchanges. In this work, we consider dynamic networks that are temporal sequences of graph snapshots, and aim at detecting abrupt changes in their structure. This task is often termed network change-point detection and has numerous applications, such as fraud detection or physical motion monitoring. Leveraging a graph neural network model, we design a method to perform online network change-point detection that can adapt to the specific network domain and localise changes with no delay. The main novelty of our method is to use a siamese graph neural network architecture for learning a data-driven graph similarity function, which allows to effectively compare the current graph and its recent history. Importantly, our method does not require prior knowledge on the network generative distribution and is agnostic to the type of change-points; moreover, it can be applied to a large variety of networks, that include for instance edge weights and node attributes. We show on synthetic and real data that our method enjoys a number of benefits: it is able to learn an adequate graph similarity function for performing online network change-point detection in diverse types of change-point settings, and requires a shorter data history to detect changes than most existing state-of-the-art baselines.", "paper_url": "http://arxiv.org/abs/2203.15470v1", "pdf_url": "http://arxiv.org/pdf/2203.15470v1", "repo_url": null}, "2203.15209": {"publish_time": "2022-03-29", "title": "OrphicX: A Causality-Inspired Latent Variable Model for Interpreting Graph Neural Networks", "author": "Wanyu Lin et.al.", "abstract": "This paper proposes a new eXplanation framework, called OrphicX, for generating causal explanations for any graph neural networks (GNNs) based on learned latent causal factors. Specifically, we construct a distinct generative model and design an objective function that encourages the generative model to produce causal, compact, and faithful explanations. This is achieved by isolating the causal factors in the latent space of graphs by maximizing the information flow measurements. We theoretically analyze the cause-effect relationships in the proposed causal graph, identify node attributes as confounders between graphs and GNN predictions, and circumvent such confounder effect by leveraging the backdoor adjustment formula. Our framework is compatible with any GNNs, and it does not require access to the process by which the target GNN produces its predictions. In addition, it does not rely on the linear-independence assumption of the explained features, nor require prior knowledge on the graph learning tasks. We show a proof-of-concept of OrphicX on canonical classification problems on graph data. In particular, we analyze the explanatory subgraphs obtained from explanations for molecular graphs (i.e., Mutag) and quantitatively evaluate the explanation performance with frequently occurring subgraph patterns. Empirically, we show that OrphicX can effectively identify the causal semantics for generating causal explanations, significantly outperforming its alternatives.", "paper_url": "http://arxiv.org/abs/2203.15209v1", "pdf_url": "http://arxiv.org/pdf/2203.15209v1", "repo_url": "https://github.com/wanyugroup/cvpr2022-orphicx"}, "2203.15182": {"publish_time": "2022-03-29", "title": "Long-term Visual Map Sparsification with Heterogeneous GNN", "author": "Ming-Fang Chang et.al.", "abstract": "We address the problem of map sparsification for long-term visual localization. For map sparsification, a commonly employed assumption is that the pre-build map and the later captured localization query are consistent. However, this assumption can be easily violated in the dynamic world. Additionally, the map size grows as new data accumulate through time, causing large data overhead in the long term. In this paper, we aim to overcome the environmental changes and reduce the map size at the same time by selecting points that are valuable to future localization. Inspired by the recent progress in Graph Neural Network(GNN), we propose the first work that models SfM maps as heterogeneous graphs and predicts 3D point importance scores with a GNN, which enables us to directly exploit the rich information in the SfM map graph. Two novel supervisions are proposed: 1) a data-fitting term for selecting valuable points to future localization based on training queries; 2) a K-Cover term for selecting sparse points with full map coverage. The experiments show that our method selected map points on stable and widely visible structures and outperformed baselines in localization performance.", "paper_url": "http://arxiv.org/abs/2203.15182v1", "pdf_url": "http://arxiv.org/pdf/2203.15182v1", "repo_url": null}, "2203.16319": {"publish_time": "2022-03-30", "title": "Multi-Robot Active Mapping via Neural Bipartite Graph Matching", "author": "Kai Ye et.al.", "abstract": "We study the problem of multi-robot active mapping, which aims for complete scene map construction in minimum time steps. The key to this problem lies in the goal position estimation to enable more efficient robot movements. Previous approaches either choose the frontier as the goal position via a myopic solution that hinders the time efficiency, or maximize the long-term value via reinforcement learning to directly regress the goal position, but does not guarantee the complete map construction. In this paper, we propose a novel algorithm, namely NeuralCoMapping, which takes advantage of both approaches. We reduce the problem to bipartite graph matching, which establishes the node correspondences between two graphs, denoting robots and frontiers. We introduce a multiplex graph neural network (mGNN) that learns the neural distance to fill the affinity matrix for more effective graph matching. We optimize the mGNN with a differentiable linear assignment layer by maximizing the long-term values that favor time efficiency and map completeness via reinforcement learning. We compare our algorithm with several state-of-the-art multi-robot active mapping approaches and adapted reinforcement-learning baselines. Experimental results demonstrate the superior performance and exceptional generalization ability of our algorithm on various indoor scenes and unseen number of robots, when only trained with 9 indoor scenes.", "paper_url": "http://arxiv.org/abs/2203.16319v1", "pdf_url": "http://arxiv.org/pdf/2203.16319v1", "repo_url": null}, "2203.16280": {"publish_time": "2022-03-30", "title": "CMMD: Cross-Metric Multi-Dimensional Root Cause Analysis", "author": "Shifu Yan et.al.", "abstract": "In large-scale online services, crucial metrics, a.k.a., key performance indicators (KPIs), are monitored periodically to check their running statuses. Generally, KPIs are aggregated along multiple dimensions and derived by complex calculations among fundamental metrics from the raw data. Once abnormal KPI values are observed, root cause analysis (RCA) can be applied to identify the reasons for anomalies, so that we can troubleshoot quickly. Recently, several automatic RCA techniques were proposed to localize the related dimensions (or a combination of dimensions) to explain the anomalies. However, their analyses are limited to the data on the abnormal metric and ignore the data of other metrics which may be also related to the anomalies, leading to imprecise or even incorrect root causes. To this end, we propose a cross-metric multi-dimensional root cause analysis method, named CMMD, which consists of two key components: 1) relationship modeling, which utilizes graph neural network (GNN) to model the unknown complex calculation among metrics and aggregation function among dimensions from historical data; 2) root cause localization, which adopts the genetic algorithm to efficiently and effectively dive into the raw data and localize the abnormal dimension(s) once the KPI anomalies are detected. Experiments on synthetic datasets, public datasets and online production environment demonstrate the superiority of our proposed CMMD method compared with baselines. Currently, CMMD is running as an online service in Microsoft Azure.", "paper_url": "http://arxiv.org/abs/2203.16280v1", "pdf_url": "http://arxiv.org/pdf/2203.16280v1", "repo_url": null}, "2203.16162": {"publish_time": "2022-03-30", "title": "AdaGrid: Adaptive Grid Search for Link Prediction Training Objective", "author": "Tim Po\u0161tuvan et.al.", "abstract": "One of the most important factors that contribute to the success of a machine learning model is a good training objective. Training objective crucially influences the model's performance and generalization capabilities. This paper specifically focuses on graph neural network training objective for link prediction, which has not been explored in the existing literature. Here, the training objective includes, among others, a negative sampling strategy, and various hyperparameters, such as edge message ratio which controls how training edges are used. Commonly, these hyperparameters are fine-tuned by complete grid search, which is very time-consuming and model-dependent. To mitigate these limitations, we propose Adaptive Grid Search (AdaGrid), which dynamically adjusts the edge message ratio during training. It is model agnostic and highly scalable with a fully customizable computational budget. Through extensive experiments, we show that AdaGrid can boost the performance of the models up to $1.9\\%$ while being nine times more time-efficient than a complete search. Overall, AdaGrid represents an effective automated algorithm for designing machine learning training objectives.", "paper_url": "http://arxiv.org/abs/2203.16162v1", "pdf_url": "http://arxiv.org/pdf/2203.16162v1", "repo_url": null}, "2203.15936": {"publish_time": "2022-03-29", "title": "A Simple Yet Effective Pretraining Strategy for Graph Few-shot Learning", "author": "Zhen Tan et.al.", "abstract": "Recently, increasing attention has been devoted to the graph few-shot learning problem, where the target novel classes only contain a few labeled nodes. Among many existing endeavors, episodic meta-learning has become the most prevailing paradigm, and its episodic emulation of the test environment is believed to equip the graph neural network models with adaptability to novel node classes. However, in the image domain, recent results have shown that feature reuse is more likely to be the key of meta-learning to few-shot extrapolation. Based on such observation, in this work, we propose a simple transductive fine-tuning based framework as a new paradigm for graph few-shot learning. In the proposed paradigm, a graph encoder backbone is pretrained with base classes, and a simple linear classifier is fine-tuned by the few labeled samples and is tasked to classify the unlabeled ones. For pretraining, we propose a supervised contrastive learning framework with data augmentation strategies specific for few-shot node classification to improve the extrapolation of a GNN encoder. Finally, extensive experiments conducted on three benchmark datasets demonstrate the superior advantage of our framework over the state-of-the-art methods.", "paper_url": "http://arxiv.org/abs/2203.15936v1", "pdf_url": "http://arxiv.org/pdf/2203.15936v1", "repo_url": null}, "2203.15935": {"publish_time": "2022-03-29", "title": "Graph Neural Networks in IoT: A Survey", "author": "Guimin Dong et.al.", "abstract": "The Internet of Things (IoT) boom has revolutionized almost every corner of people's daily lives: healthcare, home, transportation, manufacturing, supply chain, and so on. With the recent development of sensor and communication technologies, IoT devices including smart wearables, cameras, smartwatches, and autonomous vehicles can accurately measure and perceive their surrounding environment. Continuous sensing generates massive amounts of data and presents challenges for machine learning. Deep learning models (e.g., convolution neural networks and recurrent neural networks) have been extensively employed in solving IoT tasks by learning patterns from multi-modal sensory data. Graph Neural Networks (GNNs), an emerging and fast-growing family of neural network models, can capture complex interactions within sensor topology and have been demonstrated to achieve state-of-the-art results in numerous IoT learning tasks. In this survey, we present a comprehensive review of recent advances in the application of GNNs to the IoT field, including a deep dive analysis of GNN design in various IoT sensing environments, an overarching list of public data and source code from the collected publications, and future research directions. To keep track of newly published works, we collect representative papers and their open-source implementations and create a Github repository at https://github.com/GuiminDong/GNN4IoT.", "paper_url": "http://arxiv.org/abs/2203.15935v1", "pdf_url": "http://arxiv.org/pdf/2203.15935v1", "repo_url": "https://github.com/guimindong/gnn4iot"}, "2203.17149": {"publish_time": "2022-03-31", "title": "AEGNN: Asynchronous Event-based Graph Neural Networks", "author": "Simon Schaefer et.al.", "abstract": "The best performing learning algorithms devised for event cameras work by first converting events into dense representations that are then processed using standard CNNs. However, these steps discard both the sparsity and high temporal resolution of events, leading to high computational burden and latency. For this reason, recent works have adopted Graph Neural Networks (GNNs), which process events as \"static\" spatio-temporal graphs, which are inherently \"sparse\". We take this trend one step further by introducing Asynchronous, Event-based Graph Neural Networks (AEGNNs), a novel event-processing paradigm that generalizes standard GNNs to process events as \"evolving\" spatio-temporal graphs. AEGNNs follow efficient update rules that restrict recomputation of network activations only to the nodes affected by each new event, thereby significantly reducing both computation and latency for event-by-event processing. AEGNNs are easily trained on synchronous inputs and can be converted to efficient, \"asynchronous\" networks at test time. We thoroughly validate our method on object classification and detection tasks, where we show an up to a 200-fold reduction in computational complexity (FLOPs), with similar or even better performance than state-of-the-art asynchronous methods. This reduction in computation directly translates to an 8-fold reduction in computational latency when compared to standard GNNs, which opens the door to low-latency event-based processing.", "paper_url": "http://arxiv.org/abs/2203.17149v1", "pdf_url": "http://arxiv.org/pdf/2203.17149v1", "repo_url": null}, "2203.16995": {"publish_time": "2022-03-31", "title": "Message Passing Neural Networks for Hypergraphs", "author": "Sajjad Heydari et.al.", "abstract": "Hypergraph representations are both more efficient and better suited to describe data characterized by relations between two or more objects. In this work, we present the first graph neural network based on message passing capable of processing hypergraph-structured data. We show that the proposed model defines a design space for neural network models for hypergraphs, thus generalizing existing models for hypergraphs. We report experiments on a benchmark dataset for node classification, highlighting the effectiveness of the proposed model with respect to other state-of-the-art methods for graphs and hypergraphs. We also discuss the benefits of using hypergraph representations and, at the same time, highlight the limitation of using equivalent graph representations when the underlying problem has relations among more than two objects.", "paper_url": "http://arxiv.org/abs/2203.16995v1", "pdf_url": "http://arxiv.org/pdf/2203.16995v1", "repo_url": null}, "2203.16628": {"publish_time": "2022-03-30", "title": "Physics-constrained Unsupervised Learning of Partial Differential Equations using Meshes", "author": "Mike Y. Michelis et.al.", "abstract": "Enhancing neural networks with knowledge of physical equations has become an efficient way of solving various physics problems, from fluid flow to electromagnetism. Graph neural networks show promise in accurately representing irregularly meshed objects and learning their dynamics, but have so far required supervision through large datasets. In this work, we represent meshes naturally as graphs, process these using Graph Networks, and formulate our physics-based loss to provide an unsupervised learning framework for partial differential equations (PDE). We quantitatively compare our results to a classical numerical PDE solver, and show that our computationally efficient approach can be used as an interactive PDE solver that is adjusting boundary conditions in real-time and remains sufficiently close to the baseline solution. Our inherently differentiable framework will enable the application of PDE solvers in interactive settings, such as model-based control of soft-body deformations, or in gradient-based optimization methods that require a fully differentiable pipeline.", "paper_url": "http://arxiv.org/abs/2203.16628v1", "pdf_url": "http://arxiv.org/pdf/2203.16628v1", "repo_url": null}, "2204.00255": {"publish_time": "2022-04-01", "title": "NC-DRE: Leveraging Non-entity Clue Information for Document-level Relation Extraction", "author": "Liang Zhang et.al.", "abstract": "Document-level relation extraction (RE), which requires reasoning on multiple entities in different sentences to identify complex inter-sentence relations, is more challenging than sentence-level RE. To extract the complex inter-sentence relations, previous studies usually employ graph neural networks (GNN) to perform inference upon heterogeneous document-graphs. Despite their great successes, these graph-based methods, which normally only consider the words within the mentions in the process of building graphs and reasoning, tend to ignore the non-entity clue words that are not in the mentions but provide important clue information for relation reasoning. To alleviate this problem, we treat graph-based document-level RE models as an encoder-decoder framework, which typically uses a pre-trained language model as the encoder and a GNN model as the decoder, and propose a novel graph-based model NC-DRE that introduces decoder-to-encoder attention mechanism to leverage Non-entity Clue information for Document-level Relation Extraction.", "paper_url": "http://arxiv.org/abs/2204.00255v1", "pdf_url": "http://arxiv.org/pdf/2204.00255v1", "repo_url": null}, "2204.00203": {"publish_time": "2022-04-01", "title": "Graph Enhanced Contrastive Learning for Radiology Findings Summarization", "author": "Jinpeng Hu et.al.", "abstract": "The impression section of a radiology report summarizes the most prominent observation from the findings section and is the most important section for radiologists to communicate to physicians. Summarizing findings is time-consuming and can be prone to error for inexperienced radiologists, and thus automatic impression generation has attracted substantial attention. With the encoder-decoder framework, most previous studies explore incorporating extra knowledge (e.g., static pre-defined clinical ontologies or extra background information). Yet, they encode such knowledge by a separate encoder to treat it as an extra input to their models, which is limited in leveraging their relations with the original findings. To address the limitation, we propose a unified framework for exploiting both extra knowledge and the original findings in an integrated way so that the critical information (i.e., key words and their relations) can be extracted in an appropriate way to facilitate impression generation. In detail, for each input findings, it is encoded by a text encoder, and a graph is constructed through its entities and dependency tree. Then, a graph encoder (e.g., graph neural networks (GNNs)) is adopted to model relation information in the constructed graph. Finally, to emphasize the key words in the findings, contrastive learning is introduced to map positive samples (constructed by masking non-key words) closer and push apart negative ones (constructed by masking key words). The experimental results on OpenI and MIMIC-CXR confirm the effectiveness of our proposed method.", "paper_url": "http://arxiv.org/abs/2204.00203v1", "pdf_url": "http://arxiv.org/pdf/2204.00203v1", "repo_url": "https://github.com/jinpeng01/aig_cl"}, "2204.01681": {"publish_time": "2022-04-04", "title": "End-to-end multi-particle reconstruction in high occupancy imaging calorimeters with graph neural networks", "author": "Shah Rukh Qasim et.al.", "abstract": "We present an end-to-end reconstruction algorithm to build particle candidates from detector hits in next-generation granular calorimeters similar to that foreseen for the high-luminosity upgrade of the CMS detector. The algorithm exploits a distance-weighted graph neural network, trained with object condensation, a graph segmentation technique. Through a single-shot approach, the reconstruction task is paired with energy regression. We describe the reconstruction performance in terms of efficiency as well as in terms of energy resolution. In addition, we show the jet reconstruction performance of our method and discuss its inference computational cost. This work is the first-ever example of single-shot calorimetric reconstruction of ${\\cal O}(1000)$ particles in high-luminosity conditions with 200 pileup to our knowledge.", "paper_url": "http://arxiv.org/abs/2204.01681v1", "pdf_url": "http://arxiv.org/pdf/2204.01681v1", "repo_url": null}, "2204.01618": {"publish_time": "2022-04-04", "title": "Deep-Ensemble-Based Uncertainty Quantification in Spatiotemporal Graph Neural Networks for Traffic Forecasting", "author": "Tanwi Mallick et.al.", "abstract": "Deep-learning-based data-driven forecasting methods have produced impressive results for traffic forecasting. A major limitation of these methods, however, is that they provide forecasts without estimates of uncertainty, which are critical for real-time deployments. We focus on a diffusion convolutional recurrent neural network (DCRNN), a state-of-the-art method for short-term traffic forecasting. We develop a scalable deep ensemble approach to quantify uncertainties for DCRNN. Our approach uses a scalable Bayesian optimization method to perform hyperparameter optimization, selects a set of high-performing configurations, fits a generative model to capture the joint distributions of the hyperparameter configurations, and trains an ensemble of models by sampling a new set of hyperparameter configurations from the generative model. We demonstrate the efficacy of the proposed methods by comparing them with other uncertainty estimation techniques. We show that our generic and scalable approach outperforms the current state-of-the-art Bayesian and a number of other commonly used frequentist techniques.", "paper_url": "http://arxiv.org/abs/2204.01618v1", "pdf_url": "http://arxiv.org/pdf/2204.01618v1", "repo_url": null}, "2204.01376": {"publish_time": "2022-04-04", "title": "Synthetic Graph Generation to Benchmark Graph Learning", "author": "Anton Tsitsulin et.al.", "abstract": "Graph learning algorithms have attained state-of-the-art performance on many graph analysis tasks such as node classification, link prediction, and clustering. It has, however, become hard to track the field's burgeoning progress. One reason is due to the very small number of datasets used in practice to benchmark the performance of graph learning algorithms. This shockingly small sample size (~10) allows for only limited scientific insight into the problem.   In this work, we aim to address this deficiency. We propose to generate synthetic graphs, and study the behaviour of graph learning algorithms in a controlled scenario. We develop a fully-featured synthetic graph generator that allows deep inspection of different models. We argue that synthetic graph generations allows for thorough investigation of algorithms and provides more insights than overfitting on three citation datasets. In the case study, we show how our framework provides insight into unsupervised and supervised graph neural network models.", "paper_url": "http://arxiv.org/abs/2204.01376v1", "pdf_url": "http://arxiv.org/pdf/2204.01376v1", "repo_url": null}, "2204.01349": {"publish_time": "2022-04-04", "title": "MGRR-Net: Multi-level Graph Relational Reasoning Network for Facial Action Units Detection", "author": "Xuri Ge et.al.", "abstract": "The Facial Action Coding System (FACS) encodes the action units (AUs) in facial images, which has attracted extensive research attention due to its wide use in facial expression analysis. Many methods that perform well on automatic facial action unit (AU) detection primarily focus on modeling various types of AU relations between corresponding local muscle areas, or simply mining global attention-aware facial features, however, neglect the dynamic interactions among local-global features. We argue that encoding AU features just from one perspective may not capture the rich contextual information between regional and global face features, as well as the detailed variability across AUs, because of the diversity in expression and individual characteristics. In this paper, we propose a novel Multi-level Graph Relational Reasoning Network (termed MGRR-Net) for facial AU detection. Each layer of MGRR-Net performs a multi-level (i.e., region-level, pixel-wise and channel-wise level) feature learning. While the region-level feature learning from local face patches features via graph neural network can encode the correlation across different AUs, the pixel-wise and channel-wise feature learning via graph attention network can enhance the discrimination ability of AU features from global face features. The fused features from the three levels lead to improved AU discriminative ability. Extensive experiments on DISFA and BP4D AU datasets show that the proposed approach achieves superior performance than the state-of-the-art methods.", "paper_url": "http://arxiv.org/abs/2204.01349v1", "pdf_url": "http://arxiv.org/pdf/2204.01349v1", "repo_url": null}, "2204.01089": {"publish_time": "2022-04-03", "title": "Virtual Relational Knowledge Graphs for Recommendation", "author": "Lingyun Lu et.al.", "abstract": "Incorporating knowledge graph as side information has become a new trend in recommendation systems. Recent studies regard items as entities of a knowledge graph and leverage graph neural networks to assist item encoding, yet by considering each relation type individually. However, relation types are often too many and sometimes one relation type involves too few entities. We argue that it is not efficient nor effective to use every relation type for item encoding. In this paper, we propose a VRKG4Rec model (Virtual Relational Knowledge Graphs for Recommendation), which explicitly distinguish the influence of different relations for item representation learning. We first construct virtual relational graphs (VRKGs) by an unsupervised learning scheme. We also design a local weighted smoothing (LWS) mechanism for encoding nodes, which iteratively updates a node embedding only depending on the embedding of its own and its neighbors, but involve no additional training parameters. We also employ the LWS mechanism on a user-item bipartite graph for user representation learning, which utilizes encodings of items with relational knowledge to help training representations of users. Experiment results on two public datasets validate that our VRKG4Rec model outperforms the state-of-the-art methods.", "paper_url": "http://arxiv.org/abs/2204.01089v1", "pdf_url": "http://arxiv.org/pdf/2204.01089v1", "repo_url": null}, "2204.02338": {"publish_time": "2022-04-05", "title": "MGDCF: Distance Learning via Markov Graph Diffusion for Neural Collaborative Filtering", "author": "Jun Hu et.al.", "abstract": "Collaborative filtering (CF) is widely used by personalized recommendation systems, which aims to predict the preference of users with historical user-item interactions. In recent years, Graph Neural Networks (GNNs) have been utilized to build CF models and have shown promising performance. Recent state-of-the-art GNN-based CF approaches simply attribute their performance improvement to the high-order neighbor aggregation ability of GNNs. However, we observe that some powerful deep GNNs such as JKNet and DropEdge, can effectively exploit high-order neighbor information on other graph tasks but perform poorly on CF tasks, which conflicts with the explanation of these GNN-based CF research. Different from these research, we investigate the GNN-based CF from the perspective of Markov processes for distance learning with a unified framework named Markov Graph Diffusion Collaborative Filtering (MGDCF). We design a Markov Graph Diffusion Network (MGDN) as MGDCF's GNN encoder, which learns vertex representations by trading off two types of distances via a Markov process. We show the theoretical equivalence between MGDN's output and the optimal solution of a distance loss function, which can boost the optimization of CF models. MGDN can generalize state-of-the-art models such as LightGCN and APPNP, which are heterogeneous GNNs. In addition, MGDN can be extended to homogeneous GNNs with our sparsification technique. For optimizing MGDCF, we propose the InfoBPR loss function, which extends the widely used BPR loss to exploit multiple negative samples for better performance. We conduct experiments to perform detailed analysis on MGDCF. The source code is publicly available at https://github.com/hujunxianligong/MGDCF.", "paper_url": "http://arxiv.org/abs/2204.02338v1", "pdf_url": "http://arxiv.org/pdf/2204.02338v1", "repo_url": "https://github.com/hujunxianligong/MGDCF"}, "2204.02119": {"publish_time": "2022-04-05", "title": "Transition Information Enhanced Disentangled Graph Neural Networks for Session-based Recommendation", "author": "Ansong Li et.al.", "abstract": "Session-based recommendation is a practical recommendation task that predicts the next item based on an anonymous behavior sequence, and its performance relies heavily on the transition information between items in the sequence. The SOTA methods in SBR employ GNN to model neighboring item transitions from global (i.e, other sessions) and local (i.e, current session) contexts. However, most existing methods treat neighbors from different sessions equally without considering that the neighbor items from different sessions may share similar features with the target item on different aspects and may have different contributions. In other words, they have not explored finer-granularity transition information between items in the global context, leading to sub-optimal performance. In this paper, we fill this gap by proposing a novel Transition Information Enhanced Disentangled Graph Neural Network (TIE-DGNN) model to capture finer-granular transition information between items and try to interpret the reason of the transition by modeling the various factors of the item. Specifically, we propose a position-aware global graph, which utilizes the relative position information to model the neighboring item transition. Then, we slice item embeddings into blocks, each of which represents a factor, and use disentangling module to separately learn the factor embeddings over the global graph. For local context, we train item embeddings by using attention mechanisms to capture transition information from the current session. To this end, our model considers two levels of transition information. Especially in global text, we not only consider finer-granularity transition information between items but also take user intents at factor-level into account to interpret the key reason for the transition. Extensive experiments on three datasets demonstrate the superiority of our method over the SOTA methods.", "paper_url": "http://arxiv.org/abs/2204.02119v1", "pdf_url": "http://arxiv.org/pdf/2204.02119v1", "repo_url": "https://github.com/AnsongLi/TIE-DGNN"}, "2204.02002": {"publish_time": "2022-04-05", "title": "Micro-Behavior Encoding for Session-based Recommendation", "author": "Jiahao Yuan et.al.", "abstract": "Session-based Recommendation (SR) aims to predict the next item for recommendation based on previously recorded sessions of user interaction. The majority of existing approaches to SR focus on modeling the transition patterns of items. In such models, the so-called micro-behaviors describing how the user locates an item and carries out various activities on it (e.g., click, add-to-cart, and read-comments), are simply ignored. A few recent studies have tried to incorporate the sequential patterns of micro-behaviors into SR models. However, those sequential models still cannot effectively capture all the inherent interdependencies between micro-behavior operations. In this work, we aim to investigate the effects of the micro-behavior information in SR systematically. Specifically, we identify two different patterns of micro-behaviors: \"sequential patterns\" and \"dyadic relational patterns\". To build a unified model of user micro-behaviors, we first devise a multigraph to aggregate the sequential patterns from different items via a graph neural network, and then utilize an extended self-attention network to exploit the pair-wise relational patterns of micro-behaviors. Extensive experiments on three public real-world datasets show the superiority of the proposed approach over the state-of-theart baselines and confirm the usefulness of these two different micro-behavior patterns for SR.", "paper_url": "http://arxiv.org/abs/2204.02002v1", "pdf_url": "http://arxiv.org/pdf/2204.02002v1", "repo_url": null}, "2204.02944": {"publish_time": "2022-04-06", "title": "\"The Pedestrian next to the Lamppost\" Adaptive Object Graphs for Better Instantaneous Mapping", "author": "Avishkar Saha et.al.", "abstract": "Estimating a semantically segmented bird's-eye-view (BEV) map from a single image has become a popular technique for autonomous control and navigation. However, they show an increase in localization error with distance from the camera. While such an increase in error is entirely expected - localization is harder at distance - much of the drop in performance can be attributed to the cues used by current texture-based models, in particular, they make heavy use of object-ground intersections (such as shadows), which become increasingly sparse and uncertain for distant objects. In this work, we address these shortcomings in BEV-mapping by learning the spatial relationship between objects in a scene. We propose a graph neural network which predicts BEV objects from a monocular image by spatially reasoning about an object within the context of other objects. Our approach sets a new state-of-the-art in BEV estimation from monocular images across three large-scale datasets, including a 50% relative improvement for objects on nuScenes.", "paper_url": "http://arxiv.org/abs/2204.02944v1", "pdf_url": "http://arxiv.org/pdf/2204.02944v1", "repo_url": null}, "2204.02782": {"publish_time": "2022-04-06", "title": "How Do Graph Networks Generalize to Large and Diverse Molecular Systems?", "author": "Johannes Gasteiger et.al.", "abstract": "The predominant method of demonstrating progress of atomic graph neural networks are benchmarks on small and limited datasets. The implicit hypothesis behind this approach is that progress on these narrow datasets generalize to the large diversity of chemistry. This generalizability would be very helpful for research, but currently remains untested. In this work we test this assumption by identifying four aspects of complexity in which many datasets are lacking: 1. Chemical diversity (number of different elements), 2. system size (number of atoms per sample), 3. dataset size (number of data samples), and 4. domain shift (similarity of the training and test set). We introduce multiple subsets of the large Open Catalyst 2020 (OC20) dataset to independently investigate each of these aspects. We then perform 21 ablation studies and sensitivity analyses on 9 datasets testing both previously proposed and new model enhancements. We find that some improvements are consistent between datasets, but many are not and some even have opposite effects. Based on this analysis, we identify a smaller dataset that correlates well with the full OC20 dataset, and propose the GemNet-OC model, which outperforms the previous state-of-the-art on OC20 by 16%, while reducing training time by a factor of 10. Overall, our findings challenge the common belief that graph neural networks work equally well independent of dataset size and diversity, and suggest that caution must be exercised when making generalizations based on narrow datasets.", "paper_url": "http://arxiv.org/abs/2204.02782v1", "pdf_url": "http://arxiv.org/pdf/2204.02782v1", "repo_url": null}, "2204.02625": {"publish_time": "2022-04-06", "title": "Bridging the Gap of AutoGraph between Academia and Industry: Analysing AutoGraph Challenge at KDD Cup 2020", "author": "Zhen Xu et.al.", "abstract": "Graph structured data is ubiquitous in daily life and scientific areas and has attracted increasing attention. Graph Neural Networks (GNNs) have been proved to be effective in modeling graph structured data and many variants of GNN architectures have been proposed. However, much human effort is often needed to tune the architecture depending on different datasets. Researchers naturally adopt Automated Machine Learning on Graph Learning, aiming to reduce the human effort and achieve generally top-performing GNNs, but their methods focus more on the architecture search. To understand GNN practitioners' automated solutions, we organized AutoGraph Challenge at KDD Cup 2020, emphasizing on automated graph neural networks for node classification. We received top solutions especially from industrial tech companies like Meituan, Alibaba and Twitter, which are already open sourced on Github. After detailed comparisons with solutions from academia, we quantify the gaps between academia and industry on modeling scope, effectiveness and efficiency, and show that (1) academia AutoML for Graph solutions focus on GNN architecture search while industrial solutions, especially the winning ones in the KDD Cup, tend to obtain an overall solution (2) by neural architecture search only, academia solutions achieve on average 97.3% accuracy of industrial solutions (3) academia solutions are cheap to obtain with several GPU hours while industrial solutions take a few months' labors. Academic solutions also contain much fewer parameters.", "paper_url": "http://arxiv.org/abs/2204.02625v1", "pdf_url": "http://arxiv.org/pdf/2204.02625v1", "repo_url": null}, "2204.03225": {"publish_time": "2022-04-07", "title": "Explicit Feature Interaction-aware Graph Neural Networks", "author": "Minkyu Kim et.al.", "abstract": "Graph neural networks are powerful methods to handle graph-structured data. However, existing graph neural networks only learn higher-order feature interactions implicitly. Thus, they cannot capture information that occurred in low-order feature interactions. To overcome this problem, we propose Explicit Feature Interaction-aware Graph Neural Network (EFI-GNN), which explicitly learns arbitrary-order feature interactions. EFI-GNN can jointly learn with any other graph neural network. We demonstrate that the joint learning method always enhances performance on the various node classification tasks. Furthermore, since EFI-GNN is inherently a linear model, we can interpret the prediction result of EFI-GNN. With the computation rule, we can obtain an any-order feature's effect on the decision. By that, we visualize the effects of the first-order and second-order features as a form of a heatmap.", "paper_url": "http://arxiv.org/abs/2204.03225v1", "pdf_url": "http://arxiv.org/pdf/2204.03225v1", "repo_url": null}, "2204.03117": {"publish_time": "2022-04-06", "title": "BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis", "author": "Shuo Liang et.al.", "abstract": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis task that aims to align aspects and corresponding sentiments for aspect-specific sentiment polarity inference. It is challenging because a sentence may contain multiple aspects or complicated (e.g., conditional, coordinating, or adversative) relations. Recently, exploiting dependency syntax information with graph neural networks has been the most popular trend. Despite its success, methods that heavily rely on the dependency tree pose challenges in accurately modeling the alignment of the aspects and their words indicative of sentiment, since the dependency tree may provide noisy signals of unrelated associations (e.g., the \"conj\" relation between \"great\" and \"dreadful\" in Figure 2). In this paper, to alleviate this problem, we propose a Bi-Syntax aware Graph Attention Network (BiSyn-GAT+). Specifically, BiSyn-GAT+ fully exploits the syntax information (e.g., phrase segmentation and hierarchical structure) of the constituent tree of a sentence to model the sentiment-aware context of every single aspect (called intra-context) and the sentiment relations across aspects (called inter-context) for learning. Experiments on four benchmark datasets demonstrate that BiSyn-GAT+ outperforms the state-of-the-art methods consistently.", "paper_url": "http://arxiv.org/abs/2204.03117v1", "pdf_url": "http://arxiv.org/pdf/2204.03117v1", "repo_url": null}, "2204.03080": {"publish_time": "2022-04-06", "title": "Graph Neural Networks Designed for Different Graph Types: A Survey", "author": "Josephine M. Thomas et.al.", "abstract": "Graphs are ubiquitous in nature and can therefore serve as models for many practical but also theoretical problems. Based on this, the young research field of Graph Neural Networks (GNNs) has emerged. Despite the youth of the field and the speed in which new models are developed, many good surveys have been published in the last years. Nevertheless, an overview on which graph types can be modeled by GNNs is missing. In this survey, we give a detailed overview of already existing GNNs and, unlike previous surveys, categorize them according to their ability to handle different graph types. We consider GNNs operating on static as well as on dynamic graphs of different structural constitutions, with or without node or edge attributes. Moreover in the dynamic case, we separate the models in discrete-time and continuous-time dynamic graphs based on their architecture. According to our findings, there are still graph types, that are not covered by existing GNN models. Specifically, models concerning heterogeneity in attributes are missing and the deletion of nodes and edges is only covered rarely.", "paper_url": "http://arxiv.org/abs/2204.03080v1", "pdf_url": "http://arxiv.org/pdf/2204.03080v1", "repo_url": null}, "2204.04046": {"publish_time": "2022-04-08", "title": "KCD: Knowledge Walks and Textual Cues Enhanced Political Perspective Detection in News Media", "author": "Wenqian Zhang et.al.", "abstract": "Political perspective detection has become an increasingly important task that can help combat echo chambers and political polarization. Previous approaches generally focus on leveraging textual content to identify stances, while they fail to reason with background knowledge or leverage the rich semantic and syntactic textual labels in news articles. In light of these limitations, we propose KCD, a political perspective detection approach to enable multi-hop knowledge reasoning and incorporate textual cues as paragraph-level labels. Specifically, we firstly generate random walks on external knowledge graphs and infuse them with news text representations. We then construct a heterogeneous information network to jointly model news content as well as semantic, syntactic and entity cues in news articles. Finally, we adopt relational graph neural networks for graph-level representation learning and conduct political perspective detection. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods on two benchmark datasets. We further examine the effect of knowledge walks and textual cues and how they contribute to our approach's data efficiency.", "paper_url": "http://arxiv.org/abs/2204.04046v1", "pdf_url": "http://arxiv.org/pdf/2204.04046v1", "repo_url": null}, "2204.03954": {"publish_time": "2022-04-08", "title": "Bag-of-Words vs. Sequence vs. Graph vs. Hierarchy for Single- and Multi-Label Text Classification", "author": "Andor Diera et.al.", "abstract": "Graph neural networks have triggered a resurgence of graph-based text classification methods, defining today's state of the art. We show that a simple multi-layer perceptron (MLP) using a Bag of Words (BoW) outperforms the recent graph-based models TextGCN and HeteGCN in an inductive text classification setting and is comparable with HyperGAT in single-label classification. We also run our own experiments on multi-label classification, where the simple MLP outperforms the recent sequential-based gMLP and aMLP models. Moreover, we fine-tune a sequence-based BERT and a lightweight DistilBERT model, which both outperform all models on both single-label and multi-label settings in most datasets. These results question the importance of synthetic graphs used in modern text classifiers. In terms of parameters, DistilBERT is still twice as large as our BoW-based wide MLP, while graph-based models like TextGCN require setting up an $\\mathcal{O}(N^2)$ graph, where $N$ is the vocabulary plus corpus size.", "paper_url": "http://arxiv.org/abs/2204.03954v1", "pdf_url": "http://arxiv.org/pdf/2204.03954v1", "repo_url": null}, "2204.05258": {"publish_time": "2022-04-11", "title": "Multi-view graph structure learning using subspace merging on Grassmann manifold", "author": "Razieh Ghiasi et.al.", "abstract": "Many successful learning algorithms have been recently developed to represent graph-structured data. For example, Graph Neural Networks (GNNs) have achieved considerable successes in various tasks such as node classification, graph classification, and link prediction. However, these methods are highly dependent on the quality of the input graph structure. One used approach to alleviate this problem is to learn the graph structure instead of relying on a manually designed graph. In this paper, we introduce a new graph structure learning approach using multi-view learning, named MV-GSL (Multi-View Graph Structure Learning), in which we aggregate different graph structure learning methods using subspace merging on Grassmann manifold to improve the quality of the learned graph structures. Extensive experiments are performed to evaluate the effectiveness of the proposed method on two benchmark datasets, Cora and Citeseer. Our experiments show that the proposed method has promising performance compared to single and other combined graph structure learning methods.", "paper_url": "http://arxiv.org/abs/2204.05258v1", "pdf_url": "http://arxiv.org/pdf/2204.05258v1", "repo_url": null}, "2204.05141": {"publish_time": "2022-04-11", "title": "Learning Object-Centered Autotelic Behaviors with Graph Neural Networks", "author": "Ahmed Akakzia et.al.", "abstract": "Although humans live in an open-ended world and endlessly face new challenges, they do not have to learn from scratch each time they face the next one. Rather, they have access to a handful of previously learned skills, which they rapidly adapt to new situations. In artificial intelligence, autotelic agents, which are intrinsically motivated to represent and set their own goals, exhibit promising skill adaptation capabilities. However, these capabilities are highly constrained by their policy and goal space representations. In this paper, we propose to investigate the impact of these representations on the learning capabilities of autotelic agents. We study different implementations of autotelic agents using four types of Graph Neural Networks policy representations and two types of goal spaces, either geometric or predicate-based. We show that combining object-centered architectures that are expressive enough with semantic relational goals enables an efficient transfer between skills and promotes behavioral diversity. We also release our graph-based implementations to encourage further research in this direction.", "paper_url": "http://arxiv.org/abs/2204.05141v1", "pdf_url": "http://arxiv.org/pdf/2204.05141v1", "repo_url": "https://github.com/akakzia/rlgraph"}, "2204.04879": {"publish_time": "2022-04-11", "title": "How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision", "author": "Dongkwan Kim et.al.", "abstract": "Attention mechanism in graph neural networks is designed to assign larger weights to important neighbor nodes for better representation. However, what graph attention learns is not understood well, particularly when graphs are noisy. In this paper, we propose a self-supervised graph attention network (SuperGAT), an improved graph attention model for noisy graphs. Specifically, we exploit two attention forms compatible with a self-supervised task to predict edges, whose presence and absence contain the inherent information about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more expressive attention in distinguishing mislinked neighbors. We find two graph characteristics influence the effectiveness of attention forms and self-supervision: homophily and average degree. Thus, our recipe provides guidance on which attention design to use when those two graph characteristics are known. Our experiment on 17 real-world datasets demonstrates that our recipe generalizes across 15 datasets of them, and our models designed by recipe show improved performance over baselines.", "paper_url": "http://arxiv.org/abs/2204.04879v1", "pdf_url": "http://arxiv.org/pdf/2204.04879v1", "repo_url": "https://github.com/dongkwan-kim/SuperGAT"}, "2204.04874": {"publish_time": "2022-04-11", "title": "Augmentation-Free Graph Contrastive Learning", "author": "Haonan Wang et.al.", "abstract": "Graph contrastive learning (GCL) is the most representative and prevalent self-supervised learning approach for graph-structured data. Despite its remarkable success, existing GCL methods highly rely on an augmentation scheme to learn the representations invariant across different augmentation views. In this work, we revisit such a convention in GCL through examining the effect of augmentation techniques on graph data via the lens of spectral theory. We found that graph augmentations preserve the low-frequency components and perturb the middle- and high-frequency components of the graph, which contributes to the success of GCL algorithms on homophilic graphs but hinders its application on heterophilic graphs, due to the high-frequency preference of heterophilic data. Motivated by this, we propose a novel, theoretically-principled, and augmentation-free GCL method, named AF-GCL, that (1) leverages the features aggregated by Graph Neural Network to construct the self-supervision signal instead of augmentations and therefore (2) is less sensitive to the graph homophily degree. Theoretically, We present the performance guarantee for AF-GCL as well as an analysis for understanding the efficacy of AF-GCL. Extensive experiments on 14 benchmark datasets with varying degrees of heterophily show that AF-GCL presents competitive or better performance on homophilic graphs and outperforms all existing state-of-the-art GCL methods on heterophilic graphs with significantly less computational overhead.", "paper_url": "http://arxiv.org/abs/2204.04874v1", "pdf_url": "http://arxiv.org/pdf/2204.04874v1", "repo_url": null}, "2204.04740": {"publish_time": "2022-04-10", "title": "Melting temperature prediction via first principles and deep learning", "author": "Qi-Jun Hong et.al.", "abstract": "Melting is a high temperature process that requires extensive sampling of configuration space, thus making melting temperature prediction computationally very expensive and challenging. Over the past few years, I have built two methods to address this challenge, one via direct density functional theory (DFT) molecular dynamics (MD) simulations and the other via deep learning graph neural networks. The DFT approach is based on statistical analysis of small-size solid-liquid coexistence MD simulations. It eliminates the risk of metastable superheated solid in the fast-heating method, while also significantly reducing the computer cost relative to the traditional large-scale coexistence method. Being both accurate and efficient (at the speed of several days per material), it is considered as one of the best methods for direct DFT melting temperature calculation. The deep learning method is based on graph neural networks that effectively handles permutation invariance in chemical formula, which drastically improves efficiency and reduces cost. At the speed of milliseconds per material, the model is extremely fast, while being moderately accurate, especially within the composition space expanded by the dataset. I have implemented both methods into automated computer code packages, making them publicly available and free to download. The DFT and deep learning methods are highly complementary to each other, and hence they can be potentially well integrated into a framework for melting temperature prediction. I demonstrated examples of applying the methods to materials design and discovery of high-melting-point materials.", "paper_url": "http://arxiv.org/abs/2204.04740v1", "pdf_url": "http://arxiv.org/pdf/2204.04740v1", "repo_url": null}, "2204.05518": {"publish_time": "2022-04-12", "title": "Trigger-GNN: A Trigger-Based Graph Neural Network for Nested Named Entity Recognition", "author": "Yuan Sui et.al.", "abstract": "Nested named entity recognition (NER) aims to identify the entity boundaries and recognize categories of the named entities in a complex hierarchical sentence. Some works have been done using character-level, word-level, or lexicon-level based models. However, such researches ignore the role of the complementary annotations. In this paper, we propose a trigger-based graph neural network (Trigger-GNN) to leverage the nested NER. It obtains the complementary annotation embeddings through entity trigger encoding and semantic matching, and tackle nested entity utilizing an efficient graph message passing architecture, aggregation-update mode. We posit that using entity triggers as external annotations can add in complementary supervision signals on the whole sentences. It helps the model to learn and generalize more efficiently and cost-effectively. Experiments show that the Trigger-GNN consistently outperforms the baselines on four public NER datasets, and it can effectively alleviate the nested NER.", "paper_url": "http://arxiv.org/abs/2204.05518v1", "pdf_url": "http://arxiv.org/pdf/2204.05518v1", "repo_url": null}, "2204.05493": {"publish_time": "2022-04-12", "title": "Physics-informed graph neural networks enhance scalability of variational nonequilibrium optimal control", "author": "Jiawei Yan et.al.", "abstract": "When a physical system is driven away from equilibrium, the statistical distribution of its dynamical trajectories informs many of its physical properties. Characterizing the nature of the distribution of dynamical observables, such as a current or entropy production rate, has become a central problem in nonequilibrium statistical mechanics. Asymptotically, for a broad class of observables, the distribution of a given observable satisfies a large deviation principle when the dynamics is Markovian, meaning that fluctuations can be characterized in the long-time limit by computing a scaled cumulant generating function. Calculating this function is not tractable analytically (nor often numerically) for complex, interacting systems, so the development of robust numerical techniques to carry out this computation is needed to probe the properties of nonequilibrium materials. Here, we describe an algorithm that recasts this task as an optimal control problem that can be solved variationally. We solve for optimal control forces using neural network ans\\\"atze that are tailored to the physical systems to which the forces are applied. We demonstrate that this approach leads to transferable and accurate solutions in two systems featuring large numbers of interacting particles.", "paper_url": "http://arxiv.org/abs/2204.05493v1", "pdf_url": "http://arxiv.org/pdf/2204.05493v1", "repo_url": null}, "2204.05351": {"publish_time": "2022-04-11", "title": "Graph Ordering Attention Networks", "author": "Michail Chatzianastasis et.al.", "abstract": "Graph Neural Networks (GNNs) have been successfully used in many problems involving graph-structured data, achieving state-of-the-art performance. GNNs typically employ a message-passing scheme, in which every node aggregates information from its neighbors using a permutation-invariant aggregation function. Standard well-examined choices such as the mean or sum aggregation functions have limited capabilities, as they are not able to capture interactions among neighbors. In this work, we formalize these interactions using an information-theoretic framework that notably includes synergistic information. Driven by this definition, we introduce the Graph Ordering Attention (GOAT) layer, a novel GNN component that captures interactions between nodes in a neighborhood. This is achieved by learning local node orderings via an attention mechanism and processing the ordered representations using a recurrent neural network aggregator. This design allows us to make use of a permutation-sensitive aggregator while maintaining the permutation-equivariance of the proposed GOAT layer. The GOAT model demonstrates its increased performance in modeling graph metrics that capture complex information, such as the betweenness centrality and the effective size of a node. In practical use-cases, its superior modeling capability is confirmed through its success in several real-world node classification benchmarks.", "paper_url": "http://arxiv.org/abs/2204.05351v1", "pdf_url": "http://arxiv.org/pdf/2204.05351v1", "repo_url": "https://github.com/michailchatzianastasis/goat"}}}}